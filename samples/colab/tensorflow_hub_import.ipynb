{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH3IRpYTta2v"
      },
      "source": [
        "##### Copyright 2021 The IREE Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "mWGa71_Ct2ug"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License v2.0 with LLVM Exceptions.\n",
        "# See https://llvm.org/LICENSE.txt for license information.\n",
        "# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb3S0mSjpK7J"
      },
      "source": [
        "# IREE TensorFlow Hub Import\n",
        "\n",
        "This notebook demonstrates how to download, import, and compile models from [TensorFlow Hub](https://tfhub.dev/). It covers:\n",
        "\n",
        "* Downloading a model from TensorFlow Hub\n",
        "* Ensuring the model has serving signatures needed for import\n",
        "* Importing and compiling the model with IREE\n",
        "\n",
        "At the end of the notebook, the compilation artifacts are compressed into a .zip file for you to download and use in an application.\n",
        "\n",
        "See also https://iree.dev/guides/ml-frameworks/tensorflow/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rNAJKNVkKOr"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RdVc4TbOkHM2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m pip install iree-compiler iree-runtime iree-tools-tf -f https://iree.dev/pip-release-links.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m pip install tensorflow_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRwv3qI_l5O_",
        "outputId": "8d3bf1f1-1843-4fe9-80e0-a9fc5b194778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.16.0-dev20231103\n",
            "Using artifacts directory '/tmp/iree/colab_artifacts'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tempfile\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from iree.compiler import tf as tfc\n",
        "\n",
        "# Print version information for future notebook users to reference.\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "\n",
        "ARTIFACTS_DIR = os.path.join(tempfile.gettempdir(), \"iree\", \"colab_artifacts\")\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "print(f\"Using artifacts directory '{ARTIFACTS_DIR}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZAobcAhocFE"
      },
      "source": [
        "## Import pretrained [`mobilenet_v2`](https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4) model\n",
        "\n",
        "IREE supports importing TensorFlow 2 models exported in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format. This model we'll be importing is published in that format already, while other models may need to be converted first.\n",
        "\n",
        "MobileNet V2 is a family of neural network architectures for efficient on-device image classification and related tasks. This TensorFlow Hub module contains a trained instance of one particular network architecture packaged to perform image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fd0vmnloZo9",
        "outputId": "dabea3a2-d312-4729-c947-b24216a6c25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded model from tfhub to path: '/tmp/tfhub_modules/426589ad685896ab7954855255a52db3442cb38d'\n"
          ]
        }
      ],
      "source": [
        "#@title Download the pretrained model\n",
        "\n",
        "# Use the `hub` library to download the pretrained model to the local disk\n",
        "# https://www.tensorflow.org/hub/api_docs/python/hub\n",
        "HUB_PATH = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n",
        "model_path = hub.resolve(HUB_PATH)\n",
        "print(f\"Downloaded model from tfhub to path: '{model_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CedNRSQTOE7C"
      },
      "source": [
        "### Check for serving signatures and re-export as needed\n",
        "\n",
        "IREE's compiler tools, like TensorFlow's `saved_model_cli` and other tools, require \"serving signatures\" to be defined in SavedModels.\n",
        "\n",
        "More references:\n",
        "\n",
        "* https://www.tensorflow.org/tfx/serving/signature_defs\n",
        "* https://blog.tensorflow.org/2021/03/a-tour-of-savedmodel-signatures.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiO66oEYQmsd",
        "outputId": "91f724db-01cd-4dd3-c55c-ba4431233cfa"
      },
      "outputs": [
        {
          "ename": "InternalError",
          "evalue": "cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb 单元格 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#@title Check for serving signatures\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load the SavedModel from the local disk and check if it has serving signatures\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# https://www.tensorflow.org/guide/saved_model#loading_and_using_a_custom_model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload(model_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m serving_signatures \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(loaded_model\u001b[39m.\u001b[39msignatures\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded SavedModel from \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:913\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(export_dir, os\u001b[39m.\u001b[39mPathLike):\n\u001b[1;32m    912\u001b[0m   export_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(export_dir)\n\u001b[0;32m--> 913\u001b[0m result \u001b[39m=\u001b[39m load_partial(export_dir, \u001b[39mNone\u001b[39;49;00m, tags, options)[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    914\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:1044\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39minit_scope():\n\u001b[1;32m   1043\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m   1045\u001b[0m                     ckpt_options, options, filters)\n\u001b[1;32m   1046\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1047\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1048\u001b[0m         \u001b[39mstr\u001b[39m(err) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m You may be trying to load on a different device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom the computational device. Consider setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1050\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1051\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto the io_device such as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:161\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proto \u001b[39m=\u001b[39m object_graph_proto\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_export_dir \u001b[39m=\u001b[39m export_dir\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_functions \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 161\u001b[0m     function_deserialization\u001b[39m.\u001b[39;49mload_function_def_library(\n\u001b[1;32m    162\u001b[0m         library\u001b[39m=\u001b[39;49mmeta_graph\u001b[39m.\u001b[39;49mgraph_def\u001b[39m.\u001b[39;49mlibrary,\n\u001b[1;32m    163\u001b[0m         saved_object_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_proto,\n\u001b[1;32m    164\u001b[0m         wrapper_function\u001b[39m=\u001b[39;49m_WrapperFunction))\n\u001b[1;32m    165\u001b[0m \u001b[39m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m# captures.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restored_concrete_functions \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/function_deserialization.py:482\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    476\u001b[0m   \u001b[39mdel\u001b[39;00m fdef\u001b[39m.\u001b[39mattr[\u001b[39m\"\u001b[39m\u001b[39m_input_shapes\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    477\u001b[0m function_type \u001b[39m=\u001b[39m function_type_lib\u001b[39m.\u001b[39mfrom_structured_signature(\n\u001b[1;32m    478\u001b[0m     func_graph\u001b[39m.\u001b[39mstructured_input_signature,\n\u001b[1;32m    479\u001b[0m     func_graph\u001b[39m.\u001b[39mstructured_outputs,\n\u001b[1;32m    480\u001b[0m     func_graph\u001b[39m.\u001b[39mfunction_captures\u001b[39m.\u001b[39mcapture_types,\n\u001b[1;32m    481\u001b[0m )\n\u001b[0;32m--> 482\u001b[0m func \u001b[39m=\u001b[39m function_lib\u001b[39m.\u001b[39;49mConcreteFunction\u001b[39m.\u001b[39;49mfrom_func_graph(\n\u001b[1;32m    483\u001b[0m     func_graph, function_type, attrs\u001b[39m=\u001b[39;49mfdef\u001b[39m.\u001b[39;49mattr)\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m wrapper_function:\n\u001b[1;32m    485\u001b[0m   func \u001b[39m=\u001b[39m wrapper_function(func)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1076\u001b[0m, in \u001b[0;36mConcreteFunction.from_func_graph\u001b[0;34m(cls, graph, function_type, attrs, shared_func_graph)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1075\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_func_graph\u001b[39m(\u001b[39mcls\u001b[39m, graph, function_type, attrs, shared_func_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m-> 1076\u001b[0m   atomic_fn \u001b[39m=\u001b[39m atomic_function\u001b[39m.\u001b[39;49mfrom_func_graph(\n\u001b[1;32m   1077\u001b[0m       _inference_name(graph\u001b[39m.\u001b[39;49mname), graph, attrs, function_type\n\u001b[1;32m   1078\u001b[0m   )\n\u001b[1;32m   1079\u001b[0m   \u001b[39mreturn\u001b[39;00m ConcreteFunction(atomic_fn, shared_func_graph\u001b[39m=\u001b[39mshared_func_graph)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:564\u001b[0m, in \u001b[0;36mfrom_func_graph\u001b[0;34m(name, graph, attrs, function_type, overwrite)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m overwrite \u001b[39mand\u001b[39;00m bound_context\u001b[39m.\u001b[39mhas_function(name):\n\u001b[1;32m    562\u001b[0m   bound_context\u001b[39m.\u001b[39mremove_function(name)\n\u001b[0;32m--> 564\u001b[0m bound_context\u001b[39m.\u001b[39;49madd_c_function(fn)\n\u001b[1;32m    565\u001b[0m pywrap_tf_session\u001b[39m.\u001b[39mTF_DeleteFunction(fn)\n\u001b[1;32m    567\u001b[0m call_options \u001b[39m=\u001b[39m CallOptions(\n\u001b[1;32m    568\u001b[0m     collective_manager_ids_used\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    569\u001b[0m         graph, \u001b[39m\"\u001b[39m\u001b[39mcollective_manager_ids_used\u001b[39m\u001b[39m\"\u001b[39m, []\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m     is_stateful\u001b[39m=\u001b[39m\u001b[39many\u001b[39m(op\u001b[39m.\u001b[39m_is_stateful \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m operations),  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    573\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1351\u001b[0m, in \u001b[0;36mContext.add_c_function\u001b[0;34m(self, c_func)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_c_function\u001b[39m(\u001b[39mself\u001b[39m, c_func):\n\u001b[1;32m   1343\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Add a C API TF_Function to the context.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \n\u001b[1;32m   1345\u001b[0m \u001b[39m  Once added, the function (identified by its name) can be executed like any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[39m    c_func: A wrapped TF_Function (returned from TF_GraphToFunction_wrapper).\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_initialized()\n\u001b[1;32m   1352\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextAddFunction(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle, c_func)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/context.py:603\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextOptionsSetRunEagerOpAsFunction(opts, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    601\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextOptionsSetJitCompileRewrite(\n\u001b[1;32m    602\u001b[0m       opts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile_rewrite)\n\u001b[0;32m--> 603\u001b[0m   context_handle \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_NewContext(opts)\n\u001b[1;32m    604\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_DeleteContextOptions(opts)\n",
            "\u001b[0;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ],
      "source": [
        "#@title Check for serving signatures\n",
        "\n",
        "# Load the SavedModel from the local disk and check if it has serving signatures\n",
        "# https://www.tensorflow.org/guide/saved_model#loading_and_using_a_custom_model\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "loaded_model = tf.saved_model.load(model_path)\n",
        "serving_signatures = list(loaded_model.signatures.keys())\n",
        "print(f\"Loaded SavedModel from '{model_path}'\")\n",
        "print(f\"Serving signatures: {serving_signatures}\")\n",
        "\n",
        "# Also check with the saved_model_cli:\n",
        "print(\"\\n---\\n\")\n",
        "print(\"Checking for signature_defs using saved_model_cli:\\n\")\n",
        "!saved_model_cli show --dir {model_path} --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKqqX2LsReNz"
      },
      "source": [
        "Since the model we downloaded did not include any serving signatures, we'll re-export it with serving signatures defined.\n",
        "\n",
        "* https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlDG2OuqOBGC",
        "outputId": "c25d0e59-3a42-4f43-804c-c607eb9fc84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n"
          ]
        }
      ],
      "source": [
        "#@title Look up input signatures to use when exporting\n",
        "\n",
        "# To save serving signatures we need to specify a `ConcreteFunction` with a\n",
        "# TensorSpec signature. We can determine what this signature should be by\n",
        "# looking at any documentation for the model or running the saved_model_cli.\n",
        "\n",
        "!saved_model_cli show --dir {model_path} --all \\\n",
        "    2> /dev/null | grep \"inputs: TensorSpec\" | tail -n 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnb4HhMmkgiT",
        "outputId": "d5ff3d4a-0483-476e-af6e-0b3c827d4938"
      },
      "outputs": [
        {
          "ename": "InternalError",
          "evalue": "cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb 单元格 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload(model_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#@title Re-export the model using the known signature\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Get a concrete function using the signature we found above.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# inference on a single image at a time, so set it to `1`. The rest of the\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# shape is the fixed image dimensions [width=224, height=224, channels=3].\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/v-yinuoliu/code/iree/samples/colab/tensorflow_hub_import.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m call \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m.\u001b[39mget_concrete_function(tf\u001b[39m.\u001b[39mTensorSpec([\u001b[39m1\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m3\u001b[39m], tf\u001b[39m.\u001b[39mfloat32))\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:913\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(export_dir, os\u001b[39m.\u001b[39mPathLike):\n\u001b[1;32m    912\u001b[0m   export_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(export_dir)\n\u001b[0;32m--> 913\u001b[0m result \u001b[39m=\u001b[39m load_partial(export_dir, \u001b[39mNone\u001b[39;49;00m, tags, options)[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    914\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:1044\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39minit_scope():\n\u001b[1;32m   1043\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m   1045\u001b[0m                     ckpt_options, options, filters)\n\u001b[1;32m   1046\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1047\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1048\u001b[0m         \u001b[39mstr\u001b[39m(err) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m You may be trying to load on a different device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom the computational device. Consider setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1050\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1051\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto the io_device such as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:161\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proto \u001b[39m=\u001b[39m object_graph_proto\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_export_dir \u001b[39m=\u001b[39m export_dir\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_functions \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 161\u001b[0m     function_deserialization\u001b[39m.\u001b[39;49mload_function_def_library(\n\u001b[1;32m    162\u001b[0m         library\u001b[39m=\u001b[39;49mmeta_graph\u001b[39m.\u001b[39;49mgraph_def\u001b[39m.\u001b[39;49mlibrary,\n\u001b[1;32m    163\u001b[0m         saved_object_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_proto,\n\u001b[1;32m    164\u001b[0m         wrapper_function\u001b[39m=\u001b[39;49m_WrapperFunction))\n\u001b[1;32m    165\u001b[0m \u001b[39m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m# captures.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restored_concrete_functions \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/saved_model/function_deserialization.py:482\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    476\u001b[0m   \u001b[39mdel\u001b[39;00m fdef\u001b[39m.\u001b[39mattr[\u001b[39m\"\u001b[39m\u001b[39m_input_shapes\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    477\u001b[0m function_type \u001b[39m=\u001b[39m function_type_lib\u001b[39m.\u001b[39mfrom_structured_signature(\n\u001b[1;32m    478\u001b[0m     func_graph\u001b[39m.\u001b[39mstructured_input_signature,\n\u001b[1;32m    479\u001b[0m     func_graph\u001b[39m.\u001b[39mstructured_outputs,\n\u001b[1;32m    480\u001b[0m     func_graph\u001b[39m.\u001b[39mfunction_captures\u001b[39m.\u001b[39mcapture_types,\n\u001b[1;32m    481\u001b[0m )\n\u001b[0;32m--> 482\u001b[0m func \u001b[39m=\u001b[39m function_lib\u001b[39m.\u001b[39;49mConcreteFunction\u001b[39m.\u001b[39;49mfrom_func_graph(\n\u001b[1;32m    483\u001b[0m     func_graph, function_type, attrs\u001b[39m=\u001b[39;49mfdef\u001b[39m.\u001b[39;49mattr)\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m wrapper_function:\n\u001b[1;32m    485\u001b[0m   func \u001b[39m=\u001b[39m wrapper_function(func)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1076\u001b[0m, in \u001b[0;36mConcreteFunction.from_func_graph\u001b[0;34m(cls, graph, function_type, attrs, shared_func_graph)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1075\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_func_graph\u001b[39m(\u001b[39mcls\u001b[39m, graph, function_type, attrs, shared_func_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m-> 1076\u001b[0m   atomic_fn \u001b[39m=\u001b[39m atomic_function\u001b[39m.\u001b[39;49mfrom_func_graph(\n\u001b[1;32m   1077\u001b[0m       _inference_name(graph\u001b[39m.\u001b[39;49mname), graph, attrs, function_type\n\u001b[1;32m   1078\u001b[0m   )\n\u001b[1;32m   1079\u001b[0m   \u001b[39mreturn\u001b[39;00m ConcreteFunction(atomic_fn, shared_func_graph\u001b[39m=\u001b[39mshared_func_graph)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:564\u001b[0m, in \u001b[0;36mfrom_func_graph\u001b[0;34m(name, graph, attrs, function_type, overwrite)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m overwrite \u001b[39mand\u001b[39;00m bound_context\u001b[39m.\u001b[39mhas_function(name):\n\u001b[1;32m    562\u001b[0m   bound_context\u001b[39m.\u001b[39mremove_function(name)\n\u001b[0;32m--> 564\u001b[0m bound_context\u001b[39m.\u001b[39;49madd_c_function(fn)\n\u001b[1;32m    565\u001b[0m pywrap_tf_session\u001b[39m.\u001b[39mTF_DeleteFunction(fn)\n\u001b[1;32m    567\u001b[0m call_options \u001b[39m=\u001b[39m CallOptions(\n\u001b[1;32m    568\u001b[0m     collective_manager_ids_used\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    569\u001b[0m         graph, \u001b[39m\"\u001b[39m\u001b[39mcollective_manager_ids_used\u001b[39m\u001b[39m\"\u001b[39m, []\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m     is_stateful\u001b[39m=\u001b[39m\u001b[39many\u001b[39m(op\u001b[39m.\u001b[39m_is_stateful \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m operations),  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    573\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1351\u001b[0m, in \u001b[0;36mContext.add_c_function\u001b[0;34m(self, c_func)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_c_function\u001b[39m(\u001b[39mself\u001b[39m, c_func):\n\u001b[1;32m   1343\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Add a C API TF_Function to the context.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \n\u001b[1;32m   1345\u001b[0m \u001b[39m  Once added, the function (identified by its name) can be executed like any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[39m    c_func: A wrapped TF_Function (returned from TF_GraphToFunction_wrapper).\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_initialized()\n\u001b[1;32m   1352\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextAddFunction(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle, c_func)\n",
            "File \u001b[0;32m~/miniconda3/envs/iree_new/lib/python3.10/site-packages/tensorflow/python/eager/context.py:603\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextOptionsSetRunEagerOpAsFunction(opts, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    601\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_ContextOptionsSetJitCompileRewrite(\n\u001b[1;32m    602\u001b[0m       opts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile_rewrite)\n\u001b[0;32m--> 603\u001b[0m   context_handle \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_NewContext(opts)\n\u001b[1;32m    604\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39mTFE_DeleteContextOptions(opts)\n",
            "\u001b[0;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ],
      "source": [
        "loaded_model = tf.saved_model.load(model_path)\n",
        "#@title Re-export the model using the known signature\n",
        "\n",
        "# Get a concrete function using the signature we found above.\n",
        "# \n",
        "# The first element of the shape is a dynamic batch size. We'll be running\n",
        "# inference on a single image at a time, so set it to `1`. The rest of the\n",
        "# shape is the fixed image dimensions [width=224, height=224, channels=3].\n",
        "call = loaded_model.__call__.get_concrete_function(tf.TensorSpec([1, 224, 224, 3], tf.float32))\n",
        "\n",
        "# Save the model, setting the concrete function as a serving signature.\n",
        "# https://www.tensorflow.org/guide/saved_model#saving_a_custom_model\n",
        "resaved_model_path = '/tmp/resaved_model'\n",
        "tf.saved_model.save(loaded_model, resaved_model_path, signatures=call)\n",
        "clear_output()  # Skip over TensorFlow's output.\n",
        "print(f\"Saved model with serving signatures to '{resaved_model_path}'\")\n",
        "\n",
        "# Load the model back into memory and check that it has serving signatures now\n",
        "reloaded_model = tf.saved_model.load(resaved_model_path)\n",
        "reloaded_serving_signatures = list(reloaded_model.signatures.keys())\n",
        "print(f\"\\nReloaded SavedModel from '{resaved_model_path}'\")\n",
        "print(f\"Serving signatures: {reloaded_serving_signatures}\")\n",
        "\n",
        "# Also check with the saved_model_cli:\n",
        "print(\"\\n---\\n\")\n",
        "print(\"Checking for signature_defs using saved_model_cli:\\n\")\n",
        "!saved_model_cli show --dir {resaved_model_path} --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdmgASzwanSz"
      },
      "source": [
        "### Import and compile the SavedModel with IREE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLkjlHE5mdmg",
        "outputId": "c67419f8-94de-4335-ddbc-f062b7d2e48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved compiled output to '/tmp/iree/colab_artifacts/mobilenet_v2.vmfb'\n",
            "Saved iree_input to      '/tmp/iree/colab_artifacts/mobilenet_v2_iree_input.mlir'\n"
          ]
        }
      ],
      "source": [
        "#@title Import from SavedModel\n",
        "\n",
        "# The main output file from compilation is a .vmfb \"VM FlatBuffer\". This file\n",
        "# can used to run the compiled model with IREE's runtime.\n",
        "output_file = os.path.join(ARTIFACTS_DIR, \"mobilenet_v2.vmfb\")\n",
        "# As compilation runs, dump an intermediate .mlir file for future inspection.\n",
        "iree_input = os.path.join(ARTIFACTS_DIR, \"mobilenet_v2_iree_input.mlir\")\n",
        "\n",
        "# Since our SavedModel uses signature defs, we use `saved_model_tags` with\n",
        "# `import_type=\"SIGNATURE_DEF\"`. If the SavedModel used an object graph, we\n",
        "# would use `exported_names` with `import_type=\"OBJECT_GRAPH\"` instead.\n",
        "\n",
        "# We'll set `target_backends=[\"vmvx\"]` to use IREE's reference CPU backend.\n",
        "# We could instead use different backends here, or set `import_only=True` then\n",
        "# download the imported .mlir file for compilation using native tools directly.\n",
        "\n",
        "tfc.compile_saved_model(\n",
        "    resaved_model_path,\n",
        "    output_file=output_file,\n",
        "    save_temp_iree_input=iree_input,\n",
        "    import_type=\"SIGNATURE_DEF\",\n",
        "    saved_model_tags=set([\"serve\"]),\n",
        "    target_backends=[\"vmvx\"])\n",
        "clear_output()  # Skip over TensorFlow's output.\n",
        "\n",
        "print(f\"Saved compiled output to '{output_file}'\")\n",
        "print(f\"Saved iree_input to      '{iree_input}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "IEJAzOb5qASI",
        "outputId": "9a29aa51-b99d-4acd-dae8-0d97cf9786e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zipping '/tmp/iree/colab_artifacts' to '/tmp/mobilenet_colab_artifacts.zip' for download...\n",
            "  adding: mobilenet_v2.vmfb (deflated 8%)\n",
            "  adding: mobilenet_v2_iree_input.mlir (deflated 46%)\n",
            "Downloading the artifacts zip file...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_18545900-47df-4250-9a14-8453ca4b6fc2\", \"mobilenet_colab_artifacts.zip\", 41434352)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Download compilation artifacts\n",
        "\n",
        "ARTIFACTS_ZIP = \"/tmp/mobilenet_colab_artifacts.zip\"\n",
        "\n",
        "print(f\"Zipping '{ARTIFACTS_DIR}' to '{ARTIFACTS_ZIP}' for download...\")\n",
        "!cd {ARTIFACTS_DIR} && zip -r {ARTIFACTS_ZIP} .\n",
        "\n",
        "# Note: you can also download files using the file explorer on the left\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Downloading the artifacts zip file...\")\n",
        "    files.download(ARTIFACTS_ZIP)\n",
        "except ImportError:\n",
        "    print(\"Missing google_colab Python package, can't download files\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-V0X0E7LkEa4",
        "FH3IRpYTta2v"
      ],
      "name": "tensorflow_hub_import.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
