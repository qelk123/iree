Args: ../../iree-build-2/tools/iree-compile --iree-input-type=torch --iree-hal-target-backends=cuda input.mlir -o tmp.vmfb --mlir-print-ir-after-all --iree-hal-cuda-dump-ptx --iree-hal-cuda-llvm-target-arch=sm_75 --iree-hal-cuda-llvm-target-feature=+ptx75 -debug 
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistinctAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ResourceBlobManagerDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineBinaryOpExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineConstantExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineDimExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineMapStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::IntegerSetStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVMTranslationDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneRegion<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroResults<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroSuccessors<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoRegionArguments<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoTerminator<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlock<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OpInvariants<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AffineScope<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIsolatedFromAbove<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SymbolTable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasOnlyGraphRegion<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::ModuleOpGenericAdaptorBase::Properties)
Load new dialect in Context util
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectInlinerInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SizedStorageAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SerializableAttrInterface)
Ignoring repeated interface registration
Ignoring repeated interface registration
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::ReferenceTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SizeAwareTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::InferTypeSizeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SubrangeTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SizeAwareOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SubrangeOperandOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ViewLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SubrangeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::TiedOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalAddressOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalLoadIndirectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalLoadOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalStoreIndirectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalStoreOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::InitializerOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroRegions<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::detail::GlobalOpGenericAdaptorBase::Properties)
Load new dialect in Context func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface)
Load new dialect in Context cf
Load new dialect in Context arith
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::NumericCastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueBoundsOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AutomaticAllocationScope<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface::Trait<Empty>)
Load new dialect in Context torch
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface)
Load new dialect in Context torch_c
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneResult<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::torch::Torch::ValueTensorType>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneOperand<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AlwaysSpeculatableImplTrait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicResults<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::MemRefsNormalizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::func::detail::CallOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ReturnLike<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsTerminator<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::torch::Torch::IntType>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ConstantLike<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::torch::Torch::OpTrait::AllowedInModuleInitializer<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::torch::Torch::OpTrait::AllowsTypeRefinement<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::torch::Torch::OpTrait::ReadOnly<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalLoadOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::detail::GlobalLoadOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::torch::Torch::OpTrait::HasValueSemantics<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<3>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutSpecInterface)

Features:+64bit-mode,-32bit-mode,-16bit-mode,+sse2
CPU:generic
TuneCPU:generic

Subtarget features: SSELevel 2, 3DNowLevel 0, 64bit 1
G_ADD (opcode 47): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SUB (opcode 48): 1 type index, 0 imm indices
.. opcode 48 is aliased to 47
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_MUL (opcode 49): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SDIV (opcode 50): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_UDIV (opcode 51): 1 type index, 0 imm indices
.. opcode 51 is aliased to 50
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SREM (opcode 52): 1 type index, 0 imm indices
.. opcode 52 is aliased to 50
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_UREM (opcode 53): 1 type index, 0 imm indices
.. opcode 53 is aliased to 50
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SDIVREM (opcode 54): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UDIVREM (opcode 55): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_AND (opcode 56): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_OR (opcode 57): 1 type index, 0 imm indices
.. opcode 57 is aliased to 56
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_XOR (opcode 58): 1 type index, 0 imm indices
.. opcode 58 is aliased to 56
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_IMPLICIT_DEF (opcode 59): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_PHI (opcode 60): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FRAME_INDEX (opcode 61): 1 type index, 0 imm indices
.. the first uncovered type index: 1, OK
.. the first uncovered imm index: 0, OK
G_GLOBAL_VALUE (opcode 62): 1 type index, 0 imm indices
.. opcode 62 is aliased to 61
.. the first uncovered type index: 1, OK
.. the first uncovered imm index: 0, OK
G_CONSTANT_POOL (opcode 63): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_EXTRACT (opcode 64): 2 type indices, 1 imm index
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_UNMERGE_VALUES (opcode 65): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_INSERT (opcode 66): 2 type indices, 1 imm index
.. opcode 66 is aliased to 64
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_MERGE_VALUES (opcode 67): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_BUILD_VECTOR (opcode 68): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_BUILD_VECTOR_TRUNC (opcode 69): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_CONCAT_VECTORS (opcode 70): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_PTRTOINT (opcode 71): 2 type indices, 0 imm indices
.. the first uncovered type index: 2, OK
.. the first uncovered imm index: 0, OK
G_INTTOPTR (opcode 72): 2 type indices, 0 imm indices
.. the first uncovered type index: 2, OK
.. the first uncovered imm index: 0, OK
G_BITCAST (opcode 73): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FREEZE (opcode 74): 1 type index, 0 imm indices
.. the first uncovered type index: 1, OK
.. the first uncovered imm index: 0, OK
G_CONSTANT_FOLD_BARRIER (opcode 75): 1 type index, 0 imm indices
.. opcode 75 is aliased to 74
.. the first uncovered type index: 1, OK
.. the first uncovered imm index: 0, OK
G_INTRINSIC_FPTRUNC_ROUND (opcode 76): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC_TRUNC (opcode 77): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC_ROUND (opcode 78): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC_LRINT (opcode 79): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC_ROUNDEVEN (opcode 80): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_READCYCLECOUNTER (opcode 81): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_LOAD (opcode 82): 2 type indices, 0 imm indices
.. the first uncovered type index: 2, OK
.. the first uncovered imm index: 0, OK
G_SEXTLOAD (opcode 83): 2 type indices, 0 imm indices
.. the first uncovered type index: 2, OK
.. the first uncovered imm index: 0, OK
G_ZEXTLOAD (opcode 84): 2 type indices, 0 imm indices
.. the first uncovered type index: 2, OK
.. the first uncovered imm index: 0, OK
G_INDEXED_LOAD (opcode 85): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INDEXED_SEXTLOAD (opcode 86): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INDEXED_ZEXTLOAD (opcode 87): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STORE (opcode 88): 2 type indices, 0 imm indices
.. the first uncovered type index: 2, OK
.. the first uncovered imm index: 0, OK
G_INDEXED_STORE (opcode 89): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMIC_CMPXCHG_WITH_SUCCESS (opcode 90): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMIC_CMPXCHG (opcode 91): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_XCHG (opcode 92): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_ADD (opcode 93): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_SUB (opcode 94): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_AND (opcode 95): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_NAND (opcode 96): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_OR (opcode 97): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_XOR (opcode 98): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_MAX (opcode 99): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_MIN (opcode 100): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_UMAX (opcode 101): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_UMIN (opcode 102): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_FADD (opcode 103): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_FSUB (opcode 104): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_FMAX (opcode 105): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_FMIN (opcode 106): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_UINC_WRAP (opcode 107): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ATOMICRMW_UDEC_WRAP (opcode 108): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FENCE (opcode 109): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_BRCOND (opcode 110): 1 type index, 0 imm indices
.. the first uncovered type index: 1, OK
.. the first uncovered imm index: 0, OK
G_BRINDIRECT (opcode 111): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INVOKE_REGION_START (opcode 112): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC (opcode 113): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC_W_SIDE_EFFECTS (opcode 114): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC_CONVERGENT (opcode 115): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INTRINSIC_CONVERGENT_W_SIDE_EFFECTS (opcode 116): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ANYEXT (opcode 117): 2 type indices, 0 imm indices
.. opcode 117 is aliased to 123
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_TRUNC (opcode 118): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_CONSTANT (opcode 119): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FCONSTANT (opcode 120): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_VASTART (opcode 121): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VAARG (opcode 122): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SEXT (opcode 123): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SEXT_INREG (opcode 124): 1 type index, 1 imm index
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_ZEXT (opcode 125): 2 type indices, 0 imm indices
.. opcode 125 is aliased to 123
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SHL (opcode 126): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_LSHR (opcode 127): 2 type indices, 0 imm indices
.. opcode 127 is aliased to 126
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_ASHR (opcode 128): 2 type indices, 0 imm indices
.. opcode 128 is aliased to 126
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FSHL (opcode 129): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FSHR (opcode 130): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ROTR (opcode 131): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ROTL (opcode 132): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ICMP (opcode 133): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FCMP (opcode 134): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SELECT (opcode 135): 2 type indices, 0 imm indices
.. the first uncovered type index: 2, OK
.. the first uncovered imm index: 0, OK
G_UADDO (opcode 136): 2 type indices, 0 imm indices
.. opcode 136 is aliased to 137
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_UADDE (opcode 137): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_USUBO (opcode 138): 2 type indices, 0 imm indices
.. opcode 138 is aliased to 137
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_USUBE (opcode 139): 2 type indices, 0 imm indices
.. opcode 139 is aliased to 137
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SADDO (opcode 140): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SADDE (opcode 141): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SSUBO (opcode 142): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SSUBE (opcode 143): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UMULO (opcode 144): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SMULO (opcode 145): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UMULH (opcode 146): 1 type index, 0 imm indices
.. opcode 146 is aliased to 147
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_SMULH (opcode 147): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_UADDSAT (opcode 148): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SADDSAT (opcode 149): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_USUBSAT (opcode 150): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SSUBSAT (opcode 151): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_USHLSAT (opcode 152): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SSHLSAT (opcode 153): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SMULFIX (opcode 154): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UMULFIX (opcode 155): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SMULFIXSAT (opcode 156): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UMULFIXSAT (opcode 157): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SDIVFIX (opcode 158): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UDIVFIX (opcode 159): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SDIVFIXSAT (opcode 160): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UDIVFIXSAT (opcode 161): 1 type index, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FADD (opcode 162): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FSUB (opcode 163): 1 type index, 0 imm indices
.. opcode 163 is aliased to 162
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FMUL (opcode 164): 1 type index, 0 imm indices
.. opcode 164 is aliased to 162
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FMA (opcode 165): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FMAD (opcode 166): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FDIV (opcode 167): 1 type index, 0 imm indices
.. opcode 167 is aliased to 162
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FREM (opcode 168): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FPOW (opcode 169): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FPOWI (opcode 170): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FEXP (opcode 171): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FEXP2 (opcode 172): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FEXP10 (opcode 173): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FLOG (opcode 174): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FLOG2 (opcode 175): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FLOG10 (opcode 176): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FLDEXP (opcode 177): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FFREXP (opcode 178): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FNEG (opcode 179): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FPEXT (opcode 180): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FPTRUNC (opcode 181): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FPTOSI (opcode 182): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_FPTOUI (opcode 183): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SITOFP (opcode 184): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_UITOFP (opcode 185): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FABS (opcode 186): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FCOPYSIGN (opcode 187): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_IS_FPCLASS (opcode 188): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FCANONICALIZE (opcode 189): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FMINNUM (opcode 190): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FMAXNUM (opcode 191): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FMINNUM_IEEE (opcode 192): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FMAXNUM_IEEE (opcode 193): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FMINIMUM (opcode 194): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FMAXIMUM (opcode 195): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_GET_FPMODE (opcode 196): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SET_FPMODE (opcode 197): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_RESET_FPMODE (opcode 198): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_PTR_ADD (opcode 199): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_PTRMASK (opcode 200): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SMIN (opcode 201): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SMAX (opcode 202): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UMIN (opcode 203): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UMAX (opcode 204): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ABS (opcode 205): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_LROUND (opcode 206): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_LLROUND (opcode 207): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_BR (opcode 208): 0 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_BRJT (opcode 209): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_INSERT_VECTOR_ELT (opcode 210): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_EXTRACT_VECTOR_ELT (opcode 211): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SHUFFLE_VECTOR (opcode 212): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_CTTZ (opcode 213): 2 type indices, 0 imm indices
.. opcode 213 is aliased to 214
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_CTTZ_ZERO_UNDEF (opcode 214): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_CTLZ (opcode 215): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_CTLZ_ZERO_UNDEF (opcode 216): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_CTPOP (opcode 217): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_BSWAP (opcode 218): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_BITREVERSE (opcode 219): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FCEIL (opcode 220): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FCOS (opcode 221): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FSIN (opcode 222): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FSQRT (opcode 223): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FFLOOR (opcode 224): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FRINT (opcode 225): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_FNEARBYINT (opcode 226): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_ADDRSPACE_CAST (opcode 227): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_BLOCK_ADDR (opcode 228): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_JUMP_TABLE (opcode 229): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_DYN_STACKALLOC (opcode 230): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_STACKSAVE (opcode 231): 1 type index, 0 imm indices
.. opcode 231 is aliased to 230
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_STACKRESTORE (opcode 232): 1 type index, 0 imm indices
.. opcode 232 is aliased to 230
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_STRICT_FADD (opcode 233): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STRICT_FSUB (opcode 234): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STRICT_FMUL (opcode 235): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STRICT_FDIV (opcode 236): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STRICT_FREM (opcode 237): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STRICT_FMA (opcode 238): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STRICT_FSQRT (opcode 239): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_STRICT_FLDEXP (opcode 240): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_READ_REGISTER (opcode 241): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_WRITE_REGISTER (opcode 242): 1 type index, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_MEMCPY (opcode 243): 3 type indices, 1 imm index
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_MEMCPY_INLINE (opcode 244): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_MEMMOVE (opcode 245): 3 type indices, 1 imm index
.. opcode 245 is aliased to 243
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_MEMSET (opcode 246): 3 type indices, 1 imm index
.. opcode 246 is aliased to 243
.. type index coverage check SKIPPED: user-defined predicate detected
.. imm index coverage check SKIPPED: user-defined predicate detected
G_BZERO (opcode 247): 2 type indices, 1 imm index
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_SEQ_FADD (opcode 248): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_SEQ_FMUL (opcode 249): 3 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_FADD (opcode 250): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_FMUL (opcode 251): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_FMAX (opcode 252): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_FMIN (opcode 253): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_FMAXIMUM (opcode 254): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_FMINIMUM (opcode 255): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_ADD (opcode 256): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_MUL (opcode 257): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_AND (opcode 258): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_OR (opcode 259): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_XOR (opcode 260): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_SMAX (opcode 261): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_SMIN (opcode 262): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_UMAX (opcode 263): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_VECREDUCE_UMIN (opcode 264): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_SBFX (opcode 265): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
G_UBFX (opcode 266): 2 type indices, 0 imm indices
.. type index coverage check SKIPPED: no rules defined
.. imm index coverage check SKIPPED: no rules defined
CPU : generic
Target Triple : x86_64-unknown-unknown-eabi-elf
Target Feature string : 
Data Layout : e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128
Vector Width : 16
[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtin[dialect] repeated interface registration for dialect builtinLoad new dialect in Context affine
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaStartOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineMapAccessInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaWaitOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineReadOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineWriteOpInterface)
Load new dialect in Context arm_neon
Load new dialect in Context bufferization
Load new dialect in Context memref
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CopyOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableMemOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAccessorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedDimOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OffsetSizeAndStrideOpInterface)
Load new dialect in Context tensor
Load new dialect in Context complex
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TilingInterface)
Load new dialect in Context linalg
Load new dialect in Context math
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::AggregatedOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ContractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ConvolutionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::FillOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::PartitionableLoopsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PartialReductionOpInterface)
Ignoring repeated interface registration
Ignoring repeated interface registration
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetInsertionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::AllocationOpInterface)
Load new dialect in Context chlo
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferShapedTypeOpInterface)
Load new dialect in Context flow
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::DispatchTensorType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::StreamableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::ShapeAwareOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::ClosureOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectFoldInterface)
Load new dialect in Context gpu
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::AsyncTokenType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::MMAMatrixType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseHandleType<mlir::gpu::SparseHandleKind::DnTensor>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseHandleType<mlir::gpu::SparseHandleKind::SpMat>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseHandleType<mlir::gpu::SparseHandleKind::SpGEMMOp>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::AsyncOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DeviceMappingAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::ProcessorIDInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::ProcessorCountInterface)
Load new dialect in Context hal
Load new dialect in Context scf
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ParallelCombiningOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::AffinityAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::MatchAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::AllocatorType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::BufferType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::BufferViewType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::ChannelType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::CommandBufferType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::DescriptorSetLayoutType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::DeviceType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::EventType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::ExecutableType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::FenceType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::FileType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::PipelineLayoutType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::HAL::SemaphoreType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::VMConversionDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::ProcessorTileSizeInterface)
Load new dialect in Context io_parameters
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::HALConversionDialectInterface)
Load new dialect in Context iree_codegen
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Codegen::UKernelOpInterface)
Load new dialect in Context iree_input
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Input::TiedOpInterface)
Load new dialect in Context iree_linalg_ext
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::LinalgExt::LinalgExtOp)
Load new dialect in Context llvm
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMVoidType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMPPCFP128Type)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMX86MMXType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMTokenType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMLabelType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMMetadataType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMStructType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::GetResultPtrElementType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AccessGroupOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AliasAnalysisOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::FastmathFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::BranchWeightOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SafeMemorySlotAccessOpInterface)
[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvm[dialect] repeated interface registration for dialect llvmLoad new dialect in Context ml_program
Load new dialect in Context nvgpu
Load new dialect in Context nvvm
ImplicitTypeIDRegistry::lookupOrInsert(mlir::NVVM::BasicPtxBuilderInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToLLVMPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::TargetAttrInterface)
[dialect] repeated interface registration for dialect nvvm[dialect] repeated interface registration for dialect nvvm[dialect] repeated interface registration for dialect nvvm[dialect] repeated interface registration for dialect nvvm[dialect] repeated interface registration for dialect nvvmLoad new dialect in Context pdl
Load new dialect in Context pdl_interp
Load new dialect in Context rocdl
[dialect] repeated interface registration for dialect rocdl[dialect] repeated interface registration for dialect rocdl[dialect] repeated interface registration for dialect rocdl[dialect] repeated interface registration for dialect rocdl[dialect] repeated interface registration for dialect rocdlLoad new dialect in Context shape
Load new dialect in Context spirv
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::InterfaceVarABIAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::TargetEnvAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::VerCapExtAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::ArrayType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::CooperativeMatrixType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::CooperativeMatrixNVType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::ImageType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::JointMatrixINTELType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::MatrixType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::PointerType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::RuntimeArrayType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::SampledImageType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::StructType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::QueryMinVersionInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::QueryMaxVersionInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::QueryExtensionInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::spirv::QueryCapabilityInterface)
Load new dialect in Context stablehlo
ImplicitTypeIDRegistry::lookupOrInsert(mlir::hlo::HloDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::stablehlo::TokenType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VerifiableTensorEncoding)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::hlo::BoundedAttrInterface)
Load new dialect in Context stream
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::AffinityOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::StreamableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::AsyncAccessOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::TimelineOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::SubviewEffectOpInterface)
Load new dialect in Context tm_tensor
ImplicitTypeIDRegistry::lookupOrInsert(mlir::torch::TMTensor::TMTensorOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::torch::TMTensor::ScalarLoopOpInterface)
Load new dialect in Context tosa
Load new dialect in Context quant
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::AnyQuantizedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::CalibratedQuantizedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::UniformQuantizedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::quant::UniformQuantizedPerAxisType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::tosa::TosaOp)
Load new dialect in Context transform
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::PatternDescriptorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::ConversionPatternDescriptorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TypeConverterBuilderOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::MatchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformParamTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformHandleTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformValueHandleTypeInterface)
Load new dialect in Context async
Load new dialect in Context vector
ImplicitTypeIDRegistry::lookupOrInsert(mlir::vector::MaskableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::vector::MaskingOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorTransferOpInterface)
Load new dialect in Context index
Load new dialect in Context vk
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Vulkan::TargetEnvAttr)
Load new dialect in Context vm
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::VM::BufferType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::VM::ListType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::VM::OpaqueType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::VM::RefType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::VM::VMSerializableOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::VM::VMOp)
Load new dialect in Context vmvx
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::PreservedAnalyses::AllAnalysesType)
// -----// IR Dump After mlir::iree_compiler::IREE::HAL::AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::StorageUserTrait::IsMutable<Empty>)
#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
    %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
    %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
    return %2 : tensor<3xf32>
  }
  func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> {
    %int0 = torch.constant.int 0
    %0 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
    %2 = torch.aten.mm %0, %1 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
    %int0_0 = torch.constant.int 0
    %3 = torch.aten.squeeze.dim %2, %int0_0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %4 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
    %int1 = torch.constant.int 1
    %5 = torch.aten.add.Tensor %3, %4, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
    return %5 : !torch.vtensor<[3],f32>
  }
}


// -----// IR Dump After SetStrictSymbolicShapesPass (torch-iree-set-strict-symbolic-shapes) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %2 : tensor<3xf32>
}

// -----// IR Dump After SetStrictSymbolicShapesPass (torch-iree-set-strict-symbolic-shapes) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %int0 = torch.constant.int 0
  %0 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch.aten.mm %0, %1 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %int0_0 = torch.constant.int 0
  %3 = torch.aten.squeeze.dim %2, %int0_0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias :
 //===-------------------------------------------===//
Processing operation : 'tensor<torch_c.from_builtin_tensor3'(x0x56223b0d2950f32) {
>  
  %4 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %int1 = torch.constant.int 1%0 = "tor
c  h%_5c = .ftorch.aten.add.Tensorr o%m3_, b%u4i, l%tint1i : n_tensor"!(torch%.arg0vtensor<[3],f32>),  : (tensor<!4torchx.f32vtensor<[3],f32>>, ) -> !torch.int -> !torch.!vtensor<[4],f32>torch.vtensor<[3],f32>
  

return %5 : } -> failure : !pattern failed to matchtorch
.//===-------------------------------------------===//
vtensor<[3],f32>

//===-------------------------------------------===//
}Processing operation : 'func.call'(0x56223b0d3470) {
  

%1 = "func.call"(%0) <{callee = @forward}> : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %2 = "torch_c.to_builtin_tensor"(%1) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%2) : (tensor<3xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasRecursiveMemoryEffects<Empty>)
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
** Replace : 'torch.constant.int'(0x56223b0be280)
** Modified: 'torch.aten.squeeze.dim'(0x56223b0be7c0)
** Erase   : 'torch.constant.int'(0x56223b0be280func.func)
 @main(%arg0: tensor<4xf32>) -> 
tensor<//===-------------------------------------------===//
3Processing operation : 'xtorch.constant.intf32'(>0x56223b0d2180) {
   attributes {torch.args_schema = "[1, {\22type\22: \22builtins.%t1u = p"lteo\r2c2h,. c\o2n2sctoanntte.xitn\t2"2(:) \22nul <l{\value2 = 2, \022 : chiil64d}r>e : n(_) -> spec\!2torch2.:int [{\22

typ} -> efailure\ : 2pattern failed to match2
://===-------------------------------------------===//
 
\//===-------------------------------------------===//
2Processing operation : '2torch.aten.unsqueezeb'(u0x56223b0bd670i) {
l  tins.list\22, \22context\22: \22null\22, \22ch%i2l = d"rteonr_cshp.eact\e2n2.:u n[s{q\u2e2etzyep"e(\%2arg02, :% 1n)u : l(l, \22conte!xtorcht.\vtensor<[4],f32>2, 2: n!utorchl.lint,) ->  \22chi!ltorchd.rvtensor<[1,4],f32>en_sp

ec\22: []}]}} -> ,failure  : {pattern failed to match\
2//===-------------------------------------------===//
2
t//===-------------------------------------------===//
yProcessing operation : 'putil.global.loade'(\0x56223b0bdb302) {
2  : \22builtins.dict\22, \22context\22: \22[]\2%23, =  "\u2t2iclh.iglldorbeanl_.slpoeacd\"2(2): []}]} <]{"global = , @torch.assume_strict_symbolic_shapes_params.weight, }torch.return_schema> =  : "([) -> 1, tensor<{4\x232xtf32y>pe\22: 

null, } -> \failure2 : 2pattern failed to matchc
o//===-------------------------------------------===//
n
t//===-------------------------------------------===//
eProcessing operation : 'xtorch_c.from_builtin_tensort'(\0x56223b0bdb902) {
2  : null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : %tensor<44 = x"f32t>o r->c h_c.fro!mtorch_.bvtensor<[4],f32>u
i  l%t1i = ncall_ te@nforwards(o%r0")(%3) :  (: (tensor<4x3x!f32torch>.) -> vtensor<[4],f32>) -> !!torchtorch..vtensor<[3],f32>vtensor<[4,3],f32>
  %2 = 

torch_c.to_builtin_tensor %1 :} ->  failure : pattern failed to match
//===-------------------------------------------===//
!
torch//===-------------------------------------------===//
.Processing operation : 'vtensor<[3],f32>torch.aten.mm '(->0x56223b0be170 ) {
  tensor<3xf32>
  return %2 : tensor<3xf32>
}

%5 = "torch.aten.mm"(%2, %4
)//===-------------------------------------------===//
 : Processing operation : '(func.return'(0x56223b0d3620) {
  !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32>) -> !torch."vtensor<[1,3],f32>func.r

eturn"(%2)} ->  : failure( : pattern failed to matchtensor<
3//===-------------------------------------------===//
x
f32//===-------------------------------------------===//
>Processing operation : ') -> torch.aten.squeeze.dim('()0x56223b0be7c0) {
  

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %6 = "to%r2c = h".taotrecnh._scq.uteoe_zbeu.idlitmi"n(_%t5e, n%s1o)r : "((%1) : (!torch.!vtensor<[1,3],f32>torch, .vtensor<[3],f32>) -> !tensor<torch3.xintf32) -> >

!torch} -> .failurevtensor<[3],f32> : pattern failed to match
//===-------------------------------------------===//



//===-------------------------------------------===//
Processing operation : 'func.call'(0x56223b0d3470) {
  } -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "func.call"(%0) <{callee = @forward}> : (%7! = torch".uvtensor<[4],f32>t) -> il.gl!otorchb.avtensor<[3],f32>l.lo

ad"()} -> failure : pattern failed to match
//===-------------------------------------------===//

 <//===-------------------------------------------===//
{Processing operation : 'globaltorch_c.from_builtin_tensor = '(0x56223b0d2950@) {
_params.bias  }> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//
%
0//===-------------------------------------------===//
 = Processing operation : '"torch_c.from_builtin_tensort'(o0x56223b0beda0r) {
c  h_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>%8

 = "t} -> ofailurer : cpattern failed to matchh
_//===-------------------------------------------===//
c.from_builtin_tensor"(%7) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x56223b0bef00) {
  // -----// IR Dump After BitCastQuantTensorPass (torch-iree-bitcast-quant-tensor) //----- //
mlir-asm-printer: Verifying operation: func.func
%0 = "torch.constant.int"() <{value = 1 : i64}> : () -> func.func !@torchmain.(int%arg0: 

tensor<4} -> xfailuref32 : >pattern failed to match)
 -> //===-------------------------------------------===//
tensor<
3//===-------------------------------------------===//
xProcessing operation : 'f32torch.aten.add.Tensor>'(0x56223b0ece80) {
   attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22,% 9\ = 2"2tcoorncthe.xatt\e2n2.:a d\d2.2Tneunlslo\r2"2(,% 6\, 2%28c, h%i0l)d : r(en_spec\22: ![torch{.\vtensor<[3],f32>2, 2type\22!:torch .\vtensor<[3],f32>2, 2bui!ltorcht.iintn) -> s.list\22!,torch .\vtensor<[3],f32>22cont

ext\22: \22null\2
2  ,* Pattern   : '\torch.aten.add.Tensor2 -> (2)' {
chiTrying to match "l"
dren_spec\22: [  {  \** Failure : 22tonly int scalar lhs or rhs is supportedy
pe\2"2" result :0 
n  u} -> lfailurel : ,pattern failed to match 
\} -> 2failure2 : cpattern failed to matcho
n//===-------------------------------------------===//
t
e//===-------------------------------------------===//
xProcessing operation : 'tfunc.return\'(20x56223b0d37302) {
:   null, \22children_spec\22: []}]}, {\22type\22: \22builtins."dfiucntc\.2r2e,t u\r2n2"c(o%n9t)e : x(t\22: \22[]\2!2torch,. vtensor<[3],f32>\) -> 2(2)childre

n_spec\22:} ->  failure[ : ]pattern failed to match}
]//===-------------------------------------------===//
}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %2 : tensor<3xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.func.funcvtensor<[4],f32> private

 } -> SUCCESS@ : forwardoperation marked legal by the target(
%//===-------------------------------------------===//
arg0
: //===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b0d3470) {
  !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>%1 = "func.cal attributesl {"torch.assume_strict_symbolic_shapes(}% 0{)
   <%{int1callee =  = @torch.constant.intforward }1> : (
  !%torchint0. = vtensor<[4],f32>torch.constant.int) ->  0!torch.vtensor<[3],f32>
  %

0 = } -> SUCCESS : torch.aten.unsqueeze operation marked legal by the target%
arg0//===-------------------------------------------===//
, 
%//===-------------------------------------------===//
int0Legalizing operation : ' : torch_c.to_builtin_tensor'(0x56223b0d3540) {
  !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
%  2% = _params.weight" = toutil.global.loadr ch_@c_params.weight.to_builti n:_ tentensor<s4oxr3"x(f32%>1
)   : %(1 = torch_c.from_builtin_tensor !%torch_params.weight. vtensor<[3],f32>:) ->  tensor<3tensor<x4f32x>3xf32

>} -> SUCCESS  : -> operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620!) {
torch  .vtensor<[4,3],f32>
  %2 = torch.aten.mm %0, %1 : !torch".fvtensor<[1,4],f32>u, nc.retur!ntorch".(vtensor<[4,3],f32>% -> 2) : (tensor<!3torchx.f32vtensor<[1,3],f32>>
) ->   (%)3 = 

torch.aten.squeeze.dim} -> SUCCESS  : %2operation marked legal by the target, 
%//===-------------------------------------------===//
int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %5 = torch.aten.add.Tensor %3, %4, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %5 : !torch.vtensor<[3],f32>
}

// -----// IR Dump After ConvertCustomQuantOp (torch-convert-custom-quant-op) //----- //
mlir-asm-printer: Verifying operation: func.func

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3730) {
  "ffunc.funcun c@.mainr(e%targ0u: rn"tensor<(4%x9f32)> : )( -> tensor<3xf32>!torch.vtensor<[3],f32>) -> () attributes {

torch.args_schema = "[1, {\} -> 2failure2 : tpattern failed to matchy
p//===-------------------------------------------===//
e
\//===-------------------------------------------===//
2Processing operation : '2torch.aten.add.Tensor:'( 0x56223b0ece80\) {
2  2builtins.tuple\22, \22context\22: \22null\22, \22children_s%p9e = c"\t2o2r:c h[.{a\t2e2nt.yapded\.2T2e:n s\o2r2"b(u%i6l, t%i8n, s%.0l)i : s(t\22, \22cont!etorchx.tvtensor<[3],f32>\, 22: \22nu!ltorchl.\vtensor<[3],f32>2, 2, \!2torch2.cinth) -> ildren_s!ptorche.cvtensor<[3],f32>\22: [{

\22type\22: n} -> ufailurel : lpattern failed to match,
 //===-------------------------------------------===//
\
2//===-------------------------------------------===//
2Processing operation : 'ctorch_c.from_builtin_tensoro'(n0x56223b0beda0t) {
e  xt\22: null, \22children_spec\22: []}]}, {\22type\22: %\82 = 2"btuoirlcthi_ncs..fdriocmt_\b2u2i,l t\i2n2_ctoenntseoxrt"\(2%27:)  : \(22[tensor<]3\x2f322>,) ->  \22childr!etorchn._vtensor<[3],f32>spec\2

2: []}]} -> }failure] : "pattern failed to match
, //===-------------------------------------------===//
torch.assume_strict_symbolic_shapes
, //===-------------------------------------------===//
torch.return_schemaProcessing operation : ' = util.global.load"'([0x56223b0bed001) {
,   {\22type\22: null, \22context\22: null, \22children_s%p7e = c"\u2t2i:l .[g]l}o]b"a}l .{l
oad  "%(0) = torch_c.from_builtin_tensor %arg0 < {:global  = tensor<@4_params.biasx}f32>> :  (->) ->  tensor<3xf32>!torch.vtensor<[4],f32>


  %1} ->  = failurecall :  pattern failed to match@
forward//===-------------------------------------------===//
(
%//===-------------------------------------------===//
0Processing operation : ')torch.aten.squeeze.dim'(0x56223b0be7c0) {
   : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : %6 = "!ttorcho.rvtensor<[3],f32>c h->. attensor<e3nx.f32s>q
u  eereturnz e%.2d i:m "tensor<(3%x5f32, >%
1}) : (

!torch.vtensor<[1,3],f32>, !torch.int) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.mm'(0x56223b0be170) {
  %5 = "torch.aten.mm"(%2, %4) : (!torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32>) -> !torch.vtensor<[1,3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %4 = "torch_c.from_builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.glob
a//===-------------------------------------------===//
lProcessing operation : '.torch_c.from_builtin_tensorl'(o0x56223b0d2950a) {
d  "() <{global = @_params.weight}> : () -> tensor<4x3xf32>%0 = "

torch} -> _failurec : .pattern failed to matchf
r//===-------------------------------------------===//
o
m//===-------------------------------------------===//
_Processing operation : 'btorch.aten.unsqueezeu'(i0x56223b0bd670l) {
t  in_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

%2 = } -> "failuret : opattern failed to matchr
c//===-------------------------------------------===//
h
.//===-------------------------------------------===//
aProcessing operation : 'tfunc.calle'(n0x56223b0d3470.) {
u  nsqueeze"(%arg0, %1) : (!torch.vtensor<[4],f32>%, 1 = "!ftorchu.nintc) -> .call"(!%torch0.)vtensor<[1,4],f32>

 <{callee = @forward} -> }failure> :  : pattern failed to match(
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(!0x56223b0d2180torch) {
.  vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %1 = "torch.constant.int"()% <2{ = value" = tor0ch : _ci.64t}o>_ : b(u) -> iltin!_torcht.eintnsor"(

%1} -> )failure :  : (pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : '!torch.constant.inttorch'(.0x56223b0bef00vtensor<[3],f32>) {
) ->   tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3620) {
  %0 = "torch.constant.i"nftu"n(c).return <"{(value% = 2)1 : ( : tensor<i364x}f32>> : ) -> (() -> )!

torch.int} -> failure : 

pattern failed to match
} -> //===-------------------------------------------===//
failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After DecomposeComplexOps (torch-decompose-complex-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %2 : tensor<3xf32>
}

// -----// IR Dump After BitCastQuantTensorPass (torch-iree-bitcast-quant-tensor) //----- //
mlir-asm-printer: Verifying operation: func.func

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b0d3470) {
  func.func private @forward(%arg0: %1! = torch".fvtensor<[4],f32>u)n -> c.call"(%!0torch).vtensor<[3],f32> <{callee = @forward}> : ( attributes {torch.assume_strict_symbolic_shapes!}torch .{vtensor<[4],f32>
) ->   %!int1torch = .vtensor<[3],f32>torch.constant.int 1

} -> SUCCESS : operation marked legal by the target

  //===-------------------------------------------===//
%
int0//===-------------------------------------------===//
 = Legalizing operation : 'torch_c.to_builtin_tensortorch.constant.int'( 0x56223b0d35400) {
  
  %0 = torch.aten.unsqueeze %arg0, %int0 : !torch%.2vtensor<[4],f32> = , "tor!ctorchh._intc -> .to_built!itorchn._vtensor<[1,4],f32>t
e  n%s_params.weighto = r"util.global.load( %1@)_params.weight : ( !:torch .vtensor<[3],f32>) -> tensor<tensor<43xx3f32x>f32>


  } -> SUCCESS% : 1 = operation marked legal by the target
torch_c.from_builtin_tensor//===-------------------------------------------===//
 
%//===-------------------------------------------===//
_params.weightLegalizing operation : ' func.return:'( 0x56223b0d3620) {
tensor<  4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch.aten.mm" f%u0n, c%.1r : eturn"(%!2torch). : vtensor<[1,4],f32>(, tensor<3xf32>!) -> torch(.)vtensor<[4,3],f32> -> 

} -> SUCCESS : !operation marked legal by the targettorch
.//===-------------------------------------------===//
vtensor<[1,3],f32>
  %3 = torch.aten.squeeze.dim %2, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %5 = torch.aten.add.Tensor %3, %4, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %5 : !torch.vtensor<[3],f32>
}

// -----// IR Dump After ConvertTorchToTMTensor (convert-torch-to-tmtensor) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %2 : tensor<3xf32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0bef00) {
  %0 = "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0d2180) {
  %
1//===-------------------------------------------===//
 = Legalizing operation : '"func.funct'(o0x56223b0d36a0r) {
c} -> SUCCESSh : .conoperation marked legal by the targets
t//===-------------------------------------------===//
a
n//===-------------------------------------------===//
tLegalizing operation : '.torch_c.from_builtin_tensori'(n0x56223b0d2950t) {
"  () <{value = 0 : i64}> : () -> !torch.int%0 = 

"t} -> SUCCESSo : rch_operation marked legal by the targetc
.//===-------------------------------------------===//
f
r//===-------------------------------------------===//
oLegalizing operation : 'mtorch.aten.unsqueeze_'(b0x56223b0bd670u) {
i  ltin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

%} -> SUCCESS2 :  = "operation marked legal by the targett
o//===-------------------------------------------===//
r
c//===-------------------------------------------===//
hLegalizing operation : '.func.calla'(t0x56223b0d3470e) {
n  .unsqueeze"(%arg0, %1) : (!%torch1. = vtensor<[4],f32>", func!.torchc.aintl) -> l"(%0)!torch.vtensor<[1,4],f32> <{callee = 

@forward} -> SUCCESS} : > : (operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load!'(torch0x56223b0bdb30.) {
vtensor<[4],f32>  ) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %3 = "util.global.load"()%2 = "to <r{cglobalh = _c@._params.weightt}o>_ : b(u) -> ilttensor<i4nx_3txef32n>sor"(

%1)   : * Fold {
(  !} -> FAILURE : torch.vtensor<[3],f32>) -> unable to foldtensor<
3x} -> FAILURE : f32no matched legalization pattern>
//===-------------------------------------------===//



} -> SUCCESS//===-------------------------------------------===//
 : Legalizing operation : 'torch_c.from_builtin_tensoroperation marked legal by the target'(
0x56223b0bdb90//===-------------------------------------------===//
) {

  //===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%2) : (%tensor<43 = x"f32t>o) -> r(c)h_c

.} -> SUCCESSf : rooperation marked legal by the targetm
_//===-------------------------------------------===//
builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.mm'(0x56223b0be170) {
// -----// IR Dump After   ConvertTorchToLinalg (convert-torch-to-linalg) //----- //
mlir-asm-printer: Verifying operation: func.func
%5 = "torch.aten.mm"(%2, %4) : (func.func! torch@.mainvtensor<[1,4],f32>(, %arg0: tensor<4!xtorchf32.>vtensor<[4,3],f32>)) ->  -> tensor<3xf32>!torch.vtensor<[1,3],f32>

 attributes} -> SUCCESS { : torch.args_schema = "operation marked legal by the target[
1//===-------------------------------------------===//
,
 //===-------------------------------------------===//
{Legalizing operation : '\torch.aten.squeeze.dim2'(20x56223b0be7c0t) {
y  pe\22: \22builtins.tuple\22, \22context\22: \22null\22, \22chil%d6r = e"nt_osrpcehc.\a2t2e:n .[s{q\u2e2etzyep.ed\i2m2":( %\52, 2%b1u)i : l(tins.list\22, !\torch2.2vtensor<[1,3],f32>c, ontex!ttorch\.2int2) -> : \22nul!ltorch\.2vtensor<[3],f32>2, \22c

hi} -> SUCCESSl : drenoperation marked legal by the target_
s//===-------------------------------------------===//
p
e//===-------------------------------------------===//
cLegalizing operation : '\util.global.load2'(20x56223b0bed00:) {
   [{\22type\22: null, \22context\22: null, \22children_spec\22: [%]7} = ]"}u,t i{l\.2g2ltoybpael\.2l2o:a d\"2(2)builtins.di <c{tglobal\ = 22@,_params.bias }\>2 : 2(c) -> ontetensor<x3tx\f322>2: \22[

]\2  2* Fold {
, \22  c} -> FAILURE : hunable to foldi
ld} -> FAILURE : rno matched legalization patterne
n//===-------------------------------------------===//
_
s//===-------------------------------------------===//
pLegalizing operation : 'etorch_c.from_builtin_tensorc'(\0x56223b0beda02) {
2  : []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \2%28c = h"itlodrrcehn__cs.pferco\m2_2b:u i[l]t}i]n"_}t e{n
so  r%"0( = %torch_c.from_builtin_tensor7 )% : arg0( : tensor<tensor<34xxf32f32>>) ->  -> !!torchtorch..vtensor<[3],f32>vtensor<[4],f32>
  %1

 = } -> SUCCESScall :  @operation marked legal by the targetforward
(//===-------------------------------------------===//
%
0//===-------------------------------------------===//
)Legalizing operation : 'torch.aten.add.Tensor'(0x56223b0ece80 ) {
:   (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> %tensor<93 = x"f32t>o
r  creturnh .%a2t e:n .tensor<a3dxdf32.>T
e}nso

r"(%6, %8, %0) : (!torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch
.//===-------------------------------------------===//
intLegalizing operation : ') -> func.func'(0x56223b0d36a0) {
  !* Fold {
torch.vtensor<[3],f32>  } -> FAILURE : unable to fold


} -> FAILURE : no matched legalization pattern} -> SUCCESS
 : //===-------------------------------------------===//

//===-------------------------------------------===//
operation marked legal by the targetLegalizing operation : '
torch_c.from_builtin_tensor//===-------------------------------------------===//
'(
0x56223b0d2950//===-------------------------------------------===//
) {
Legalizing operation : '  func.return'(0x56223b0d3730) {
  %0 = "torch_c.from"_fbuunicl.trient_utrenn"s(o%r9")( : %(arg0) : (tensor<4x!f32torch>.) -> vtensor<[3],f32>) -> ()!torch.vtensor<[4],f32>

} -> SUCCESS

 : } -> SUCCESS : operation marked legal by the targetoperation marked legal by the target

//===-------------------------------------------===//
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b0d3470) {
  %1 = "func.call"(%0) <{callee = @forward}> : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  // -----// IR Dump After ConvertCustomQuantOp (torch-convert-custom-quant-op) //----- //
mlir-asm-printer: Verifying operation: func.func%
2 = "torch_c.to_builtin_tensor"(%1) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%2) : (tensor<3xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %0 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch.aten.mm %0, %1 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %3 = torch.aten.squeeze.dim %2, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %5 = torch.aten.add.Tensor %3, %4, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %5 : !torch.vtensor<[3],f32>
}

// -----// IR Dump After ConvertTorchToSCF (convert-torch-to-scf) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %2 : tensor<3xf32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b0d3470) {
  %1 = "func.call"(%0) <{callee = @forward}> : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %2 = "torch_c.to_builtin_tensor"(%1) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%2) : (tensor<
3//===-------------------------------------------===//
xProcessing operation : 'f32torch.constant.int>'() -> 0x56223b0bef00() {
)  

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
%0 = "torch.constant.in// -----// IR Dump After tConvertTorchToArith" ((convert-torch-to-arith)) //----- //
mlir-asm-printer: Verifying operation: func.func
 <{value = 1 : i64}> : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x56223b0d2180) {
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32>%1 = " attributest {otorch.args_schemar = c"h[.1c,o n{s\t2a2ntty.pien\t2"2(:) \22buil <t{ivaluen = s.0tu : plei\642}2>, :  (\) -> 22conte!xtorcht.\int22: \

22n} -> ufailurel : lpattern failed to match\
2//===-------------------------------------------===//
2
,//===-------------------------------------------===//
 Processing operation : '\torch.aten.unsqueeze2'(20x56223b0bd670c) {
h  ildren_spec\22: [{\22type\22: \22builtins.list\22, \22context\22:% 2\ = 2"2tnourlclh\.2a2t,e n\.2u2ncshqiulederzeen"_(s%parg0e, c%\12)2 : :( [{\22type\22: n!utorchl.lvtensor<[4],f32>,,  \22!ctorcho.nintt) -> ext\22: nu!ltorchl.,vtensor<[1,4],f32> \22ch

ildren_spec\22} -> :failure  : [pattern failed to match]
}//===-------------------------------------------===//
]
}//===-------------------------------------------===//
,Processing operation : ' util.global.load{'(\0x56223b0bdb302) {
2  type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children%_3s = p"euct\i2l2.:g l[o]b}a]l}.]l"oa, dtorch.assume_strict_symbolic_shapes", (torch.return_schema) = "[1, {\2 <2{tglobaly = pe@\_params.weight2}2>: :  (n) -> ulltensor<,4 x\32x2f32c>ontext\

22: n} -> ufailurel : lpattern failed to match,
 //===-------------------------------------------===//
\
2//===-------------------------------------------===//
2Processing operation : 'ctorch_c.from_builtin_tensorh'(i0x56223b0bdb90l) {
d  ren_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> %!4torch = ."vtensor<[4],f32>t
o  r%c1h = _callc .@fforwardr(o%m0_)built i:n _(tenso!rtorch".(vtensor<[4],f32>%)3 -> ) : (!torchtensor<.4vtensor<[3],f32>x
3  x%f322> = ) -> torch_c.to_builtin_tensor %1 : !!torchtorch..vtensor<[4,3],f32>vtensor<[3],f32> -> tensor<

3xf32>
  } -> returnfailure  : %pattern failed to match2
 //===-------------------------------------------===//
:
 //===-------------------------------------------===//
tensor<Processing operation : '3torch.aten.mmx'(f320x56223b0be170>) {

  }

%5 = "torch.aten.mm"(%2, %4) : (!torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32>) -> !torch.vtensor<[1,3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.squeeze.dim'(0x56223b0be7c0) {
  %6 = "torch.aten.squeeze.dim"(%5, %1) : (!torch.vtensor<[1,3],f32>, !torch.int) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %7 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %8 = "torch_c.from_builtin_tensor"(%7) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.aten.add.Tensor'(0x56223b0ece80) {
  %9 = "torch.aten.add.Tensor"(%6, %8, %0) : (!torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%9) : (!torch.vtensor<[3],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After DecomposeComplexOps (torch-decompose-complex-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %0 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch.aten.mm %0, %1 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %3 = torch.aten.squeeze.dim %2, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %5 = torch.aten.add.Tensor %3, %4, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %5 : !torch.vtensor<[3],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0bef00) {
  %0 = "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0d2180) {
  %1 = "torch.constant.int"() <{value = 0 : i64}> : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.unsqueeze'(0x56223b0bd670) {
  %2 = "torch.aten.unsqueeze"(%arg0, %1) : (!torch.vtensor<[4],f32>, !torch.int) -> !torch.vtensor<[1,4],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %4 = "torch_c.from_builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.mm'(0x56223b0be170) {
  %5 = "torch.aten.mm"(%2, %4) : (!torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32>) -> !torch.vtensor<[1,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.squeeze.dim'(0x56223b0be7c0) {
  %6 = "torch.aten.squeeze.dim"(%5, %1) : (!torch.vtensor<[1,3],f32>, !torch.int) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %7 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %8 = "torch_c.from_builtin_tensor"(%7) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.add.Tensor'(0x56223b0ece80) {
  %9 = "torch.aten.add.Tensor"(%6, %8, %0) : (!torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%9) : (!torch.vtensor<[3],f32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ConvertTorchToTMTensor (convert-torch-to-tmtensor) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %0 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch.aten.mm %0, %1 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %3 = torch.aten.squeeze.dim %2, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %5 = torch.aten.add.Tensor %3, %4, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %5 : !torch.vtensor<[3],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0bef00) {
  %0 = "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenScalarToTensorLike"
    ** Failure : not a supported Scalar to Tensor like op
"(anonymous namespace)::ConvertAtenScalarToTensorLike" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0d2180) {
  %1 = "torch.constant.int"() <{value = 0 : i64}> : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenScalarToTensorLike"
    ** Failure : not a supported Scalar to Tensor like op
"(anonymous namespace)::ConvertAtenScalarToTensorLike" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.unsqueeze'(0x56223b0bd670) {
  %2 = "torch.aten.unsqueeze"(%arg0, %1) : (!torch.vtensor<[4],f32>, !torch.int) -> !torch.vtensor<[1,4],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.aten.unsqueeze -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenUnsqueezeOp"
    ** Insert  : 'tensor.expand_shape'(0x7f71f0008560)
    ** Replace : 'torch.aten.unsqueeze'(0x56223b0bd670)
"(anonymous namespace)::ConvertAtenUnsqueezeOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
      %4 = "tensor.expand_shape"(%0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[4],f32> to tensor<4xf32>
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %1 = builtin.unrealized_conversion_cast %int0 : !torch.int to i64
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %2 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %3 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %4 = torch.aten.mm %2, %3 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %5 = torch.aten.squeeze.dim %4, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch.aten.add.Tensor %5, %6, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %7 : !torch.vtensor<[3],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %6 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'util.global.load -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenScalarToTensorLike"
    ** Failure : not a supported Scalar to Tensor like op
"(anonymous namespace)::ConvertAtenScalarToTensorLike" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'util.global.load -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'util.global.load -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %7 = "torch_c.from_builtin_tensor"(%6) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.mm'(0x56223b0be170) {
  %8 = "torch.aten.mm"(%5, %7) : (!torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32>) -> !torch.vtensor<[1,3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.aten.mm -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenMmOp"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::ConstantOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface::Trait<Empty>)
    ** Insert  : 'arith.constant'(0x7f71f00150c0)
    ** Insert  : 'tensor.dim'(0x7f71f0015130)
    ** Insert  : 'arith.constant'(0x7f71f00151e0)
    ** Insert  : 'tensor.dim'(0x7f71f0015250)
    ** Insert  : 'tensor.empty'(0x7f71f0015780)
    ** Insert  : 'arith.constant'(0x7f71f00160c0)
    ** Insert  : 'linalg.fill'(0x7f71f0016a30)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::MulFOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::AddFOpGenericAdaptorBase::Properties)
    ** Insert  : 'linalg.matmul'(0x7f71f0017a70)
    ** Insert  : 'tensor.cast'(0x7f71f0017b60)
    ** Replace : 'torch.aten.mm'(0x56223b0be170)
"(anonymous namespace)::ConvertAtenMmOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f00150c0) {
      %9 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x7f71f0015130) {
      %10 = "tensor.dim"(%4, %9) : (tensor<1x4xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f00151e0) {
      %11 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.dim'(0x7f71f0015250) {
      %12 = "tensor.dim"(%8, %11) : (tensor<4x3xf32>, index) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x7f71f0015780) {
      %13 = "tensor.empty"(%10, %12) : (index, index) -> tensor<?x?xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
      %14 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.fill'(0x7f71f0016a30) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.matmul'(0x7f71f0017a70) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x7f71f0017b60) {
      %17 = "tensor.cast"(%16) : (tensor<?x?xf32>) -> tensor<1x3xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::linalg::YieldOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AttrSizedOperandSegments<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ContractionOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::FillOpInterface::Trait<Empty>)
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[4],f32> to tensor<4xf32>
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %1 = builtin.unrealized_conversion_cast %int0 : !torch.int to i64
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %2 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %3 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %4 = builtin.unrealized_conversion_cast %3 : !torch.vtensor<[4,3],f32> to tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %expanded, %c0 : tensor<1x4xf32>
  %c1 = arith.constant 1 : index
  %dim_0 = tensor.dim %4, %c1 : tensor<4x3xf32>
  %5 = tensor.empty(%dim, %dim_0) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %7 = linalg.matmul ins(%expanded, %4 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%6 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %7 : tensor<?x?xf32> to tensor<1x3xf32>
  %8 = torch.aten.mm %2, %3 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %9 = torch.aten.squeeze.dim %8, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %11 = torch.aten.add.Tensor %9, %10, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %11 : !torch.vtensor<[3],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.squeeze.dim'(0x56223b0be7c0) {
  %19 = "torch.aten.squeeze.dim"(%18, %2) : (!torch.vtensor<[1,3],f32>, !torch.int) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.aten.squeeze.dim -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenSqueezeDimOp"
    ** Insert  : 'tensor.collapse_shape'(0x7f71f001e6f0)
    ** Replace : 'torch.aten.squeeze.dim'(0x56223b0be7c0)
"(anonymous namespace)::ConvertAtenSqueezeDimOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
      %19 = "tensor.collapse_shape"(%17) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[4],f32> to tensor<4xf32>
  %int1 = torch.constant.int 1
  %int0 = torch.constant.int 0
  %1 = builtin.unrealized_conversion_cast %int0 : !torch.int to i64
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %2 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %3 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %4 = builtin.unrealized_conversion_cast %3 : !torch.vtensor<[4,3],f32> to tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %expanded, %c0 : tensor<1x4xf32>
  %c1 = arith.constant 1 : index
  %dim_0 = tensor.dim %4, %c1 : tensor<4x3xf32>
  %5 = tensor.empty(%dim, %dim_0) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %7 = linalg.matmul ins(%expanded, %4 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%6 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %7 : tensor<?x?xf32> to tensor<1x3xf32>
  %8 = torch.aten.mm %2, %3 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %9 = torch.aten.squeeze.dim %8, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %11 = torch.aten.add.Tensor %9, %10, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %11 : !torch.vtensor<[3],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %21 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'util.global.load -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenScalarToTensorLike"
    ** Failure : not a supported Scalar to Tensor like op
"(anonymous namespace)::ConvertAtenScalarToTensorLike" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'util.global.load -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Failure : not a supported elementwise op
"(anonymous namespace)::ConvertElementwiseOp" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'util.global.load -> ()' {
Trying to match "(anonymous namespace)::ConvertReductionOp"
    ** Failure : not a supported reduce op
"(anonymous namespace)::ConvertReductionOp" result 0
  } -> FAILURE : pattern failed to match
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %22 = "torch_c.from_builtin_tensor"(%21) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.aten.add.Tensor'(0x56223b0ece80) {
  %23 = "torch.aten.add.Tensor"(%20, %22, %1) : (!torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.aten.add.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertAtenScalarToTensorLike"
    ** Failure : not a supported Scalar to Tensor like op
"(anonymous namespace)::ConvertAtenScalarToTensorLike" result 0
  } -> FAILURE : pattern failed to match

  * Pattern : 'torch.aten.add.Tensor -> ()' {
Trying to match "(anonymous namespace)::ConvertElementwiseOp"
    ** Insert  : 'arith.constant'(0x7f71f001e810)
    ** Insert  : 'arith.constant'(0x7f71f001e880)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::IndexType>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedDimOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<Empty>)
    ** Insert  : 'arith.constant'(0x7f71f001e9a0)
    ** Insert  : 'arith.constant'(0x7f71f0016130)
    ** Insert  : 'arith.constant'(0x7f71f001eaf0)
    ** Insert  : 'tensor.empty'(0x7f71f001eb60)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::detail::GenericOpGenericAdaptorBase::Properties)
    ** Insert  : 'arith.sitofp'(0x7f71f001fdd0)
    ** Insert  : 'arith.mulf'(0x7f71f001e8f0)
    ** Insert  : 'arith.addf'(0x7f71f001fe60)
    ** Insert  : 'linalg.yield'(0x7f71f001ff00)
    ** Insert  : 'linalg.generic'(0x7f71f0006060)
    ** Insert  : 'tensor.cast'(0x7f71f001ff90)
    ** Replace : 'torch.aten.add.Tensor'(0x56223b0ece80)
"(anonymous namespace)::ConvertElementwiseOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f001e810) {
      %25 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f001e880) {
      %26 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f001e9a0) {
      %27 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f0016130) {
      %28 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f001eaf0) {
      %29 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
      %30 = "tensor.empty"() : () -> tensor<3xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.sitofp'(0x7f71f001fdd0) {
      %34 = "arith.sitofp"(%2) : (i64) -> f32

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.mulf'(0x7f71f001e8f0) {
      %35 = "arith.mulf"(%arg2, %34) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
      %36 = "arith.addf"(%arg1, %35) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
      "linalg.yield"(%36) : (f32) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x7f71f001ff90) {
      %32 = "tensor.cast"(%31) : (tensor<3xf32>) -> tensor<3xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = builtin.unrealized_conversion_cast %arg0 : !torch.vtensor<[4],f32> to tensor<4xf32>
  %int1 = torch.constant.int 1
  %1 = builtin.unrealized_conversion_cast %int1 : !torch.int to i64
  %int0 = torch.constant.int 0
  %2 = builtin.unrealized_conversion_cast %int0 : !torch.int to i64
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %3 = torch.aten.unsqueeze %arg0, %int0 : !torch.vtensor<[4],f32>, !torch.int -> !torch.vtensor<[1,4],f32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %4 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %5 = builtin.unrealized_conversion_cast %4 : !torch.vtensor<[4,3],f32> to tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %expanded, %c0 : tensor<1x4xf32>
  %c1 = arith.constant 1 : index
  %dim_0 = tensor.dim %5, %c1 : tensor<4x3xf32>
  %6 = tensor.empty(%dim, %dim_0) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %8 = linalg.matmul ins(%expanded, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%7 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %8 : tensor<?x?xf32> to tensor<1x3xf32>
  %9 = torch.aten.mm %3, %4 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[4,3],f32> -> !torch.vtensor<[1,3],f32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %10 = torch.aten.squeeze.dim %9, %int0 : !torch.vtensor<[1,3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = builtin.unrealized_conversion_cast %11 : !torch.vtensor<[3],f32> to tensor<3xf32>
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c0_3 = arith.constant 0 : index
  %c3_4 = arith.constant 3 : index
  %13 = tensor.empty() : tensor<3xf32>
  %14 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %12 : tensor<3xf32>, tensor<3xf32>) outs(%13 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %16 = arith.sitofp %1 : i64 to f32
    %17 = arith.mulf %in_6, %16 : f32
    %18 = arith.addf %in, %17 : f32
    linalg.yield %18 : f32
  } -> tensor<3xf32>
  %cast_5 = tensor.cast %14 : tensor<3xf32> to tensor<3xf32>
  %15 = torch.aten.add.Tensor %10, %11, %int1 : !torch.vtensor<[3],f32>, !torch.vtensor<[3],f32>, !torch.int -> !torch.vtensor<[3],f32>
  return %15 : !torch.vtensor<[3],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%33) : (!torch.vtensor<[3],f32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
** Insert  : 'torch_c.to_i64'(0x7f71f00216e0)
** Insert  : 'torch_c.to_builtin_tensor'(0x7f71f0021770)
** Insert  : 'torch_c.to_builtin_tensor'(0x7f71f0023020)
** Insert  : 'torch_c.to_builtin_tensor'(0x7f71f00230b0)
** Insert  : 'torch_c.from_builtin_tensor'(0x7f71f0023140)
// -----// IR Dump After ConvertTorchToLinalg (convert-torch-to-linalg) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %int1 = torch.constant.int 1
  %1 = torch_c.to_i64 %int1
  %int0 = torch.constant.int 0
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %expanded, %c0 : tensor<1x4xf32>
  %c1 = arith.constant 1 : index
  %dim_0 = tensor.dim %3, %c1 : tensor<4x3xf32>
  %4 = tensor.empty(%dim, %dim_0) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %6 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c3 = arith.constant 3 : index
  %c0_3 = arith.constant 0 : index
  %c3_4 = arith.constant 3 : index
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %12 = arith.sitofp %1 : i64 to f32
    %13 = arith.mulf %in_6, %12 : f32
    %14 = arith.addf %in, %13 : f32
    linalg.yield %14 : f32
  } -> tensor<3xf32>
  %cast_5 = tensor.cast %10 : tensor<3xf32> to tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %cast_5 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %11 : !torch.vtensor<[3],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f00230b0) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0bef00) {
  %1 = "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_i64'(0x7f71f00216e0) {
  %2 = "torch_c.to_i64"(%1) : (!torch.int) -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0d2180) {
  %3 = "torch.constant.int"() <{value = 0 : i64}> : () -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %5 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %6 = "torch_c.from_builtin_tensor"(%5) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %7 = "torch_c.to_builtin_tensor"(%6) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00150c0) {
  %8 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x7f71f0015130) {
  %9 = "tensor.dim"(%4, %8) : (tensor<1x4xf32>, index) -> index

  * Fold {
    ** Insert  : 'arith.constant'(0x7f71f0009910)
    ** Replace : 'tensor.dim'(0x7f71f0015130)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f0009910) {
      %9 = "arith.constant"() <{value = 1 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00151e0) {
  %11 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.dim'(0x7f71f0015250) {
  %12 = "tensor.dim"(%7, %11) : (tensor<4x3xf32>, index) -> index

  * Fold {
    ** Insert  : 'arith.constant'(0x7f71f0021510)
    ** Replace : 'tensor.dim'(0x7f71f0015250)

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f0021510) {
      %12 = "arith.constant"() <{value = 3 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f0015780) {
  %14 = "tensor.empty"(%10, %13) : (index, index) -> tensor<?x?xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %15 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x7f71f0016a30) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f000a430) {
  "linalg.yield"(%arg1) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0017a70) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0017890) {
  %32 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0017940) {
  %33 = "arith.addf"(%arg3, %32) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00179e0) {
  "linalg.yield"(%33) : (f32) -> ()

  * Fold {
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsCommutative<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultType<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Elementwise<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Scalarizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Vectorizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Tensorizable<Empty>)
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x7f71f0017b60) {
  %18 = "tensor.cast"(%17) : (tensor<?x?xf32>) -> tensor<1x3xf32>

  * Fold {
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface::Trait<Empty>)
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %19 = "tensor.collapse_shape"(%18) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %20 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %21 = "torch_c.from_builtin_tensor"(%20) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %22 = "torch_c.to_builtin_tensor"(%21) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e810) {
  %23 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e880) {
  %24 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e9a0) {
  %25 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0016130) {
  %26 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001eaf0) {
  %27 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %28 = "tensor.empty"() : () -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.sitofp'(0x7f71f001fdd0) {
  %32 = "arith.sitofp"(%2) : (i64) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f001e8f0) {
  %33 = "arith.mulf"(%arg2, %32) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %34 = "arith.addf"(%arg1, %33) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%34) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x7f71f001ff90) {
  %30 = "tensor.cast"(%29) : (tensor<3xf32>) -> tensor<3xf32>

  * Fold {
    ** Replace : 'tensor.cast'(0x7f71f001ff90)
  } -> SUCCESS
} -> SUCCESS : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %31 = "torch_c.from_builtin_tensor"(%30) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%31) : (!torch.vtensor<[3],f32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
// -----// IR Dump After ConvertTorchToSCF (convert-torch-to-scf) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %int1 = torch.constant.int 1
  %1 = torch_c.to_i64 %int1
  %int0 = torch.constant.int 0
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c1_0 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %4 = tensor.empty(%c1, %c3) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %6 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c3_3 = arith.constant 3 : index
  %c0_4 = arith.constant 0 : index
  %c3_5 = arith.constant 3 : index
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %12 = arith.sitofp %1 : i64 to f32
    %13 = arith.mulf %in_6, %12 : f32
    %14 = arith.addf %in, %13 : f32
    linalg.yield %14 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %11 : !torch.vtensor<[3],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f00230b0) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0bef00) {
  %1 = "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertTorchConstantIntOp"
    ** Insert  : 'arith.constant'(0x7f71f000a6a0)
    ** Replace : 'torch.constant.int'(0x56223b0bef00)
"(anonymous namespace)::ConvertTorchConstantIntOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f000a6a0) {
      %1 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %c1_i64 = arith.constant 1 : i64
  %int1 = torch.constant.int 1
  %1 = torch_c.to_i64 %int1
  %int0 = torch.constant.int 0
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c1_0 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %4 = tensor.empty(%c1, %c3) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %6 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c3_3 = arith.constant 3 : index
  %c0_4 = arith.constant 0 : index
  %c3_5 = arith.constant 3 : index
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %12 = arith.sitofp %1 : i64 to f32
    %13 = arith.mulf %in_6, %12 : f32
    %14 = arith.addf %in, %13 : f32
    linalg.yield %14 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %11 : !torch.vtensor<[3],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_i64'(0x7f71f00216e0) {
  %3 = "torch_c.to_i64"(%2) : (!torch.int) -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch.constant.int'(0x56223b0d2180) {
  %4 = "torch.constant.int"() <{value = 0 : i64}> : () -> !torch.int

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch.constant.int -> ()' {
Trying to match "(anonymous namespace)::ConvertTorchConstantIntOp"
    ** Insert  : 'arith.constant'(0x7f71f00092b0)
    ** Replace : 'torch.constant.int'(0x56223b0d2180)
"(anonymous namespace)::ConvertTorchConstantIntOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x7f71f00092b0) {
      %4 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %c1_i64 = arith.constant 1 : i64
  %int1 = torch.constant.int 1
  %1 = torch_c.to_i64 %int1
  %c0_i64 = arith.constant 0 : i64
  %int0 = torch.constant.int 0
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c1_0 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %4 = tensor.empty(%c1, %c3) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %6 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c3_3 = arith.constant 3 : index
  %c0_4 = arith.constant 0 : index
  %c3_5 = arith.constant 3 : index
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %12 = arith.sitofp %1 : i64 to f32
    %13 = arith.mulf %in_6, %12 : f32
    %14 = arith.addf %in, %13 : f32
    linalg.yield %14 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %11 : !torch.vtensor<[3],f32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %6 = "tensor.expand_shape"(%0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %8 = "torch_c.from_builtin_tensor"(%7) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %9 = "torch_c.to_builtin_tensor"(%8) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00150c0) {
  %10 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0009910) {
  %11 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00151e0) {
  %12 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0021510) {
  %13 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f0015780) {
  %14 = "tensor.empty"(%11, %13) : (index, index) -> tensor<?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %15 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x7f71f0016a30) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f000a430) {
  "linalg.yield"(%arg1) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0017a70) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0017890) {
  %31 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0017940) {
  %32 = "arith.addf"(%arg3, %31) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00179e0) {
  "linalg.yield"(%32) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x7f71f0017b60) {
  %18 = "tensor.cast"(%17) : (tensor<?x?xf32>) -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %19 = "tensor.collapse_shape"(%18) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %20 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %21 = "torch_c.from_builtin_tensor"(%20) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %22 = "torch_c.to_builtin_tensor"(%21) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e810) {
  %23 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e880) {
  %24 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e9a0) {
  %25 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0016130) {
  %26 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001eaf0) {
  %27 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %28 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.sitofp'(0x7f71f001fdd0) {
  %31 = "arith.sitofp"(%3) : (i64) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f001e8f0) {
  %32 = "arith.mulf"(%arg2, %31) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %33 = "arith.addf"(%arg1, %32) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%33) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %30 = "torch_c.from_builtin_tensor"(%29) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%30) : (!torch.vtensor<[3],f32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
** Insert  : 'torch_c.from_i64'(0x7f71f000ae40)
// -----// IR Dump After ConvertTorchToArith (convert-torch-to-arith) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %c1_i64 = arith.constant 1 : i64
  %1 = torch_c.from_i64 %c1_i64
  %2 = torch_c.to_i64 %1
  %c0_i64 = arith.constant 0 : i64
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %3 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %4 = torch_c.to_builtin_tensor %3 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c1_0 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %5 = tensor.empty(%c1, %c3) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %7 = linalg.matmul ins(%expanded, %4 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%6 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %7 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %8 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %9 = torch_c.to_builtin_tensor %8 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c3_3 = arith.constant 3 : index
  %c0_4 = arith.constant 0 : index
  %c3_5 = arith.constant 3 : index
  %10 = tensor.empty() : tensor<3xf32>
  %11 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %9 : tensor<3xf32>, tensor<3xf32>) outs(%10 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %13 = arith.sitofp %2 : i64 to f32
    %14 = arith.mulf %in_6, %13 : f32
    %15 = arith.addf %in, %14 : f32
    linalg.yield %15 : f32
  } -> tensor<3xf32>
  %12 = torch_c.from_builtin_tensor %11 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %12 : !torch.vtensor<[3],f32>
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultShape<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ml_program::detail::GlobalOpGenericAdaptorBase::Properties)

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b0d3470) {
  %1 = "func.call"(%0) <{callee = @forward}> : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %2 = "torch_c.to_builtin_tensor"(%1) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%2) : (tensor<3xf32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f00230b0) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f000a6a0) {
  %1 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_i64'(0x7f71f000ae40) {
  %2 = "torch_c.from_i64"(%1) : (i64) -> !torch.int

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_i64'(0x7f71f00216e0) {
  %3 = "torch_c.to_i64"(%2) : (!torch.int) -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00092b0) {
  %4 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %5 = "tensor.expand_shape"(%0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %6 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %7 = "torch_c.from_builtin_tensor"(%6) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %8 = "torch_c.to_builtin_tensor"(%7) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00150c0) {
  %9 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0009910) {
  %10 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00151e0) {
  %11 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0021510) {
  %12 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f0015780) {
  %13 = "tensor.empty"(%10, %12) : (index, index) -> tensor<?x?xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %14 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x7f71f0016a30) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f000a430) {
  "linalg.yield"(%arg1) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0017a70) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0017890) {
  %30 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0017940) {
  %31 = "arith.addf"(%arg3, %30) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00179e0) {
  "linalg.yield"(%31) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x7f71f0017b60) {
  %17 = "tensor.cast"(%16) : (tensor<?x?xf32>) -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %18 = "tensor.collapse_shape"(%17) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %19 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %20 = "torch_c.from_builtin_tensor"(%19) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %21 = "torch_c.to_builtin_tensor"(%20) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e810) {
  %22 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e880) {
  %23 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e9a0) {
  %24 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0016130) {
  %25 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001eaf0) {
  %26 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %27 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.sitofp'(0x7f71f001fdd0) {
  %30 = "arith.sitofp"(%3) : (i64) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f001e8f0) {
  %31 = "arith.mulf"(%arg2, %30) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %32 = "arith.addf"(%arg1, %31) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%32) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %29 = "torch_c.from_builtin_tensor"(%28) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%29) : (!torch.vtensor<[3],f32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
// -----// IR Dump After ConvertTorchConversionToMLProgram (convert-torch-conversion-to-mlprogram) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface::Trait<Empty>)
affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  ml_program.global private mutable @global_seed(dense<0> : tensor<i64>) : tensor<i64>
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
    %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
    %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
    return %2 : tensor<3xf32>
  }
  func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
    %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
    %c1_i64 = arith.constant 1 : i64
    %1 = torch_c.from_i64 %c1_i64
    %2 = torch_c.to_i64 %1
    %c0_i64 = arith.constant 0 : i64
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %3 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
    %4 = torch_c.to_builtin_tensor %3 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c1_0 = arith.constant 1 : index
    %c3 = arith.constant 3 : index
    %5 = tensor.empty(%c1, %c3) : tensor<?x?xf32>
    %cst = arith.constant 0.000000e+00 : f32
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
    %7 = linalg.matmul ins(%expanded, %4 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%6 : tensor<?x?xf32>) -> tensor<?x?xf32>
    %cast = tensor.cast %7 : tensor<?x?xf32> to tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %8 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
    %9 = torch_c.to_builtin_tensor %8 : !torch.vtensor<[3],f32> -> tensor<3xf32>
    %c1_1 = arith.constant 1 : index
    %c0_2 = arith.constant 0 : index
    %c3_3 = arith.constant 3 : index
    %c0_4 = arith.constant 0 : index
    %c3_5 = arith.constant 3 : index
    %10 = tensor.empty() : tensor<3xf32>
    %11 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %9 : tensor<3xf32>, tensor<3xf32>) outs(%10 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %13 = arith.sitofp %2 : i64 to f32
      %14 = arith.mulf %in_6, %13 : f32
      %15 = arith.addf %in, %14 : f32
      linalg.yield %15 : f32
    } -> tensor<3xf32>
    %12 = torch_c.from_builtin_tensor %11 : tensor<3xf32> -> !torch.vtensor<[3],f32>
    return %12 : !torch.vtensor<[3],f32>
  }
}




//===-------------------------------------------===//
//===-------------------------------------------===//
Legalizing operation : 'Legalizing operation : 'func.funcfunc.func'('(0x56223b0d36a00x56223b0bcbc0) {
) {
    * Fold {
* Fold {
  } -> FAILURE : unable to fold  
} -> FAILURE : } -> FAILURE : no matched legalization patternunable to fold

//===-------------------------------------------===//

} -> FAILURE : //===-------------------------------------------===//
no matched legalization patternLegalizing operation : '
torch_c.to_builtin_tensor//===-------------------------------------------===//
'(
0x7f71f00230b0//===-------------------------------------------===//
) {
Legalizing operation : '  torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %0 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[4],f32>) -> tensor<%40x = f32">torc

h_  c* Fold {
.fr  o} -> FAILURE : munable to fold_
b} -> FAILURE : uno matched legalization patterni
l//===-------------------------------------------===//
t
i//===-------------------------------------------===//
nLegalizing operation : '_arith.constantt'(e0x7f71f000a6a0n) {
s  or"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>%1 = "arit

h.co  n* Fold {
stant"()  } -> FAILURE : unable to fold
 <} -> FAILURE : {no matched legalization patternvalue
 = //===-------------------------------------------===//
1
 : //===-------------------------------------------===//
iLegalizing operation : '64func.call}'(>0x56223b0d3470 : ) {
(  ) -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_i64'(0x7f71f000ae40) {
  %2 = "torch_c.from_i64"(%1) : (%i164 = ) -> "fu!ntorchc..intcal

l  "* Fold {
(%0)ImplicitTypeIDRegistry::lookupOrInsert(mlir::torch::Torch::detail::ConstantIntOpGenericAdaptorBase::Properties)
 <{callee = @  forward  ** Insert  : '}torch.constant.int>'( : 0x7f71f0005970()
    ** Replace : 'torch_c.from_i64'(0x7f71f000ae40)

!  torch  .//===-------------------------------------------===//
vtensor<[4],f32>  ) ->   Legalizing operation : 'torch.constant.int'(0x7f71f0005970) {
      !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(%0x56223b0d35402) {
 =   "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

      * Fold {
      } -> FAILURE : unable to fold
    } -> FAILURE : no matched legalization pattern
    //===-------------------------------------------===//
  } -> FAILURE : failed to legalize generated constant 'torch.constant.int'
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_i64'(0x7f71f00216e0) {
  %2 = "torch_c.to_builtin_tensor"(%1) : (%3 = "torch_c.to_!itorch6.4vtensor<[3],f32>") -> (%2tensor<)3 : x(f32>!torch.int) -> i64



    * Fold {
* Fold {
  } -> FAILURE :   unable to fold} -> FAILURE : 
unable to fold} -> FAILURE : 
no matched legalization pattern
} -> FAILURE : //===-------------------------------------------===//
no matched legalization pattern

//===-------------------------------------------===//
//===-------------------------------------------===//
Legalizing operation : 'arith.constant
'(//===-------------------------------------------===//
0x7f71f00092b0Legalizing operation : ') {
func.return  '(0x56223b0d3620) {
  %4 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> SUCCESS : "operation marked legal by the targetf
u//===-------------------------------------------===//
n
c//===-------------------------------------------===//
.Legalizing operation : 'rtensor.expand_shapee'(t0x7f71f0008560u) {
r  n"(%2) : (tensor<3xf32>) -> ()

  * Fold {
  %} -> FAILURE : 5unable to fold = 
"t} -> FAILURE : eno matched legalization patternn
s//===-------------------------------------------===//
or.expand_shape"(%0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  // -----// IR Dump After ExpandOps (memref-expand) //----- //
mlir-asm-printer: Verifying operation: func.func
%6 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %7 = "torch_c.from_builfunc.functin_ t@emainnso(r"%(arg0%: 6) : (tensor<tensor<44xx3f32x>f32)> -> ) -> tensor<3x!f32torch>.vtensor<[4,3],f32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor attributes'( {0x7f71f0023020torch.args_schema) {
 =   "[1, {\22type\22: \22builtins.tuple\22,% 8\ = 2"2tcoorncthe_xct.\t2o2_:b u\i2l2tniunl_lt\e2n2s,o r\"2(2%c7h)i : l(dren_!storchp.evtensor<[4,3],f32>c) -> \tensor<242x:3 x[f32{>\22

t  y* Fold {
pe\  2} -> FAILURE : 2unable to fold:
 } -> FAILURE : \no matched legalization pattern2
2//===-------------------------------------------===//
b
u//===-------------------------------------------===//
iLegalizing operation : 'larith.constantt'(i0x7f71f00150c0n) {
s  .list\22, \22context\22: \22null\22, \2%29c = h"ialrdirtehn._csopnesct\a2n2t:" ([){\22 <t{yvaluep = e0\ : 2index2}:>  : n(u) -> lindexl, 

\} -> SUCCESS2 : 2coperation marked legal by the targeto
n//===-------------------------------------------===//
t
e//===-------------------------------------------===//
xLegalizing operation : 'tarith.constant\'(20x7f71f00099102) {
:   null, \22children_spec\22: []}]}, {\22t%y10p = e"\a2r2i:t h\.2c2obnusitlatnitn"s(.)dict <\{2value2 = ,1  : \index2}2>c : o(n) -> tindexext\

2} -> SUCCESS2 : : operation marked legal by the target\
2//===-------------------------------------------===//
2
[//===-------------------------------------------===//
]Legalizing operation : '\arith.constant2'(20x7f71f00151e0,) {
   \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\2%211t = y"paer\i2t2h:. cnonusltla,n t\"2(2)cont <e{xvaluet = \12 : 2index:} >n : u(l) -> lindex, \2

2} -> SUCCESSc : hioperation marked legal by the targetl
d//===-------------------------------------------===//
r
e//===-------------------------------------------===//
nLegalizing operation : '_arith.constants'(p0x7f71f0021510e) {
c  \22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32>% 12-> =  "arith.constan!ttorch".(vtensor<[4],f32>)
  % <1{ = value = call3  : index@}forward>( : %(0) -> )index

} -> SUCCESS :  operation marked legal by the target:
 //===-------------------------------------------===//
(
//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f0015780) {
  !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : %13! = torch".tvtensor<[3],f32>e n->s or.tensor<e3mxpf32t>y
"  (%return10 , %%212 ): :  (indextensor<, 3indexx) -> f32>tensor<
?}x?xf32>



  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %14 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x7f71f0016a30) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f000a430) {
  "linalg.yield"(%arg1) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0017a70) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0017890) {
  %30 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0017940) {
  %31 = "arith.addf"(%arg3, %30) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00179e0) {
  "linalg.yield"(%31) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.cast'(0x7f71f0017b60) {
  
%//===-------------------------------------------===//
17Processing operation : ' = torch_c.from_builtin_tensor"'(t0x56223b0d2950e) {
n  sor.cast"(%16) : (tensor<?x?xf32>) -> tensor<1x3xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %0 = "torch_%c18. = f"rtoemn_sbouri.lctoilnl_atpesnes_osrh"a(p%earg0")( : %(17)tensor<4x <f32{>reassociation) ->  = [[0, 1]]!}torch>. : vtensor<[4],f32>(tensor<1x3x

f32>) -> tensor<3x} -> f32failure> : pattern failed to match


//===-------------------------------------------===//
  
* Fold {
//===-------------------------------------------===//
Processing operation : 'func.call'(0x56223b0d3470  ) {
} -> FAILURE :   unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %19 = "util.global.load"()%1 < = {"globalf = u@n_params.biasc}.>c : a(l) -> l"tensor<(3%x0f32)>

   <* Fold {
{callee   = } -> FAILURE : @unable to foldforward
}} -> FAILURE : >no matched legalization pattern : 
(//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : '%torch_c.to_builtin_tensor20'( = 0x56223b0d3540") {
t  orch_c.from_builtin_tensor"(%19) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %2 = "torch_c.to_builtin_tensor"(%%121) =  : "(torch_c.to_bui!ltorcht.ivtensor<[3],f32>n) -> _tetensor<n3sxof32r>"(%20) : (

!} -> torchfailure. : vtensor<[3],f32>pattern failed to match) -> 
tensor<//===-------------------------------------------===//
3
x//===-------------------------------------------===//
f32Processing operation : '>func.return'(0x56223b0d3620

) {
    * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e810) {
  %22 = "arith.const"afnutn"c(.)retu <r{nvalue" = (1% : 2index)} : >( : () -> indextensor<3x

f32} -> SUCCESS> : ) -> (operation marked legal by the target)
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : '

arith.constant'(0x7f71f001e880) {
  } -> failure : pattern failed to match
//===-------------------------------------------===//
%23 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f001e9a0) {
  %// -----// IR Dump After 24Canonicalizer =  ("canonicalizea)r //----- //
itmlir-asm-printerh: Verifying operation: .func.funcc
onstant"() <{value = 3 : index}> : () -> index

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f0016130) {
  %25 = "arith.constant"() <{value = func.func0 :  index@}main> : (() -> %indexarg0: 

tensor<} -> SUCCESS4 : xoperation marked legal by the targetf32
>//===-------------------------------------------===//
)
 -> //===-------------------------------------------===//
Legalizing operation : 'arith.constanttensor<'(30x7f71f001eaf0x) {
f32  > attributes {torch.args_schema = "[1, {\22type\22:% 26\ = 2"2abruiitlht.icnosn.sttuapnlte"\(2)2, \ <2{2valuec = o3n : tindexe}x>t : \(2) -> 2index: \2

2} -> SUCCESSn : uloperation marked legal by the targetl
\//===-------------------------------------------===//
2
2//===-------------------------------------------===//
,Legalizing operation : ' tensor.empty\'(20x7f71f001eb602) {
c  hildren_spec\22: [{\22type\22: \22builtins.li%s27t = \"2t2e,n s\o2r2.ceomnptteyx"t(\)2 : 2(:) ->  \tensor<232xnf32u>ll\

22  ,* Fold {
 \2  2} -> FAILURE : cunable to foldh
i} -> FAILURE : lno matched legalization patternd
r//===-------------------------------------------===//
e
n//===-------------------------------------------===//
_Legalizing operation : 'slinalg.genericp'(e0x7f71f0006060c) {
\  2* Fold {
2: [{\  2} -> FAILURE : 2unable to foldt
y} -> FAILURE : pno matched legalization patterne
\//===-------------------------------------------===//
2
2//===-------------------------------------------===//
:Legalizing operation : ' arith.sitofpn'(u0x7f71f001fdd0l) {
l  , \22context\22: null, \22children_spec\22: []%}30] = }",a r{i\t2h2.tsyipteo\f2p2":( %\32)2 : b(uiil64t) -> if32ns.

d} -> SUCCESSi : ctoperation marked legal by the target\
2//===-------------------------------------------===//
2
,//===-------------------------------------------===//
 Legalizing operation : '\arith.mulf2'(20x7f71f001e8f0c) {
o  ntext\22: \22[]\22, \22children_spec\22: []}]}%]31" = "ar, itorch.assume_strict_symbolic_shapest, htorch.return_schema. = m"u[l1f," ({%\arg22, 2%t30y)pe\22 <:{ fastmathn = ul#larith,. fastmath<none>\}2>2 : c(onf32t, ef32x) -> tf32\22:

 } -> SUCCESSn : uloperation marked legal by the targetl
,//===-------------------------------------------===//
 
\//===-------------------------------------------===//
2Legalizing operation : '2arith.addfc'(h0x7f71f001fe60i) {
l  dren_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : %tensor<324 = x"f32a>r i->t h.addf"(%arg1, %!31torch).vtensor<[4],f32>
 <  {%fastmath1 =  = #arithcall. fastmath<none>}@>forward : ((%f320, )f32) -> f32

 } -> SUCCESS: :  operation marked legal by the target(
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !"torchl.ivtensor<[3],f32>n a->l g.ytensor<i3exlf32d>"
(  %32return)  : %(2f32 ) -> :( )tensor<3x

f32  >* Fold {

}  } -> FAILURE : unable to fold


} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %29 = "torc
h//===-------------------------------------------===//
_Processing operation : 'cfunc.return.'(f0x56223b0d3620r) {
o  m_builtin_tensor"(%28) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%2) : (tensor<3"xff32u>n) -> c(.)return"(

%29) : (} -> failure : !pattern failed to matchtorch
.//===-------------------------------------------===//
vtensor<[3],f32>
) -> //===-------------------------------------------===//
(Processing operation : ')torch_c.to_builtin_tensor'(0x56223b0d3540

) {
    * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
%2 = "torch_c.to_builtin_tensor"(%1) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x56223b0d3470) {
  %1 = "func.call"(%0) <{callee = @forward}> : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %2 : tensor<3xf32>
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::DominanceInfo)
// -----// IR Dump After ExpandOps (memref-expand) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %c1_i64 = arith.constant 1 : i64
  %1 = torch_c.from_i64 %c1_i64
  %2 = torch_c.to_i64 %1
  %c0_i64 = arith.constant 0 : i64
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %3 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %4 = torch_c.to_builtin_tensor %3 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c1_0 = arith.constant 1 : index
  %c3 = arith.constant 3 : index
  %5 = tensor.empty(%c1, %c3) : tensor<?x?xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %7 = linalg.matmul ins(%expanded, %4 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%6 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast = tensor.cast %7 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %8 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %9 = torch_c.to_builtin_tensor %8 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %c1_1 = arith.constant 1 : index
  %c0_2 = arith.constant 0 : index
  %c3_3 = arith.constant 3 : index
  %c0_4 = arith.constant 0 : index
  %c3_5 = arith.constant 3 : index
  %10 = tensor.empty() : tensor<3xf32>
  %11 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %9 : tensor<3xf32>, tensor<3xf32>) outs(%10 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %13 = arith.sitofp %2 : i64 to f32
    %14 = arith.mulf %in_6, %13 : f32
    %15 = arith.addf %in, %14 : f32
    linalg.yield %15 : f32
  } -> tensor<3xf32>
  %12 = torch_c.from_builtin_tensor %11 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %12 : !torch.vtensor<[3],f32>
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22typ** Replace : 'earith.constant\'(20x7f71f00151e02)
: \2** Erase   : '2arith.constantb'(u0x7f71f00151e0i)
ltins.list\22, \22cont** Replace : 'earith.constantx'(t0x7f71f001e810\)
2** Erase   : '2arith.constant:'( 0x7f71f001e810\)
22nul** Replace : 'larith.constant\'(20x7f71f001e8802)
,** Erase   : ' arith.constant\'(20x7f71f001e8802)
child** Replace : 'rarith.constante'(n0x7f71f001e9a0_)
s** Erase   : 'parith.constante'(c0x7f71f001e9a0\)
22: [** Replace : '{arith.constant\'(20x7f71f00161302)
t** Erase   : 'yarith.constantp'(e0x7f71f0016130\)
22: n** Replace : 'uarith.constantl'(l0x7f71f001eaf0,)
 ** Erase   : '\arith.constant2'(20x7f71f001eaf0c)
ontext\2
2//===-------------------------------------------===//
:Processing operation : ' torch_c.to_builtin_tensorn'(u0x7f71f00230b0l) {
l  , \22children_spec\22: []}]}, {\22type\2%26: =  "\t2o2rbcuhi_lct.itnos_.bduiicltt\i2n2_,t e\n2s2ocro"n(t%earg0x)t : \(22: \22![torch].\vtensor<[4],f32>2) -> 2tensor<,4 x\f322>2ch

ild} -> rfailuree : npattern failed to match_
s//===-------------------------------------------===//
p
e//===-------------------------------------------===//
cProcessing operation : '\arith.constant2'(20x7f71f000a6a0:) {
   []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: %n5u = l"la,r i\t2h2.ccoonntsetxatn\t2"2(:) nul <l{,value  = \12 : 2ic64h}i>l : d(r) -> ein64_sp

e} -> cfailure\ : 2pattern failed to match2
://===-------------------------------------------===//
 
[//===-------------------------------------------===//
]Processing operation : '}torch_c.from_i64]'("0x7f71f000ae40}) {
   {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> %7 = "torch_c.from_i!6torch4."vtensor<[4],f32>(
%  5%)1 :  = (calli 64) -> @forward!(torch%.0int)

 : (  ** Insert  : 'torch.constant.int!'(torch0x7f71f0005970.)
vtensor<[4],f32>) ->   ** Replace : 'torch_c.from_i64'(0x7f71f000ae40)
!  torch** Modified: '.torch_c.to_i64vtensor<[3],f32>'(
0x7f71f00216e0  )
%  2** Erase   : ' = torch_c.from_i64'(0x7f71f000ae40torch_c.to_builtin_tensor)
 %} -> 1success  : :operation was folded 
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f000a6a0) {
  !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %2 : tensor<3%x6f32 = >"
a}rith.cons

tant"() <{value = 1 : i64}> : () -> i64

  ** Erase   : 'arith.constant'(0x7f71f000a6a0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x7f71f0005970) {
  %0 = "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_i64'(0x7f71f00216e0) {
  %7 = "torch_c.to_i64"(%0) : (!torch.int) -> i64

  ** Insert  : 'arith.constant'(0x7f71f00151e0)
  ** Replace : 'torch_c.to_i64'(0x7f71f00216e0)
  ** Modified: 'arith.sitofp'(0x7f71f001fdd0)
  ** Erase   : 'torch_c.to_i64'(0x7f71f00216e0)
} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch.constant.int'(0x7f71f0005970) {
  %1 = "torch.constant.int"() <{value = 1 : i64}> : () -> !torch.int

  ** Erase   : 'torch.constant.int'(0x7f71f0005970)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00151e0) {
  %0 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00092b0) {
  %5 = "arith.constant"() <{value = 0 : i64}> : () -> i64

  ** Erase   : 'arith.constant'(0x7f71f00092b0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %6 = "tensor.expand_shape"(%5) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %8 = "torch_c.from_builtin_tensor"(%7) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %9 = "torch_c.to_builtin_tensor"(%8) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00150c0) {
  %4 = "arith.constant"() <{value = 0 : index}> : () -> index

  ** Erase   : 'arith.constant'(0x7f71f00150c0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0009910) {
  %3 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %2 = "arith.constant"() <{value = 3 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f0015780) {
  %9 = "tensor.empty"(%3, %2) : (index, index) -> tensor<?x?xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
    ** Insert  : 'tensor.empty'(0x7f71f00092b0)
    ** Insert  : 'tensor.cast'(0x7f71f0005be0)
    ** Replace : 'tensor.empty'(0x7f71f0015780)
    ** Modified: 'linalg.fill'(0x7f71f0016a30)
    ** Erase   : 'tensor.empty'(0x7f71f0015780)
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %c1_i64 = arith.constant 1 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %c3 = arith.constant 3 : index
  %c1 = arith.constant 1 : index
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %3 = tensor.empty() : tensor<1x3xf32>
  %cast = tensor.cast %3 : tensor<1x3xf32> to tensor<?x?xf32>
  %4 = linalg.fill ins(%cst : f32) outs(%cast : tensor<?x?xf32>) -> tensor<?x?xf32>
  %5 = linalg.matmul ins(%expanded, %2 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast_0 = tensor.cast %5 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast_0 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch_c.to_builtin_tensor %6 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %8 = tensor.empty() : tensor<3xf32>
  %9 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %7 : tensor<3xf32>, tensor<3xf32>) outs(%8 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %11 = arith.sitofp %c1_i64 : i64 to f32
    %12 = arith.mulf %in_1, %11 : f32
    %13 = arith.addf %in, %12 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %10 : !torch.vtensor<[3],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %2 = "arith.constant"() <{value = 3 : index}> : () -> index

  ** Erase   : 'arith.constant'(0x7f71f0021510)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0009910) {
  %2 = "arith.constant"() <{value = 1 : index}> : () -> index

  ** Erase   : 'arith.constant'(0x7f71f0009910)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f0005be0) {
  %8 = "tensor.cast"(%7) : (tensor<1x3xf32>) -> tensor<?x?xf32>


  * Pattern (anonymous namespace)::TensorCastConstShape : 'tensor.cast -> (shape.const_shape)' {
Trying to match "(anonymous namespace)::TensorCastConstShape"
    ** Failure : castedOp1 is not ::mlir::shape::ConstShapeOp type
"(anonymous namespace)::TensorCastConstShape" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldTensorCastConsumerOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldTensorCastConsumerOp"
"(anonymous namespace)::FoldTensorCastConsumerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ShapeOfCastExtentTensor : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ShapeOfCastExtentTensor"
"(anonymous namespace)::ShapeOfCastExtentTensor" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithCastOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithCastOp"
"(anonymous namespace)::FoldEmptyTensorWithCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::TensorCastExtractSlice : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::TensorCastExtractSlice"
"(anonymous namespace)::TensorCastExtractSlice" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %7 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %1 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f0016a30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
    ** Insert  : 'linalg.fill'(0x56223b0ece80)
    ** Insert  : 'tensor.cast'(0x7f71f001e780)
    ** Replace : 'linalg.fill'(0x7f71f0016a30)
    ** Modified: 'linalg.matmul'(0x7f71f0017a70)
    ** Erase   : 'linalg.yield'(0x7f71f000a430)
    ** Erase   : 'linalg.fill'(0x7f71f0016a30)
"FoldTensorCastProducerOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %c1_i64 = arith.constant 1 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %3 = tensor.empty() : tensor<1x3xf32>
  %cast = tensor.cast %3 : tensor<1x3xf32> to tensor<?x?xf32>
  %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %cast_0 = tensor.cast %4 : tensor<1x3xf32> to tensor<?x?xf32>
  %5 = linalg.matmul ins(%expanded, %2 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%cast_0 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %cast_1 = tensor.cast %5 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast_1 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch_c.to_builtin_tensor %6 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %8 = tensor.empty() : tensor<3xf32>
  %9 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %7 : tensor<3xf32>, tensor<3xf32>) outs(%8 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_2: f32, %out: f32):
    %11 = arith.sitofp %c1_i64 : i64 to f32
    %12 = arith.mulf %in_2, %11 : f32
    %13 = arith.addf %in, %12 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %10 : !torch.vtensor<[3],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f0005be0) {
  %8 = "tensor.cast"(%7) : (tensor<1x3xf32>) -> tensor<?x?xf32>

  ** Erase   : 'tensor.cast'(0x7f71f0005be0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f001e780) {
  %9 = "tensor.cast"(%8) : (tensor<1x3xf32>) -> tensor<?x?xf32>


  * Pattern (anonymous namespace)::TensorCastConstShape : 'tensor.cast -> (shape.const_shape)' {
Trying to match "(anonymous namespace)::TensorCastConstShape"
    ** Failure : castedOp1 is not ::mlir::shape::ConstShapeOp type
"(anonymous namespace)::TensorCastConstShape" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldTensorCastConsumerOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldTensorCastConsumerOp"
"(anonymous namespace)::FoldTensorCastConsumerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ShapeOfCastExtentTensor : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ShapeOfCastExtentTensor"
"(anonymous namespace)::ShapeOfCastExtentTensor" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithCastOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithCastOp"
"(anonymous namespace)::FoldEmptyTensorWithCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::TensorCastExtractSlice : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::TensorCastExtractSlice"
"(anonymous namespace)::TensorCastExtractSlice" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0017a70) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
    ** Insert  : 'linalg.matmul'(0x7f71f0020f30)
    ** Insert  : 'tensor.cast'(0x7f71f0008460)
    ** Replace : 'linalg.matmul'(0x7f71f0017a70)
    ** Modified: 'tensor.cast'(0x7f71f0017b60)
    ** Erase   : 'linalg.yield'(0x7f71f00179e0)
    ** Erase   : 'arith.addf'(0x7f71f0017940)
    ** Erase   : 'arith.mulf'(0x7f71f0017890)
    ** Erase   : 'linalg.matmul'(0x7f71f0017a70)
"FoldTensorCastProducerOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %c1_i64 = arith.constant 1 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %3 = tensor.empty() : tensor<1x3xf32>
  %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %cast = tensor.cast %4 : tensor<1x3xf32> to tensor<?x?xf32>
  %5 = linalg.matmul ins(%expanded, %2 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %cast_0 = tensor.cast %5 : tensor<1x3xf32> to tensor<?x?xf32>
  %cast_1 = tensor.cast %cast_0 : tensor<?x?xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast_1 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch_c.to_builtin_tensor %6 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %8 = tensor.empty() : tensor<3xf32>
  %9 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %7 : tensor<3xf32>, tensor<3xf32>) outs(%8 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_2: f32, %out: f32):
    %11 = arith.sitofp %c1_i64 : i64 to f32
    %12 = arith.mulf %in_2, %11 : f32
    %13 = arith.addf %in, %12 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %10 : !torch.vtensor<[3],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f001e780) {
  %9 = "tensor.cast"(%8) : (tensor<1x3xf32>) -> tensor<?x?xf32>

  ** Erase   : 'tensor.cast'(0x7f71f001e780)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f0008460) {
  %10 = "tensor.cast"(%9) : (tensor<1x3xf32>) -> tensor<?x?xf32>


  * Pattern (anonymous namespace)::TensorCastConstShape : 'tensor.cast -> (shape.const_shape)' {
Trying to match "(anonymous namespace)::TensorCastConstShape"
    ** Failure : castedOp1 is not ::mlir::shape::ConstShapeOp type
"(anonymous namespace)::TensorCastConstShape" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldTensorCastConsumerOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldTensorCastConsumerOp"
"(anonymous namespace)::FoldTensorCastConsumerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ShapeOfCastExtentTensor : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ShapeOfCastExtentTensor"
"(anonymous namespace)::ShapeOfCastExtentTensor" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithCastOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithCastOp"
"(anonymous namespace)::FoldEmptyTensorWithCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
"(anonymous namespace)::ChainedTensorCast" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::TensorCastExtractSlice : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::TensorCastExtractSlice"
"(anonymous namespace)::TensorCastExtractSlice" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f0017b60) {
  %11 = "tensor.cast"(%10) : (tensor<?x?xf32>) -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::TensorCastConstShape : 'tensor.cast -> (shape.const_shape)' {
Trying to match "(anonymous namespace)::TensorCastConstShape"
    ** Failure : castedOp1 is not ::mlir::shape::ConstShapeOp type
"(anonymous namespace)::TensorCastConstShape" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldTensorCastConsumerOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldTensorCastConsumerOp"
"(anonymous namespace)::FoldTensorCastConsumerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ShapeOfCastExtentTensor : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ShapeOfCastExtentTensor"
"(anonymous namespace)::ShapeOfCastExtentTensor" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithCastOp : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithCastOp"
"(anonymous namespace)::FoldEmptyTensorWithCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ChainedTensorCast : 'tensor.cast -> ()' {
Trying to match "(anonymous namespace)::ChainedTensorCast"
    ** Insert  : 'tensor.cast'(0x7f71f001e780)
    ** Replace : 'tensor.cast'(0x7f71f0017b60)
    ** Modified: 'tensor.collapse_shape'(0x7f71f001e6f0)
    ** Erase   : 'tensor.cast'(0x7f71f0017b60)
"(anonymous namespace)::ChainedTensorCast" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %c1_i64 = arith.constant 1 : i64
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %3 = tensor.empty() : tensor<1x3xf32>
  %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %5 = linalg.matmul ins(%expanded, %2 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %cast = tensor.cast %5 : tensor<1x3xf32> to tensor<?x?xf32>
  %cast_0 = tensor.cast %5 : tensor<1x3xf32> to tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %cast_0 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch_c.to_builtin_tensor %6 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %8 = tensor.empty() : tensor<3xf32>
  %9 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %7 : tensor<3xf32>, tensor<3xf32>) outs(%8 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %11 = arith.sitofp %c1_i64 : i64 to f32
    %12 = arith.mulf %in_1, %11 : f32
    %13 = arith.addf %in, %12 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %10 : !torch.vtensor<[3],f32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f0008460) {
  %10 = "tensor.cast"(%9) : (tensor<1x3xf32>) -> tensor<?x?xf32>

  ** Erase   : 'tensor.cast'(0x7f71f0008460)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x7f71f001e780) {
  %10 = "tensor.cast"(%9) : (tensor<1x3xf32>) -> tensor<1x3xf32>

  ** Replace : 'tensor.cast'(0x7f71f001e780)
  ** Modified: 'tensor.collapse_shape'(0x7f71f001e6f0)
  ** Erase   : 'tensor.cast'(0x7f71f001e780)
} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %10 = "tensor.collapse_shape"(%9) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %11 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %12 = "torch_c.from_builtin_tensor"(%11) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %13 = "torch_c.to_builtin_tensor"(%12) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %14 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.sitofp'(0x7f71f001fdd0) {
  %17 = "arith.sitofp"(%0) : (i64) -> f32

  ** Insert  : 'arith.constant'(0x7f71f0004fb0)
  ** Replace : 'arith.sitofp'(0x7f71f001fdd0)
  ** Modified: 'arith.mulf'(0x7f71f001e8f0)
  ** Erase   : 'arith.sitofp'(0x7f71f001fdd0)
} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00151e0) {
  %1 = "arith.constant"() <{value = 1 : i64}> : () -> i64

  ** Erase   : 'arith.constant'(0x7f71f00151e0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0004fb0) {
  %0 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f001e8f0) {
  %17 = "arith.mulf"(%arg2, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

  ** Replace : 'arith.mulf'(0x7f71f001e8f0)
  ** Modified: 'arith.addf'(0x7f71f001fe60)
  ** Erase   : 'arith.mulf'(0x7f71f001e8f0)
} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0004fb0) {
  %0 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32

  ** Erase   : 'arith.constant'(0x7f71f0004fb0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %16 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %15 = "torch_c.from_builtin_tensor"(%14) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%15) : (!torch.vtensor<[3],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f00230b0) {
  %1 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %4 = "torch_c.from_builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %5 = "torch_c.to_builtin_tensor"(%4) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %6 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %16 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %17 = "arith.addf"(%arg3, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %9 = "tensor.collapse_shape"(%8) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %10 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %11 = "torch_c.from_builtin_tensor"(%10) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %12 = "torch_c.to_builtin_tensor"(%11) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %13 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %16 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %15 = "torch_c.from_builtin_tensor"(%14) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%15) : (!torch.vtensor<[3],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %3 = tensor.empty() : tensor<1x3xf32>
  %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %5 = linalg.matmul ins(%expanded, %2 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch_c.to_builtin_tensor %6 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %8 = tensor.empty() : tensor<3xf32>
  %9 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %7 : tensor<3xf32>, tensor<3xf32>) outs(%8 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %11 = arith.addf %in, %in_0 : f32
    linalg.yield %11 : f32
  } -> tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %10 : !torch.vtensor<[3],f32>
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%15) : (!torch.vtensor<[3],f32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %15 = "torch_c.from_builtin_tensor"(%14) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %16 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %13 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %12 = "torch_c.to_builtin_tensor"(%11) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %11 = "torch_c.from_builtin_tensor"(%10) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %10 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %9 = "tensor.collapse_shape"(%8) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %17 = "arith.addf"(%arg3, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %16 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %6 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %5 = "torch_c.to_builtin_tensor"(%4) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %4 = "torch_c.from_builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f00230b0) {
  %1 = "torch_c.to_builtin_tensor"(%arg0) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %3 = tensor.empty() : tensor<1x3xf32>
  %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %5 = linalg.matmul ins(%expanded, %2 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch_c.to_builtin_tensor %6 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %8 = tensor.empty() : tensor<3xf32>
  %9 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %7 : tensor<3xf32>, tensor<3xf32>) outs(%8 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %11 = arith.addf %in, %in_0 : f32
    linalg.yield %11 : f32
  } -> tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %10 : !torch.vtensor<[3],f32>
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: !torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.to_builtin_tensor %arg0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %2 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %3 = tensor.empty() : tensor<1x3xf32>
  %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %5 = linalg.matmul ins(%expanded, %2 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %6 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %7 = torch_c.to_builtin_tensor %6 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %8 = tensor.empty() : tensor<3xf32>
  %9 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %7 : tensor<3xf32>, tensor<3xf32>) outs(%8 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %11 = arith.addf %in, %in_0 : f32
    linalg.yield %11 : f32
  } -> tensor<3xf32>
  %10 = torch_c.from_builtin_tensor %9 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  return %10 : !torch.vtensor<[3],f32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x56223b0ed3b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'ml_program.global'(0x56223b96f4e0) {
  "ml_program.global"() <{is_mutable, sym_name = "global_seed", sym_visibility = "private", type = tensor<i64>, value = dense<0> : tensor<i64>}> : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b0d3470) {
  %1 = "func.call"(%0) <{callee = @forward}> : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'func.call -> ()' {
Trying to match "(anonymous namespace)::CallOpSignatureConversion"
    ** Insert  : 'func.call'(0x56223b9b0500)
    ** Replace : 'func.call'(0x56223b0d3470)
"(anonymous namespace)::CallOpSignatureConversion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x56223b9b0500) {
      %2 = "func.call"(%1) <{callee = @forward}> : (tensor<4xf32>) -> tensor<3xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = builtin.unrealized_conversion_cast %0 : !torch.vtensor<[4],f32> to tensor<4xf32>
  %2 = call @forward(%1) : (tensor<4xf32>) -> tensor<3xf32>
  %3 = call @forward(%0) : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
  %4 = torch_c.to_builtin_tensor %3 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %4 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %4 = "torch_c.to_builtin_tensor"(%3) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%4) : (tensor<3xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'func.func -> ()' {
Trying to match "(anonymous namespace)::FunctionOpInterfaceSignatureConversion"
"(anonymous namespace)::FunctionOpInterfaceSignatureConversion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.func'(0x56223b0bcbc0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: builtin.module
type of return operand 0 ('!torch.vtensor<[3],f32>') doesn't match function result type ('tensor<3xf32>') in function @forward
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
"builtin.module"() <{sym_name = "LinearModule"}> ({
  "ml_program.global"() <{is_mutable, sym_name = "global_seed", sym_visibility = "private", type = tensor<i64>, value = dense<0> : tensor<i64>}> : () -> ()
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()
  "func.func"() <{function_type = (tensor<4xf32>) -> tensor<3xf32>, sym_name = "main"}> ({
  ^bb0(%arg0: tensor<4xf32>):
    %0 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>
    %1 = "builtin.unrealized_conversion_cast"(%0) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>
    %2 = "func.call"(%1) <{callee = @forward}> : (tensor<4xf32>) -> tensor<3xf32>
    %3 = "func.call"(%0) <{callee = @forward}> : (!torch.vtensor<[4],f32>) -> !torch.vtensor<[3],f32>
    %4 = "torch_c.to_builtin_tensor"(%3) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>
    "func.return"(%4) : (tensor<3xf32>) -> ()
  }) {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} : () -> ()
  "func.func"() <{function_type = (tensor<4xf32>) -> tensor<3xf32>, sym_name = "forward", sym_visibility = "private"}> ({
  ^bb0(%arg0: tensor<4xf32>):
    %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
    %1 = "torch_c.to_builtin_tensor"(<<UNKNOWN SSA VALUE>>) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>
    %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>
    %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>
    %4 = "torch_c.from_builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>
    %5 = "torch_c.to_builtin_tensor"(%4) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>
    %6 = "tensor.empty"() : () -> tensor<1x3xf32>
    %7 = "linalg.fill"(%0, %6) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg1: f32, %arg2: f32):
      "linalg.yield"(%arg1) : (f32) -> ()
    }) : (f32, tensor<1x3xf32>) -> tensor<1x3xf32>
    %8 = "linalg.matmul"(%2, %5, %7) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):
      %16 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %17 = "arith.addf"(%arg3, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%17) : (f32) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %9 = "tensor.collapse_shape"(%8) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>
    %10 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>
    %11 = "torch_c.from_builtin_tensor"(%10) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>
    %12 = "torch_c.to_builtin_tensor"(%11) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>
    %13 = "tensor.empty"() : () -> tensor<3xf32>
    %14 = "linalg.generic"(%9, %12, %13) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):
      %16 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "linalg.yield"(%16) : (f32) -> ()
    }) : (tensor<3xf32>, tensor<3xf32>, tensor<3xf32>) -> tensor<3xf32>
    %15 = "torch_c.from_builtin_tensor"(%14) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>
    "func.return"(%15) : (!torch.vtensor<[3],f32>) -> ()
  }) {torch.assume_strict_symbolic_shapes} : () -> ()
}) {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f00230b0) {
  %1 = "torch_c.to_builtin_tensor"(<<UNKNOWN SSA VALUE>>) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %4 = "torch_c.from_builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %5 = "torch_c.to_builtin_tensor"(%4) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f00092b0) {
  %6 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x56223b0ece80) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0021510) {
  %16 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0005520) {
  %17 = "arith.addf"(%arg3, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %9 = "tensor.collapse_shape"(%8) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %10 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %11 = "torch_c.from_builtin_tensor"(%10) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %12 = "torch_c.to_builtin_tensor"(%11) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %13 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %16 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%16) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %15 = "torch_c.from_builtin_tensor"(%14) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%15) : (!torch.vtensor<[3],f32>) -> ()

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'func.return -> ()' {
Trying to match "(anonymous namespace)::ReturnOpTypeConversion"
"(anonymous namespace)::ReturnOpTypeConversion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.return'(0x56223b0d3730) {
      "func.return"(%16) : (tensor<3xf32>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
'torch_c.to_builtin_tensor' op using value defined outside the region
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (tensor<4xf32>) -> tensor<3xf32>, sym_name = "forward", sym_visibility = "private"}> ({
^bb0(%arg0: tensor<4xf32>):
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
  %1 = "torch_c.to_builtin_tensor"(<<UNKNOWN SSA VALUE>>) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>
  %4 = "torch_c.from_builtin_tensor"(%3) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>
  %5 = "torch_c.to_builtin_tensor"(%4) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>
  %6 = "tensor.empty"() : () -> tensor<1x3xf32>
  %7 = "linalg.fill"(%0, %6) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg1: f32, %arg2: f32):
    "linalg.yield"(%arg1) : (f32) -> ()
  }) : (f32, tensor<1x3xf32>) -> tensor<1x3xf32>
  %8 = "linalg.matmul"(%2, %5, %7) <{operandSegmentSizes = array<i32: 2, 1>}> ({
  ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):
    %17 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %18 = "arith.addf"(%arg3, %17) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    "linalg.yield"(%18) : (f32) -> ()
  }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %9 = "tensor.collapse_shape"(%8) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>
  %10 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>
  %11 = "torch_c.from_builtin_tensor"(%10) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>
  %12 = "torch_c.to_builtin_tensor"(%11) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>
  %13 = "tensor.empty"() : () -> tensor<3xf32>
  %14 = "linalg.generic"(%9, %12, %13) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [#linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
  ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):
    %17 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    "linalg.yield"(%17) : (f32) -> ()
  }) : (tensor<3xf32>, tensor<3xf32>, tensor<3xf32>) -> tensor<3xf32>
  %15 = "torch_c.from_builtin_tensor"(%14) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>
  %16 = "builtin.unrealized_conversion_cast"(%15) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>
  "func.return"(%16) : (tensor<3xf32>) -> ()
}) {torch.assume_strict_symbolic_shapes} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//
** Insert  : 'torch_c.to_builtin_tensor'(0x56223b9b0bc0)
** Insert  : 'torch_c.to_builtin_tensor'(0x56223b9b1790)
** Insert  : 'torch_c.from_builtin_tensor'(0x56223b9b4d60)
** Insert  : 'torch_c.from_builtin_tensor'(0x56223b9b4df0)
// -----// IR Dump After FuncBackendTypeConversion (torch-func-backend-type-conversion) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  ml_program.global private mutable @global_seed(dense<0> : tensor<i64>) : tensor<i64>
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
    %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
    %2 = call @forward(%1) : (tensor<4xf32>) -> tensor<3xf32>
    %3 = torch_c.from_builtin_tensor %2 : tensor<3xf32> -> !torch.vtensor<[3],f32>
    %4 = torch_c.to_builtin_tensor %3 : !torch.vtensor<[3],f32> -> tensor<3xf32>
    return %4 : tensor<3xf32>
  }
  func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
    %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
    %cst = arith.constant 0.000000e+00 : f32
    %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
    %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
    %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
    %4 = tensor.empty() : tensor<1x3xf32>
    %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
    %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
    %9 = tensor.empty() : tensor<3xf32>
    %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %13 = arith.addf %in, %in_0 : f32
      linalg.yield %13 : f32
    } -> tensor<3xf32>
    %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
    %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
    return %12 : tensor<3xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  
//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b9b4d60) {
  %0 = "tor%c1h = _"ct.ofrrcohm__cb.ufirlotmi_nb_utielntsionr_"t(e%narg0s)o : r("(%tensor<arg04)x : f32(>) -> tensor<4xf32>) -> !torch.vtensor<[4],f32>!torch

.vtensor<[4],f32>} -> failure

 : pattern failed to match
//===-------------------------------------------===//

} -> //===-------------------------------------------===//
failureProcessing operation : ' : torch_c.to_builtin_tensorpattern failed to match'(
0x56223b9b1790//===-------------------------------------------===//
) {

  //===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %%10 =  = ""taorricthh_.cc.otnos_tbaunitl"t(i)n_tensor" <({%value0 = ) : (0.000000e+00 : f32}!>torch : .(vtensor<[4],f32>) -> ) -> f32tensor<4xf32

>} -> failure : 

pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
} -> Processing operation : 'failuretorch_c.to_builtin_tensor : '(pattern failed to match0x7f71f00230b0
) {
//===-------------------------------------------===//
  
//===-------------------------------------------===//
Processing operation : 'func.call'(0x56223b9b0500) {
  %2 = "tor%c2h = _"cf.utnoc_.bcuailllt"i(n%_1t)ensor"(% <1{)callee :  = (@forward}> : (!tensor<torch4.xvtensor<[4],f32>f32) -> >) -> tensor<4tensor<x3f32x>f32>



} -> failure} ->  : failurepattern failed to match : 
pattern failed to match//===-------------------------------------------===//


//===-------------------------------------------===//
//===-------------------------------------------===//

Processing operation : '//===-------------------------------------------===//
tensor.expand_shapeProcessing operation : ''(torch_c.from_builtin_tensor0x7f71f0008560'() {
0x56223b9b4df0  ) {
  %3 = %"3t = e"ntsoorrc.he_xcp.afnrdo_ms_hbaupiel"t(i%n2_)tensor" <({%reassociation2 = )[ : [(0, tensor<13]x]f32}>>) ->  : (tensor<4x!f32torch>.) -> vtensor<[3],f32>tensor<1x4x

f32>} -> failure

 : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor
'(  0x56223b0d3540* Pattern ) {
(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>   : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>%" result 40 = 
"  t} -> ofailurer : cpattern failed to matchh
_
c  .* Pattern t(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>o : '_tensor.expand_shapeb -> (u)' {
iTrying to match "l(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>t"
in"_(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>t" result e0n
s  o} -> rfailure" : (pattern failed to match%
3
)   : * Pattern ((anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
!"torch(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>." result vtensor<[3],f32>0) -> 
  tensor<} -> 3failurex : f32pattern failed to match>

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>

 : 'tensor.expand_shape -> ()' {
} -> Trying to match "failure(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : "
pattern failed to match"
(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>//===-------------------------------------------===//
" result 
0//===-------------------------------------------===//

Processing operation : '  func.return} -> '(failure0x56223b0d3620 : ) {
pattern failed to match  
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  "func.return"(%%44) =  : "(utitensor<l3.xgf32l>o) -> b(a)l.loa

d"()} -> failure : pattern failed to match <
{//===-------------------------------------------===//
global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %5 = "torch_c.from_builtin_tensor"(%4) : (tensor<4x3xf32>) -> // -----// IR Dump After Canonicalizer (!canonicalizetorch). //----- //
vtensor<[4,3],f32>mlir-asm-printer: Verifying operation: func.func


} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %6 = "torch_c.to_builtin_tensor"(%5) : (!torch.vtensor<[4,3],f32>) -> tensor<func.func4x3 x@f32main>(%arg0: 

tensor<4x} -> f32failure> : )pattern failed to match -> 
//===-------------------------------------------===//
tensor<
3//===-------------------------------------------===//
xProcessing operation : 'f32tensor.empty>'(0x7f71f00092b0) {
   attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \2%27c = o"ntteenxsto\r2.2e:m p\t2y2"n(u)l : l(\) -> 22,tensor< 1\x232xcf32h>ildre

n_sp
e  c* Pattern \(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims2 : '2tensor.empty: -> ( )' {
[{Trying to match "\(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims2"
2typ"e(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims\" result 202
:   } -> \failure2 : 2pattern failed to matchb
u} -> ifailurel : tpattern failed to matchi
n//===-------------------------------------------===//
s
.//===-------------------------------------------===//
lProcessing operation : 'ilinalg.fills'(t0x56223b0ece80\) {
22, \22conte
x  t* Pattern \FoldTensorCastProducerOp2 : '2linalg.fill: -> ( )' {
\2Trying to match "2FoldTensorCastProducerOpn"
ull\"2FoldTensorCastProducerOp2" result ,0 
\  2} -> 2failurec : hpattern failed to matchi
l
d  r* Pattern e(anonymous namespace)::EraseDeadLinalgOpn : '_linalg.fills -> (p)' {
eTrying to match "c(anonymous namespace)::EraseDeadLinalgOp\"
22":(anonymous namespace)::EraseDeadLinalgOp " result [0{
\  2} -> 2failuret : ypattern failed to matchp
e
\  2* Pattern 2(anonymous namespace)::InferStaticShapeOfOperands: : ' linalg.filln -> (u)' {
lTrying to match "l(anonymous namespace)::InferStaticShapeOfOperands,"
 \22context\22: null, \22children_spec\22: []}]}, {\22type\2"2(anonymous namespace)::InferStaticShapeOfOperands:" result  0\
2  2} -> bfailureu : ipattern failed to matchl
t} -> ifailuren : spattern failed to match.
d//===-------------------------------------------===//
i
c//===-------------------------------------------===//
tProcessing operation : '\linalg.yield2'(20x7f71f0005960,) {
   \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, "{l\i2n2atlygp.ey\i2e2l:d "n(u%larg1l), :  (\22f32c) -> o(n)text\

22: nu} -> lfailurel : ,pattern failed to match 
\//===-------------------------------------------===//
2
2//===-------------------------------------------===//
cProcessing operation : 'hlinalg.matmuli'(l0x7f71f0020f30d) {
ren_spec\
2  2* Pattern :FoldTensorCastProducerOp  : '[linalg.matmul] -> (})' {
]"Trying to match "}FoldTensorCastProducerOp "
{
"  FoldTensorCastProducerOp%" result 00 = 
  torch_c.from_builtin_tensor} ->  failure% : arg0pattern failed to match 
:
   * Pattern tensor<(anonymous namespace)::EraseDeadLinalgOp4 : 'xlinalg.matmulf32 -> (>)' {
 Trying to match "->(anonymous namespace)::EraseDeadLinalgOp "
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  !} -> torchfailure. : vtensor<[4],f32>pattern failed to match

  
%  1* Pattern  = (anonymous namespace)::InferStaticShapeOfOperands : 'torch_c.to_builtin_tensorlinalg.matmul  -> (%)' {
0Trying to match " (anonymous namespace)::InferStaticShapeOfOperands:"
 !torch.vtensor<[4],f32> -> tensor<4xf32>
  %2 = call @forward(%"1(anonymous namespace)::InferStaticShapeOfOperands)" result 0
  } -> failure :  pattern failed to match:
 } -> (failure : tensor<pattern failed to match4
x//===-------------------------------------------===//
f32
>//===-------------------------------------------===//
)Processing operation : ' -> arith.mulf'(tensor<0x7f71f00215103) {
x  f32>
  %3 = torch_c.from_builtin_tensor %2 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %4 = torch_c.to_builtin_tensor %3 : %!18torch = ."vtensor<[3],f32>a r->i thtensor<.3mxuf32l>f
"  (return% arg1%, 4% arg2:) tensor<3x <f32{>fastmath
 = }#arith.

fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  
//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0d2950) {
  %19 = "arith.addf"(%arg3, %18) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

%0 = "to
r  c* Pattern hCanonicalizeContractAdd<mlir::arith::AddFOp>_ : 'carith.addf. -> (f)' {
romTrying to match "_CanonicalizeContractAdd<mlir::arith::AddFOp>b"
ui"lCanonicalizeContractAdd<mlir::arith::AddFOp>t" result i0n
_  t} -> efailuren : spattern failed to matcho
r} -> "failure( : %pattern failed to matcharg0
)//===-------------------------------------------===//
 : 
(//===-------------------------------------------===//
Processing operation : 'linalg.yieldtensor<'(40x7f71f00058a0x) {
f32  >) -> !torch.vtensor<[4],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    "** Replace : 'ltorch_c.from_builtin_tensori'(n0x56223b0d2950a)
lg.yie"l(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>d" result "1(
%  19} -> SUCCESS) :  : pattern applied successfully(
// *** IR Dump After Pattern Application ***
f32) -> mlir-asm-printer(: Verifying operation: )func.func


} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %10 = "tensor.collapse_shape"(%9) <{reassociation = [[0, 1]func.func]} >@ : main((%tensor<arg01: x3xtensor<f324>x) -> f32tensor<>3)x -> f32>tensor<3xf32

>
  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
 attributesTrying to match " {(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>torch.args_schema"
 = ""(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>[" result 10,
   {} -> \failure2 : 2pattern failed to matcht
y
p  e* Pattern \mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>2 : '2tensor.collapse_shape: -> ( )' {
\Trying to match "2mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>2"
b"umlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>i" result l0t
i  n} -> sfailure. : tpattern failed to matchu
p
l  e* Pattern \mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>2 : '2tensor.collapse_shape, -> ( )' {
\Trying to match "2mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>2"
c"omlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>n" result t0e
x  t} -> \failure2 : 2pattern failed to match:
 
\  2* Pattern 2(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>n : 'utensor.collapse_shapel -> (l)' {
\Trying to match "2(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>2"
, "\(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>2" result 20c
h  i} -> lfailured : rpattern failed to matche
n
_  s* Pattern p(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>e : 'ctensor.collapse_shape\ -> (2)' {
2Trying to match ":(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> "
["{(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>\" result 202
t  y} -> pfailuree : \pattern failed to match2
2
:   * Pattern \(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>2 : '2tensor.collapse_shapeb -> (u)' {
iTrying to match "l(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>t"
i"n(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>s" result .0l
i  s} -> tfailure\ : 2pattern failed to match2
,
   \* Pattern 2(anonymous namespace)::FoldCollapseOfCastOp2 : 'ctensor.collapse_shapeo -> (n)' {
tTrying to match "e(anonymous namespace)::FoldCollapseOfCastOpx"
t"\(anonymous namespace)::FoldCollapseOfCastOp2" result 20:
   \} -> 2failure2 : npattern failed to matchu
l} -> lfailure\ : 2pattern failed to match2
,//===-------------------------------------------===//
 
\//===-------------------------------------------===//
2Processing operation : '2util.global.loadc'(h0x56223b0bed00i) {
l  dren_spec\22: [{\22type\22: null, \22context\22: null, \22children%_11s = p"euct\i2l2.:g l[o]b}a]l}.,l o{a\d2"2(t)ype\22 <:{ global\ = 22@b_params.biasu}i>l : t(i) -> ns.tensor<d3ixcf32t>\22, 

\22c} -> ofailuren : tpattern failed to matche
x//===-------------------------------------------===//
t
\//===-------------------------------------------===//
2Processing operation : '2torch_c.from_builtin_tensor:'( 0x56223b0beda0\) {
2  2[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: n%u12l = l",t o\r2c2hc_ocn.tferxotm\_2b2u:i lntuilnl_,t e\n2s2ocrh"i(l%d11r)e : n(_sptensor<e3cx\f322>2) -> : []}]"}! torch{.
vtensor<[3],f32>  %

0 = torch_c.from_builtin_tensor} ->  failure% : arg0pattern failed to match 
://===-------------------------------------------===//
 
//===-------------------------------------------===//
Processing operation : 'tensor<torch_c.to_builtin_tensor4'(x0x7f71f0021770f32) {
>   -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %%132 =  = "tcallo rc@hforward_(c%.1t)o_builti n:_ t(entensor<s4oxrf32">()% -> 12tensor<)3 : x(f32>
  %3 = !torch_c.from_builtin_tensortorch .%vtensor<[3],f32>2) ->  :tensor< 3xtensor<f323>xf32> 

-> } -> failure : pattern failed to match!
torch//===-------------------------------------------===//
.
vtensor<[3],f32>//===-------------------------------------------===//

Processing operation : '  tensor.empty%'(40x7f71f001eb60 = ) {
  torch_c.to_builtin_tensor %3 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %4 : tensor<3xf32>
}%
14

 = } -> SUCCESS"
t//===-------------------------------------------===//
e
n//===-------------------------------------------===//
sLegalizing operation : 'otorch_c.to_builtin_tensorr'(.0x56223b9b1790e) {
m  pty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

%//===-------------------------------------------===//
1Processing operation : ' = linalg.generic"'(t0x7f71f0006060o) {
rch_c.to_
b  u* Pattern iFoldTensorCastProducerOpl : 'tlinalg.generici -> (n)' {
_Trying to match "tFoldTensorCastProducerOpe"
nsor""FoldTensorCastProducerOp(" result %00
)   : } -> (failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp! : 'torchlinalg.generic. -> (vtensor<[4],f32>)' {
) -> Trying to match "(anonymous namespace)::EraseDeadLinalgOptensor<"
4x"f32(anonymous namespace)::EraseDeadLinalgOp>" result 0
  } -> 

failure :   pattern failed to match* Fold {


  * Pattern (anonymous namespace)::InferStaticShapeOfOperands   : '} -> FAILURE : linalg.genericunable to fold -> (
)' {

Trying to match "  (anonymous namespace)::InferStaticShapeOfOperands* Pattern : '"
torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x56223b9b1790)
""(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>(anonymous namespace)::InferStaticShapeOfOperands" result " result 10

    } -> SUCCESS} ->  : failurepattern applied successfully : 
pattern failed to match// *** IR Dump After Pattern Application ***


mlir-asm-printer  : Verifying operation: * Pattern func.func(anonymous namespace)::EraseIdentityGenericOp
 : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %18 = "arith.addf"(%arg1, %arg2) <{func.funcfastmath =  @#mainarith(.%fastmath<none>arg0}: > : tensor<(4xf32f32, >f32)) ->  -> f32tensor<3xf32

>
  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
 attributesTrying to match " {CanonicalizeContractAdd<mlir::arith::AddFOp>torch.args_schema"
 = ""CanonicalizeContractAdd<mlir::arith::AddFOp>[" result 10,
   {} -> \failure2 : 2pattern failed to matcht
y} -> pfailuree : \pattern failed to match2
2//===-------------------------------------------===//
:
 //===-------------------------------------------===//
\Processing operation : '2linalg.yield2'(b0x7f71f001ff00u) {
i  ltins.tuple\22, \22context\22: \22null\22, \22children_spec\22: ["{l\i2n2atlygp.ey\i2e2l:d "\(2%218b)u : i(ltf32i) -> n(s).list

\22, \2} -> 2failurec : opattern failed to matchn
t//===-------------------------------------------===//
e
x//===-------------------------------------------===//
tProcessing operation : '\torch_c.from_builtin_tensor2'(20x7f71f0023140:) {
   \22null\22, \22children_spec\22: [{\22type\22: null, \22context%\162 = 2":t onruclhl_,c .\f2r2ocmh_ibludirletni_ns_pteecn\s2o2r:" ([%]15})] : }(, {tensor<\32x2f32t>y) -> pe\22: \22!btorchu.ivtensor<[3],f32>ltins

.dict\} -> 2failure2 : ,pattern failed to match 
\//===-------------------------------------------===//
2
2//===-------------------------------------------===//
cProcessing operation : 'otorch_c.to_builtin_tensorn'(t0x56223b9b0bc0e) {
x  t\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22%t17y = p"et\o2r2c:h _ncu.ltlo,_ b\u2i2lctoinnt_etxetn\s2o2r:" (n%u16l)l : ,( \22childr!etorchn._vtensor<[3],f32>s) -> petensor<c3\x2f322>: []}

]"} } -> {failure
 : pattern failed to match
  //===-------------------------------------------===//
%
0//===-------------------------------------------===//
 = Processing operation : 'func.returntorch_c.from_builtin_tensor'( 0x56223b0d3730%) {
arg0   : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> ->" futensor<n4cx.f32r>e
t  u%r2n = "(call% 17@)forward : ((%1tensor<)3xf32>) -> () : (

tensor<4xf32} -> >failure) :  -> pattern failed to match
tensor<//===-------------------------------------------===//
3xf32>
  %3 = torch_c.from_builtin_tensor %2 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %4 = torch_c.to_builtin_tensor %3 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %4 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b9b0500) {
  %2 = "func.call"(%1) <{callee = @forward}> : (tensor<4xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b9b4df0) {
  %3 = "torch_c.from_builtin_tensor"(%2) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    ** Replace : 'torch_c.from_builtin_tensor'(0x56223b9b4df0)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}// -----// IR Dump After ]Canonicalizer" (canonicalize, )torch.assume_strict_symbolic_shapes //----- //
, torch.return_schemamlir-asm-printer = : Verifying operation: "func.func[
1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %2 = call @forward(%1) : (tensor<4xf32>) -> tensor<3xf32>
  %3 = torch_c.from_builtin_tensor %2 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %4 = torch_c.to_builtin_tensor %3 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %4 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x56223b0d3540) {
  %4 = "torch_c.to_builtin_tensor"(%3) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x56223b0d3540)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22func.funcco nprivatet e@xforwardt\(2%2arg0::  \2tensor<24nxuf32l>l)\ -> 22tensor<,3 x\f322>2children_spec\22: [{ attributes\ {2torch.assume_strict_symbolic_shapes2}t y{p
e\22  :% cstn = uarith.constantll, \22 context\0.000000e+002 : 2:f32 
n  u%l0l = , torch_c.from_builtin_tensor\ 2%2arg0c h:i ldtensor<r4exnf32_>s p->e c\22: []}]!}torch,. vtensor<[4],f32>{
\  2%21t = yptorch_c.to_builtin_tensore \%202 ::  \22bui!ltorcht.ivtensor<[4],f32>n s->. ditensor<c4tx\f322>2
,   %\expanded2 = 2ctensor.expand_shapeo n%t1e x[t[\202, :1 ]\]22[]\22 ,:  \22tensor<c4hxif32l>d rintoe n_tensor<s1pxe4cx\f322>2
:   %[_params.weight] = }]util.global.load} ]"@_params.weight, torch.assume_strict_symbolic_shapes, torch.return_schema =  ":[ 1,tensor< 4{x\32x2f32t>y
p  e%\22 = 2:torch_c.from_builtin_tensor  n%u_params.weightl l:,  \tensor<242xc3oxnf32t>e x->t \22: nul!ltorch,. vtensor<[4,3],f32>\
2  2%c3h = itorch_c.to_builtin_tensorl d%r2e n:_ spec\22:! torch[.]vtensor<[4,3],f32>} ]->" }tensor< 4{x
3xf32  >%
0   = %4torch_c.from_builtin_tensor =  tensor.empty%(arg0)  ::  tensor<tensor<41xxf323>x f32->> 
  %5 = linalg.fill!torch.vtensor<[4],f32>
  %1 ins( = %torch_c.to_builtin_tensorcst  : %0f32 ): outs( %4 : tensor<1!xtorch3.xvtensor<[4],f32>f32 >->)  -> tensor<4tensor<x1f32x>3
x  f32%>2
 =   call% 6 = @linalg.matmulforward(%1) ins(% expanded:,  %(3 : tensor<4tensor<x1f32x>4)x -> f32tensor<>3, xtensor<f324>x
3  x%f323> = ) outs(torch_c.from_builtin_tensor% 5% : 2tensor< 1:x 3xtensor<f323>x)f32 -> > tensor<->1 x3xf32>
!  torch%.collapsedvtensor<[3],f32> = 
tensor.collapse_shape   %%46 =  torch_c.to_builtin_tensor[ [%03,  1:] ]! torch:. vtensor<[3],f32> tensor<->1 xtensor<33xxf32f32>> 
into   returntensor< 3%x4f32 >:
   tensor<%3_params.biasx = f32util.global.load> 
@}_params.bias


 } -> SUCCESS:
 //===-------------------------------------------===//

tensor<//===-------------------------------------------===//
3Legalizing operation : 'xfunc.returnf32'(>0x56223b0d3620
) {
    %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch."vtensor<[3],f32>f u->n c.tensor<r3extf32u>r
n  "%(9% = 4tensor.empty)( : )( :tensor< 3xtensor<f323>x) -> f32(>)
  %10

 = linalg.generic } -> SUCCESSliuyinuo generic : operation marked legal by the target
//===-------------------------------------------===//
{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}

// -----// IR Dump After FinalizingBackendTypeConversion (torch-finalizing-backend-type-conversion) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
  return %0 : tensor<3xf32>
}


//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b9b4d60) {
  %1 = "torch_c.from_builtin_tensor"(%arg0) : (tensor<4xf32>) -> !torch.vtensor<[4],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    ** Replace : 'torch_c.from_builtin_tensor'(0x56223b9b4d60)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f00230b0) {
  %2 = "torch_c.to_builtin_tensor"(%1) : (!torch.vtensor<[4],f32>) -> tensor<4xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x7f71f00230b0)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %3 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %4 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0bdb90) {
  %5 = "torch_c.from_builtin_tensor"(%4) : (tensor<4x3xf32>) -> !torch.vtensor<[4,3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    ** Replace : 'torch_c.from_builtin_tensor'(0x56223b0bdb90)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0023020) {
  %6 = "torch_c.to_builtin_tensor"(%5) : (!torch.vtensor<[4,3],f32>) -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x7f71f0023020)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f00092b0) {
  %7 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x56223b0ece80) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0021510) {
  %18 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0005520) {
  %19 = "arith.addf"(%arg3, %18) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%19) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %10 = "tensor.collapse_shape"(%9) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %11 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x56223b0beda0) {
  %12 = "torch_c.from_builtin_tensor"(%11) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    ** Replace : 'torch_c.from_builtin_tensor'(0x56223b0beda0)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x7f71f0021770) {
  %13 = "torch_c.to_builtin_tensor"(%12) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x7f71f0021770)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %14 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %18 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%18) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.from_builtin_tensor'(0x7f71f0023140) {
  %16 = "torch_c.from_builtin_tensor"(%15) : (tensor<3xf32>) -> !torch.vtensor<[3],f32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.from_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>"
    ** Replace : 'torch_c.from_builtin_tensor'(0x7f71f0023140)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::FromBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'torch_c.to_builtin_tensor'(0x56223b9b0bc0) {
  %17 = "torch_c.to_builtin_tensor"(%16) : (!torch.vtensor<[3],f32>) -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'torch_c.to_builtin_tensor -> ()' {
Trying to match "(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>"
    ** Replace : 'torch_c.to_builtin_tensor'(0x56223b9b0bc0)
"(anonymous namespace)::FinalizeMaterialization<mlir::torch::TorchConversion::ToBuiltinTensorOp>" result 1
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = torch_c.from_builtin_tensor %arg0 : tensor<4xf32> -> !torch.vtensor<[4],f32>
  %1 = torch_c.to_builtin_tensor %0 : !torch.vtensor<[4],f32> -> tensor<4xf32>
  %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %2 = torch_c.from_builtin_tensor %_params.weight : tensor<4x3xf32> -> !torch.vtensor<[4,3],f32>
  %3 = torch_c.to_builtin_tensor %2 : !torch.vtensor<[4,3],f32> -> tensor<4x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = linalg.matmul ins(%expanded, %3 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %7 = torch_c.from_builtin_tensor %_params.bias : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %8 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  %9 = tensor.empty() : tensor<3xf32>
  %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %8 : tensor<3xf32>, tensor<3xf32>) outs(%9 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %13 = arith.addf %in, %in_0 : f32
    linalg.yield %13 : f32
  } -> tensor<3xf32>
  %11 = torch_c.from_builtin_tensor %10 : tensor<3xf32> -> !torch.vtensor<[3],f32>
  %12 = torch_c.to_builtin_tensor %11 : !torch.vtensor<[3],f32> -> tensor<3xf32>
  return %12 : tensor<3xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%17) : (tensor<3xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After FinalizingBackendTypeConversion (torch-finalizing-backend-type-conversion) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %0 = tensor.empty() : tensor<1x3xf32>
  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %3 = tensor.empty() : tensor<3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3xf32>
  return %4 : tensor<3xf32>
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
    return %0 : tensor<3xf32>
  }
  func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
    %cst = arith.constant 0.000000e+00 : f32
    %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %0 = tensor.empty() : tensor<1x3xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %3 = tensor.empty() : tensor<3xf32>
    %4 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3xf32>
    return %4 : tensor<3xf32>
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return
'(//===-------------------------------------------===//
0x56223b0d3730Processing operation : ') {
func.return  '(0x56223b0d3620) {
  "func.return"(%9) : (tensor<3xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "func.ret"ulrinn"a(lg.%y0i)el : d("(%10) : tensor<(3f32x) -> (f32)>) -> 

()} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(

0x7f71f0006060) {
} -> failure : pattern failed to match
//===-------------------------------------------===//
} -> 
failure//===-------------------------------------------===//
 : Processing operation : 'pattern failed to matcharith.addf
'(//===-------------------------------------------===//
0x7f71f001fe60) {

  //===-------------------------------------------===//
Processing operation : 'func.call'(0x56223b9b0500) {
  %10 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %0 = "func.call"(%arg0%)8 = "tensor.empty"() < : ({) -> calleetensor< = 3x@f32forward>}

> : } -> (failure : pattern failed to match
//===-------------------------------------------===//

tensor<//===-------------------------------------------===//
4Processing operation : 'xutil.global.load'(f320x56223b0bed00>) {
) ->   tensor<3xf32>

} -> failure : pattern failed to match%
7//===-------------------------------------------===//
 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %6 = "tensor.collapse_shap// -----// IR Dump After eConvertTMTensorToLinalgExt" ((torch-iree-tm-tensor-to-linalg-ext%)5 //----- //
)mlir-asm-printer: Verifying operation:  <func.func{
reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%11) : (f32) -> ()

} -> failure : func.funcpattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
 Processing operation : '@arith.addfmain'(0x7f71f0005520() {
  %arg0: tensor<4xf32>) -> tensor<3xf32>%11 = "arith.addf"(%arg3, %10) <{fastmath = #arith.fastmath<none> attributes} {>torch.args_schema :  = ("f32[, 1f32,) ->  f32{\

22ty} -> pfailuree : \pattern failed to match2
2//===-------------------------------------------===//
:
 //===-------------------------------------------===//
\Processing operation : '2linalg.matmul2'(b0x7f71f0020f30u) {
ilti} -> nfailures : .pattern failed to matcht
u//===-------------------------------------------===//
p
l//===-------------------------------------------===//
eProcessing operation : '\arith.mulf2'(20x7f71f0021510,) {
   \22context\22: \22null\2%210, =  "\a2r2icthhi.lmdurlefn"_(s%parg1e, c%\arg22)2:  <[{{fastmath\ = 2#2aritht.yfastmath<none>p}e>\ : 2(2f32:,  f32\) -> 2f322b

uil} -> tfailurei : npattern failed to matchs
.//===-------------------------------------------===//
l
i//===-------------------------------------------===//
sProcessing operation : 'tlinalg.fill\'(20x56223b0ece802) {
, \22} -> cfailureo : npattern failed to matcht
e//===-------------------------------------------===//
x
t//===-------------------------------------------===//
\Processing operation : '2linalg.yield2'(:0x7f71f0005960 ) {
\  22null\22, \22children_s"pleicn\a2l2g:. y[i{e\l2d2"t(y%parg1e)\ : 2(2f32:) ->  (n)ul

l,} ->  failure\ : 2pattern failed to match2
c//===-------------------------------------------===//
o
n//===-------------------------------------------===//
tProcessing operation : 'etensor.emptyx'(t0x7f71f00092b0\) {
2  2: null, \22children_spe%c3\ = 2"2t:ensor. e[m]p}t]y}",( ){ : \(2) -> 2ttensor<y1pxe3\x2f322>: 

\2} -> 2failureb : upattern failed to matchi
l//===-------------------------------------------===//
t
i//===-------------------------------------------===//
nProcessing operation : 'sutil.global.load.'(d0x56223b0bdb30i) {
c  t\22, \22context\22: \22%[2] = \"2u2t,i l\.2g2lcohbialld.rleona_ds"p(e)c\2 <2{:global  = [@]_params.weight}}]>} : ](") -> tensor<4, xtorch.assume_strict_symbolic_shapes3, xtorch.return_schemaf32 = >"[

1,} ->  failure{ : \pattern failed to match2
2//===-------------------------------------------===//
t
y//===-------------------------------------------===//
pProcessing operation : 'etensor.expand_shape\'(20x7f71f00085602) {
:   null, \22context\22: nu%l1l = ," t\e2n2scohri.ledxrpeann_ds_psehca\p2e2":( %[arg0])}] <"{reassociation} =  [{[
0, 1]]}  > : %(0tensor< = 4xf32>call) ->  tensor<1x4@xforwardf32(>%arg0

)} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant '(:0x7f71f00160c0 ) {
(  tensor<4xf32>) -> tensor<3xf32>
  return% 0 = %"0a r:i th.ctensor<o3nxstf32a>n
t}"() <{

value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ConvertTMTensorToLinalgExt (torch-iree-tm-tensor-to-linalg-ext) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %0 = tensor.empty() : tensor<1x3xf32>
  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %3 = tensor.empty() : tensor<3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3xf32>
  return %4 : tensor<3xf32>
}


//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x56223b0ed3b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b9b0500) {
  %0 = "func.call"(%arg0) <{callee = @forward}> : (tensor<4xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%0) : (tensor<3xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %1 = "tensor.expand_shape"(%arg0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %2 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f00092b0) {
  %3 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x56223b0ece80) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0021510) {
  %10 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0005520) {
  %11 = "arith.addf"(%arg3, %10) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%11) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %6 = "tensor.collapse_shape"(%5) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %7 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %8 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %10 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%10) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%9) : (tensor<3xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After IREEImportPublic (iree-import-public) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
    return %0 : tensor<3xf32>
  }
  func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
    %cst = arith.constant 0.000000e+00 : f32
    %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %0 = tensor.empty() : tensor<1x3xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %3 = tensor.empty() : tensor<3xf32>
    %4 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3xf32>
    return %4 : tensor<3xf32>
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x56223b0ed3b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d36a0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x56223b9b0500) {
  %0 = "func.call"(%arg0) <{callee = @forward}> : (tensor<4xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%0) : (tensor<3xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0bcbc0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %1 = "tensor.expand_shape"(%arg0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %2 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f00092b0) {
  %3 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x56223b0ece80) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0021510) {
  %10 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0005520) {
  %11 = "arith.addf"(%arg3, %10) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%11) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %6 = "tensor.collapse_shape"(%5) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %7 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %8 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %10 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%10) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%9) : (tensor<3xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ImportMLProgram (iree-import-ml-program) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
    return %0 : tensor<3xf32>
  }
  func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
    %cst = arith.constant 0.000000e+00 : f32
    %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %0 = tensor.empty() : tensor<1x3xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %3 = tensor.empty() : tensor<3xf32>
    %4 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3xf32>
    return %4 : tensor<3xf32>
  }
}


// -----// IR Dump After SanitizeModuleNames (iree-sanitize-module-names) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
    return %0 : tensor<3xf32>
  }
  func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
    %cst = arith.constant 0.000000e+00 : f32
    %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %0 = tensor.empty() : tensor<1x3xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %3 = tensor.empty() : tensor<3xf32>
    %4 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3xf32>
    return %4 : tensor<3xf32>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::ConvertStreamableOpsPass (iree-abi-convert-streamable-ops) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
    return %0 : tensor<3xf32>
  }
  func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
    %cst = arith.constant 0.000000e+00 : f32
    %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %0 = tensor.empty() : tensor<1x3xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %3 = tensor.empty() : tensor<3xf32>
    %4 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3xf32>
    return %4 : tensor<3xf32>
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<1>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::TiedOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::ShapeAwareOpInterface::Trait<Empty>)
// -----// IR Dump After mlir::iree_compiler::IREE::ABI::WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = call @_main(%0) : (tensor<4xf32>) -> tensor<3xf32>
    %2 = hal.tensor.export %1 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
  func.func private @_main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
    return %0 : tensor<3xf32>
  }
  func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
    %cst = arith.constant 0.000000e+00 : f32
    %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %0 = tensor.empty() : tensor<1x3xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %3 = tensor.empty() : tensor<3xf32>
    %4 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3xf32>
    return %4 : tensor<3xf32>
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallGraph)

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %1 = "tensor.expand_shape"(%arg0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %2 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %3 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %10 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %11 = "arith.addf"(%arg3, %10) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%11) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %6 = "tensor.collapse_shape"(%5) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %7 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %8 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %10 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%10) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3730) {
  "func.return"(%9) : (tensor<3xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @forward(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.assume_strict_symbolic_shapes} {
  %cst = arith.constant 0.000000e+00 : f32
  %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %0 = tensor.empty() : tensor<1x3xf32>
  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %3 = tensor.empty() : tensor<3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3xf32>
  return %4 : tensor<3xf32>
}

* Inliner: Initial calls in SCC are: {
}

//===-------------------------------------------===//
Processing operation : 'func.call'(0x56223b9b0500) {
  %0 = "func.call"(%arg0) <{callee = @forward}> : (tensor<4xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%0) : (tensor<3xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @_main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %0 = call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
  return %0 : tensor<3xf32>
}

* Inliner: Initial calls in SCC are: {
  0. mlir-asm-printer: Verifying operation: func.func
%0 = func.call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>,
}
* Inlining call: 0. mlir-asm-printer: Verifying operation: func.func
%0 = func.call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>
* new inlineHistory entry: 0. [mlir-asm-printer: Verifying operation: func.func
%0 = func.call @forward(%arg0) : (tensor<4xf32>) -> tensor<3xf32>, root]

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %1 = "tensor.expand_shape"(%arg0) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %2 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %3 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %10 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %11 = "arith.addf"(%arg3, %10) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%11) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %6 = "tensor.collapse_shape"(%5) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %7 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %8 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %10 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%10) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b0d3620) {
  "func.return"(%9) : (tensor<3xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func private @_main(%arg0: tensor<4xf32>) -> tensor<3xf32> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.assume_strict_symbolic_shapes, torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
  %cst = arith.constant 0.000000e+00 : f32
  %expanded = tensor.expand_shape %arg0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %0 = tensor.empty() : tensor<1x3xf32>
  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %2 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %2 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %3 = tensor.empty() : tensor<3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%3 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3xf32>
  return %4 : tensor<3xf32>
}

* Inliner: Initial calls in SCC are: {
}

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %0 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x56223b96f000) {
  %1 = "func.call"(%0) <{callee = @_main}> : (tensor<4xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %2 = "hal.tensor.export"(%1) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%2) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = call @_main(%0) : (tensor<4xf32>) -> tensor<3xf32>
  %2 = hal.tensor.export %1 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

* Inliner: Initial calls in SCC are: {
  0. mlir-asm-printer: Verifying operation: func.func
%1 = func.call @_main(%0) : (tensor<4xf32>) -> tensor<3xf32>,
}
* Inlining call: 0. mlir-asm-printer: Verifying operation: func.func
%1 = func.call @_main(%0) : (tensor<4xf32>) -> tensor<3xf32>
* new inlineHistory entry: 0. [mlir-asm-printer: Verifying operation: func.func
%1 = func.call @_main(%0) : (tensor<4xf32>) -> tensor<3xf32>, root]

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

* Inliner: Initial calls in SCC are: {
}
* Inliner: Initial calls in SCC are: {
}
// -----// IR Dump After Inliner (inline) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x56223b0ed3b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d3460) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x56223b0ece80) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After DemoteF64ToF32 (iree-util-demote-f64-to-f32) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'func.return -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'hal.tensor.export -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'linalg.yield -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'linalg.generic -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'arith.addf -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'tensor.empty -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'util.global.load -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'linalg.yield -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'arith.addf -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'linalg.matmul -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'arith.mulf -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'linalg.fill -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'linalg.yield -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'tensor.empty -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'util.global.load -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'tensor.expand_shape -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'hal.tensor.import -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32


  * Pattern mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands : 'arith.constant -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands"
"mlir::iree_compiler::GlobalOptimization::ReplaceZeroExtentOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After RemoveZeroExtentTensors (iree-global-opt-remove-zero-extent-tensors) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern : 'linalg.generic -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern"
"mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp> : 'linalg.generic -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp>"
"mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern : 'linalg.matmul -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern"
"mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp> : 'linalg.matmul -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp>"
"mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern : 'linalg.fill -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern"
"mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachElementwisePattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp> : 'linalg.fill -> ()' {
Trying to match "mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp>"
"mlir::iree_compiler::GlobalOptimization::(anonymous namespace)::DetachSplatConstantOutsOperands<mlir::linalg::LinalgOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After DetachElementwiseFromNamedOps (iree-global-opt-detach-elementwise-from-named-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After LinalgNamedOpConversion (linalg-named-op-conversion) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Convert1X1FilterConv2DToMatmul (iree-global-opt-convert-1x1-filter-conv2d-to-matmul) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern (anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults"
"(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveUnusedCycleInGenericOp"
"(anonymous namespace)::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After EraseUnusedLinalgOperands (iree-global-opt-erase-unused-linalg-operands) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After ExpandTensorShapes (iree-flow-expand-tensor-shapes) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d3460) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x56223b0ece80) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f0006060) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
// -----// IR Dump After ConvertElementwiseToLinalg (convert-elementwise-to-linalg) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After GeneralizeLinalgNamedOps (iree-flow-generalize-linalg-named-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 1
: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 1
: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 1
: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
// -----// IR Dump After RaiseSpecialOps (iree-flow-raise-special-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern (anonymous namespace)::DropUnitDims : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::DropUnitDims"
"(anonymous namespace)::DropUnitDims" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::MoveInitOperandsToInput : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::MoveInitOperandsToInput"
"(anonymous namespace)::MoveInitOperandsToInput" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After FoldUnitExtentDims (iree-flow-fold-unit-extent-dims) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FuseDequantizationMatmulPattern : 'linalg.generic -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FuseDequantizationMatmulPattern"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FuseDequantizationMatmulPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After FuseDequantizationMatmul (iree-flow-fuse-dequantization-matmul) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %1 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %2 = "tensor.expand_shape"(%1) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %3 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %7 = "tensor.collapse_shape"(%6) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After MaterializeHomogeneousEncodings (iree-global-opt-materialize-homogeneous-encodings) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


moving immutable global _params.weight load to the entry block
moving immutable global _params.bias load to the entry block
==== REARRANGING BLOCK ACCESSES ====
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IREE::Util::YieldPoint<Empty>)
// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====
// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: builtin.module
  !! traversal incomplete due to public function-like op @main
mlir-asm-printer: Verifying operation: builtin.module
FuncAnalysis: INCOMPLETE! @main(!hal.buffer_view) -> !hal.buffer_view 
  args: 1
    %arg0: non-uniform used !hal.buffer_view 
  results: 1
    %result#0: non-uniform used !hal.buffer_view 
  callOps: 0
// -----// IR Dump After IPO (iree-util-ipo) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::ConstExprAnalysis)
mlir-asm-printer: Verifying operation: builtin.module
CONSTANT ROOT: mlir-asm-printer: Verifying operation: builtin.module
%_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
CONSTANT ROOT: mlir-asm-printer: Verifying operation: builtin.module
%_params.bias = util.global.load @_params.bias : tensor<3xf32>
CONSTANT ROOT: mlir-asm-printer: Verifying operation: builtin.module
%cst = arith.constant 0.000000e+00 : f32
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%1 = tensor.empty() : tensor<1x3xf32>
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  EXPAND TO UNKNOWN: mlir-asm-printer: Verifying operation: builtin.module
%4 = tensor.empty() : tensor<3xf32>
PROCESS WORKLIST:
  RESOLVED AS NON_CONSTANT: mlir-asm-printer: Verifying operation: builtin.module
%expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  RESOLVED AS CONSTANT: mlir-asm-printer: Verifying operation: builtin.module
%1 = tensor.empty() : tensor<1x3xf32>
  RESOLVED AS CONSTANT: mlir-asm-printer: Verifying operation: builtin.module
%4 = tensor.empty() : tensor<3xf32>
PROCESS WORKLIST:
  RESOLVED AS NON_CONSTANT: mlir-asm-printer: Verifying operation: builtin.module
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  RESOLVED AS CONSTANT: mlir-asm-printer: Verifying operation: builtin.module
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  RESOLVED AS NON_CONSTANT: mlir-asm-printer: Verifying operation: builtin.module
%collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
PROCESS WORKLIST:
  RESOLVED AS NON_CONSTANT: mlir-asm-printer: Verifying operation: builtin.module
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>

FOUND CONSTANTS:
----------------

::mlir-asm-printer: Verifying operation: builtin.module
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    WITH ROOTS:
      mlir-asm-printer: Verifying operation: builtin.module
%cst = arith.constant 0.000000e+00 : f32
    WITH PRODUCERS:
      mlir-asm-printer: Verifying operation: builtin.module
%cst = arith.constant 0.000000e+00 : f32
      mlir-asm-printer: Verifying operation: builtin.module
%1 = tensor.empty() : tensor<1x3xf32>


ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%_params.bias = util.global.load @_params.bias : tensor<3xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%cst = arith.constant 0.000000e+00 : f32
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%1 = tensor.empty() : tensor<1x3xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
ConstExprHoistPolicy(INVARIANT, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%4 = tensor.empty() : tensor<3xf32>
ConstExprHoistPolicy(0, DISABLE_HOIST): mlir-asm-printer: Verifying operation: builtin.module
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
ConstExprHoistPolicy(1, CONVERGED)
// -----// IR Dump After HoistIntoGlobals (iree-util-hoist-into-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After JitGlobals (iree-consteval-jit-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInputLegality (iree-verify-input-legality) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After TensorPadToTensorInsertSlice (iree-flow-tensor-pad-to-tensor-insert-slice) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
    %1 = tensor.empty() : tensor<1x3xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
    %4 = tensor.empty() : tensor<3xf32>
    %5 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3xf32>
    %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 1
: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] start recursive lhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 1
: 0
[transform-matchers] -------
[transform-matchers] } end recursive match[transform-matchers] start recursive rhs OR match {
[transform-matchers] matching: mlir-asm-printer: Verifying operation: func.func
%5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
^bb0(%in: f32, %in_0: f32, %out: f32):
  %7 = arith.addf %in, %in_0 : f32
  linalg.yield %7 : f32
} -> tensor<3xf32>
[transform-matchers] op is a linalg interface implementation': 1
[transform-matchers] operation type is one of {linalg.generic}: 1
: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
[transform-matchers] } end recursive match: 0
[transform-matchers] -------
// -----// IR Dump After RaiseSpecialOps (iree-flow-raise-special-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::GenericOpInterchangePattern : 'linalg.generic -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::GenericOpInterchangePattern"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::GenericOpInterchangePattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern (anonymous namespace)::CollapseLinalgDimensions<mlir::linalg::GenericOp> : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::CollapseLinalgDimensions<mlir::linalg::GenericOp>"
"(anonymous namespace)::CollapseLinalgDimensions<mlir::linalg::GenericOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After CollapseDims (iree-flow-collapse-dims) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f001fe60) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %_params.bias : tensor<3xf32>, tensor<3xf32>) outs(%4 : tensor<3xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f001ff00) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f0006060) {

  * Pattern (anonymous namespace)::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FuseElementwiseOps"
"(anonymous namespace)::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithGenericOp"
"(anonymous namespace)::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldScalarOrSplatConstant"
"(anonymous namespace)::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveOutsDependency"
"(anonymous namespace)::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults"
"(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveUnusedCycleInGenericOp"
"(anonymous namespace)::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldWithProducerReshapeOpByExpansion"
    ** Insert  : 'tensor.expand_shape'(0x56223b8f7290)
    ** Insert  : 'tensor.expand_shape'(0x56223be1f210)
    ** Insert  : 'linalg.generic'(0x56223be1ffb0)
    ** Insert  : 'tensor.collapse_shape'(0x56223be224d0)
    ** Replace : 'linalg.generic'(0x7f71f0006060)
    ** Modified: 'hal.tensor.export'(0x56223b9ed920)
    ** Erase   : 'linalg.yield'(0x7f71f001ff00)
    ** Erase   : 'arith.addf'(0x7f71f001fe60)
    ** Erase   : 'linalg.generic'(0x7f71f0006060)
"(anonymous namespace)::FoldWithProducerReshapeOpByExpansion" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %expanded_1 = tensor.expand_shape %4 [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%expanded_1 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_3: f32, %out: f32):
    %7 = arith.addf %in, %in_3 : f32
    linalg.yield %7 : f32
  } -> tensor<1x3xf32>
  %collapsed_2 = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %6 = hal.tensor.export %collapsed_2 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %14 = "hal.tensor.export"(%13) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x56223be224d0) {
  %13 = "tensor.collapse_shape"(%12) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1ffb0) {

  * Pattern (anonymous namespace)::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FuseElementwiseOps"
"(anonymous namespace)::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithGenericOp"
"(anonymous namespace)::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldScalarOrSplatConstant"
"(anonymous namespace)::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveOutsDependency"
    ** Insert  : 'tensor.empty'(0x56223be225c0)
    ** Modified: 'linalg.generic'(0x56223be1ffb0)
"(anonymous namespace)::RemoveOutsDependency" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = tensor.empty() : tensor<3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %expanded_1 = tensor.expand_shape %4 [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %5 = tensor.empty() : tensor<1x3xf32>
  %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%5 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_3: f32, %out: f32):
    %8 = arith.addf %in, %in_3 : f32
    linalg.yield %8 : f32
  } -> tensor<1x3xf32>
  %collapsed_2 = tensor.collapse_shape %6 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %7 = hal.tensor.export %collapsed_2 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1ffb0) {

  * Pattern (anonymous namespace)::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FuseElementwiseOps"
"(anonymous namespace)::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithGenericOp"
"(anonymous namespace)::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldScalarOrSplatConstant"
"(anonymous namespace)::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveOutsDependency"
"(anonymous namespace)::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults"
"(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveUnusedCycleInGenericOp"
"(anonymous namespace)::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldWithProducerReshapeOpByExpansion"
"(anonymous namespace)::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldConstantTranspose"
"(anonymous namespace)::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be225c0) {
  %12 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223be1f210) {
  %11 = "tensor.expand_shape"(%9) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>

  ** Erase   : 'tensor.expand_shape'(0x56223be1f210)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223b8f7290) {
  %10 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::FoldReshapeWithGenericOpByExpansion : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithGenericOpByExpansion"
    ** Failure : producer not a generic op
"(anonymous namespace)::FoldReshapeWithGenericOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f001eb60) {
  %9 = "tensor.empty"() : () -> tensor<3xf32>

  ** Erase   : 'tensor.empty'(0x7f71f001eb60)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x7f71f001e6f0) {
  %8 = "tensor.collapse_shape"(%7) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

  ** Erase   : 'tensor.collapse_shape'(0x7f71f001e6f0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %14 = "arith.addf"(%arg3, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %13 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldReshapeWithGenericOpByExpansion : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithGenericOpByExpansion"
    ** Failure : producer not a generic op
"(anonymous namespace)::FoldReshapeWithGenericOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%12) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %12 = "hal.tensor.export"(%11) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x56223be224d0) {
  %11 = "tensor.collapse_shape"(%10) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1ffb0) {

  * Pattern (anonymous namespace)::FuseElementwiseOps : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FuseElementwiseOps"
"(anonymous namespace)::FuseElementwiseOps" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldFillWithGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithGenericOp"
"(anonymous namespace)::FoldFillWithGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldScalarOrSplatConstant : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldScalarOrSplatConstant"
"(anonymous namespace)::FoldScalarOrSplatConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveOutsDependency : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveOutsDependency"
"(anonymous namespace)::RemoveOutsDependency" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults"
"(anonymous namespace)::DeduplicateAndRemoveDeadOperandsAndResults" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::RemoveUnusedCycleInGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::RemoveUnusedCycleInGenericOp"
"(anonymous namespace)::RemoveUnusedCycleInGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldWithProducerReshapeOpByExpansion : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldWithProducerReshapeOpByExpansion"
"(anonymous namespace)::FoldWithProducerReshapeOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldConstantTranspose : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldConstantTranspose"
"(anonymous namespace)::FoldConstantTranspose" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be223d0) {
  %13 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be225c0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223b8f7290) {
  %8 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::FoldReshapeWithGenericOpByExpansion : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithGenericOpByExpansion"
    ** Failure : producer not a generic op
"(anonymous namespace)::FoldReshapeWithGenericOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %14 = "arith.addf"(%arg3, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %13 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldReshapeWithGenericOpByExpansion : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithGenericOpByExpansion"
    ** Failure : producer not a generic op
"(anonymous namespace)::FoldReshapeWithGenericOpByExpansion" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

--- After first fixed point ---
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%4 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %7 = arith.addf %in, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %6 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%12) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %12 = "hal.tensor.export"(%11) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x56223be224d0) {
  %11 = "tensor.collapse_shape"(%10) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1ffb0) {

  * Pattern (anonymous namespace)::FoldWithProducerReshapeOpByCollapsing : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::FoldWithProducerReshapeOpByCollapsing"
"(anonymous namespace)::FoldWithProducerReshapeOpByCollapsing" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be223d0) {
  %13 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be225c0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223b8f7290) {
  %8 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %14 = "arith.addf"(%arg3, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %13 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

--- After second fixed point ---
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%4 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %7 = arith.addf %in, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %6 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%12) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %12 = "hal.tensor.export"(%11) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x56223be224d0) {
  %11 = "tensor.collapse_shape"(%10) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1ffb0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be223d0) {
  %13 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be225c0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223b8f7290) {
  %8 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %14 = "arith.addf"(%arg3, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %13 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldEmptyTensorWithReshapeOp<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After FusionOfTensorOps (iree-flow-fusion-of-tensor-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%4 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %7 = arith.addf %in, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %6 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %13 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %14 = "arith.addf"(%arg3, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223b8f7290) {
  %8 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>"
"mlir::ComposeExpandOfCollapseOp<mlir::tensor::ExpandShapeOp, mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::ExpandShapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be225c0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1ffb0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be223d0) {
  %13 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x56223be224d0) {
  %11 = "tensor.collapse_shape"(%10) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern (anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldFillWithTensorReshape<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>"
"mlir::ComposeReassociativeReshapeOps<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>"
"mlir::ComposeCollapseOfExpandOp<mlir::tensor::CollapseShapeOp, mlir::tensor::ExpandShapeOp, mlir::tensor::CastOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithConstant<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithSplat<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>"
"(anonymous namespace)::FoldReshapeWithFromElements<mlir::tensor::CollapseShapeOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldCollapseOfCastOp : 'tensor.collapse_shape -> ()' {
Trying to match "(anonymous namespace)::FoldCollapseOfCastOp"
"(anonymous namespace)::FoldCollapseOfCastOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %12 = "hal.tensor.export"(%11) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%12) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = tensor.empty() : tensor<1x3xf32>
  %5 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%4 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %7 = arith.addf %in, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %5 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %6 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::PostDominanceInfo)
// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_1 : f32
    linalg.yield %6 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %4 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %5 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SplitReduction (iree-flow-split-reduction-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_1 : f32
    linalg.yield %6 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %4 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %5 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %11 = "hal.tensor.export"(%10) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x56223be224d0) {
  %10 = "tensor.collapse_shape"(%9) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%12) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1ffb0) {

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::GenericOpInterchangePattern : 'linalg.generic -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::GenericOpInterchangePattern"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::GenericOpInterchangePattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be223d0) {
  %12 = "arith.addf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223b8f7290) {
  %8 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f0005520) {
  %13 = "arith.addf"(%arg3, %12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x7f71f0020f30) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x7f71f0021510) {
  %12 = "arith.mulf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg1) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_1 : f32
    linalg.yield %6 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %4 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %5 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

Num scalar dispatches : 0
// -----// IR Dump After FormScalarDispatches (iree-flow-form-scalar-dispatches) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_1 : f32
    linalg.yield %6 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %4 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %5 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


--- After fuse with producer ---
mlir-asm-printer: Verifying operation: builtin.module
^bb0(%arg0: !hal.buffer_view):
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul {__root_op__ = 0 : i64} ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_1 : f32
    linalg.yield %6 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %4 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %5 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  func.return %5 : !hal.buffer_view



--- After deciding fusion groups ---
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = linalg.matmul {__fused_op__ = [0]} ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %4 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) attrs =  {__root_op__ = 0 : i64} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_1 : f32
    linalg.yield %6 : f32
  } -> tensor<1x3xf32>
  %collapsed = tensor.collapse_shape %4 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %5 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::detail::DispatchRegionOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NRegions<2>::Impl<Empty>)

--- After creating flow.dispatch.region ---
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.region -> (tensor<1x3xf32>) {
    %5 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %7 = arith.addf %in, %in_1 : f32
      linalg.yield %7 : f32
    } -> tensor<1x3xf32>
    flow.return %6 : tensor<1x3xf32>
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchRegions (iree-flow-form-dispatch-regions) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.region -> (tensor<1x3xf32>) {
    %5 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %7 = arith.addf %in, %in_1 : f32
      linalg.yield %7 : f32
    } -> tensor<1x3xf32>
    flow.return %6 : tensor<1x3xf32>
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

[CollapseDims] : After collapsing generic ops: 
mlir-asm-printer: Verifying operation: builtin.module
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.region -> (tensor<1x3xf32>) {
    %5 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %7 = arith.addf %in, %in_1 : f32
      linalg.yield %7 : f32
    } -> tensor<1x3xf32>
    flow.return %6 : tensor<1x3xf32>
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}
// -----// IR Dump After CollapseDimensions (iree-flow-collapse-dimensions) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.region -> (tensor<1x3xf32>) {
    %5 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%2 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%1 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %7 = arith.addf %in, %in_1 : f32
      linalg.yield %7 : f32
    } -> tensor<1x3xf32>
    flow.return %6 : tensor<1x3xf32>
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After CloneProducersIntoDispatchRegions (iree-flow-clone-producers-into-dispatch-regions) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.region -> (tensor<1x3xf32>) {
    %5 = tensor.empty() : tensor<1x3xf32>
    %cst_1 = arith.constant 0.000000e+00 : f32
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %7 = linalg.matmul ins(%expanded, %_params.weight : tensor<1x4xf32>, tensor<4x3xf32>) outs(%6 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %8 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %expanded_0 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%5 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %9 = arith.addf %in, %in_2 : f32
      linalg.yield %9 : f32
    } -> tensor<1x3xf32>
    flow.return %8 : tensor<1x3xf32>
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::detail::DispatchTensorLoadOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::detail::DispatchTensorStoreOpGenericAdaptorBase::Properties)

--- After forming of dispatch workgroups ---
mlir-asm-printer: Verifying operation: func.func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::ClosureOpInterface::Trait<Empty>)
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%expanded, %_params.weight, %expanded_0) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %5 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %6 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %7 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %8 = tensor.empty() : tensor<1x3xf32>
    %cst_1 = arith.constant 0.000000e+00 : f32
    %9 = linalg.fill ins(%cst_1 : f32) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %10 = linalg.matmul ins(%5, %6 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10, %7 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%8 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %12 = arith.addf %in, %in_2 : f32
      linalg.yield %12 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %11, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}


--- After other conversions ---
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%expanded, %_params.weight, %expanded_0) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %5 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %6 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %7 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %8 = tensor.empty() : tensor<1x3xf32>
    %cst_1 = arith.constant 0.000000e+00 : f32
    %9 = linalg.fill ins(%cst_1 : f32) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %10 = linalg.matmul ins(%5, %6 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10, %7 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%8 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %12 = arith.addf %in, %in_2 : f32
      linalg.yield %12 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %11, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x3xf32> into tensor<3xf32>
  %4 = hal.tensor.export %collapsed "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OffsetSizeAndStrideOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<2>::Impl<Empty>)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%10) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %10 = "hal.tensor.export"(%9) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.collapse_shape'(0x56223be224d0) {
  %9 = "tensor.collapse_shape"(%8) <{reassociation = [[0, 1]]}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::CollapseShapeOp> : 'tensor.collapse_shape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::CollapseShapeOp>"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::detail::TensorReshapeOpGenericAdaptorBase::Properties)
    ** Insert  : 'flow.tensor.reshape'(0x56223b1253b0)
    ** Replace : 'tensor.collapse_shape'(0x56223be224d0)
    ** Modified: 'hal.tensor.export'(0x56223b9ed920)
    ** Erase   : 'tensor.collapse_shape'(0x56223be224d0)
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::CollapseShapeOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %expanded_0 = tensor.expand_shape %_params.bias [[0, 1]] : tensor<3xf32> into tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%expanded, %_params.weight, %expanded_0) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst_1 = arith.constant 0.000000e+00 : f32
    %6 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %9 = tensor.empty() : tensor<1x3xf32>
    %10 = linalg.fill ins(%cst_1 : f32) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.matmul ins(%6, %7 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %8 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%9 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %13 = arith.addf %in, %in_2 : f32
      linalg.yield %13 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %12, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  }
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %10 = "hal.tensor.export"(%9) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::StreamableOpInterface::Trait<Empty>)
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %9 = "flow.tensor.reshape"(%8) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223be1fe80) {
  "flow.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x56223b0ee600) {
  "flow.dispatch.tensor.store"(%18, %arg4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1d420) {
  "linalg.yield"(%19) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1f100) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb7dcd0) {
  %19 = "arith.addf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%20) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb6f820) {
  %20 = "arith.addf"(%arg7, %19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223b1a5570) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be223d0) {
  %19 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223be1dc10) {

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertLinalgFillPattern : 'linalg.fill -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertLinalgFillPattern"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertLinalgFillPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %11 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f0005520) {
  %15 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223ba466e0) {
  %14 = "flow.dispatch.tensor.load"(%arg3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb8e3e0) {
  %13 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb83690) {
  %12 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x56223b8f7290) {
  %7 = "tensor.expand_shape"(%2) <{reassociation = [[0, 1]]}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::ExpandShapeOp>"
    ** Insert  : 'flow.tensor.reshape'(0x56223b125310)
    ** Replace : 'tensor.expand_shape'(0x56223b8f7290)
    ** Modified: 'flow.dispatch.workgroups'(0x56223be1ffb0)
    ** Erase   : 'tensor.expand_shape'(0x56223b8f7290)
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::ExpandShapeOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %expanded = tensor.expand_shape %0 [[0, 1]] : tensor<4xf32> into tensor<1x4xf32>
  %1 = tensor.empty() : tensor<1x3xf32>
  %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1x3xf32>) -> tensor<1x3xf32>
  %3 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %4 = flow.dispatch.workgroups(%expanded, %_params.weight, %3) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst_0 = arith.constant 0.000000e+00 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %8 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %9 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %10 = tensor.empty() : tensor<1x3xf32>
    %11 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.matmul ins(%7, %8 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%11 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %13 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %9 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%10 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %14 = arith.addf %in, %in_1 : f32
      linalg.yield %14 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %13, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  }
  %5 = flow.tensor.reshape %4 : tensor<1x3xf32> -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %7 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223b0ece80) {
  ** Erase   : 'linalg.yield'(0x7f71f0005960)
  ** Erase   : 'linalg.fill'(0x56223b0ece80)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f00092b0) {
  %5 = "tensor.empty"() : () -> tensor<1x3xf32>

  ** Erase   : 'tensor.empty'(0x7f71f00092b0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.expand_shape'(0x7f71f0008560) {
  %4 = "tensor.expand_shape"(%3) <{reassociation = [[0, 1]]}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::ExpandShapeOp> : 'tensor.expand_shape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::ExpandShapeOp>"
    ** Insert  : 'flow.tensor.reshape'(0x56223b125450)
    ** Replace : 'tensor.expand_shape'(0x7f71f0008560)
    ** Modified: 'flow.dispatch.workgroups'(0x56223be1ffb0)
    ** Erase   : 'tensor.expand_shape'(0x7f71f0008560)
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertTensorReshapePattern<mlir::tensor::ExpandShapeOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst_0 = arith.constant 0.000000e+00 : f32
    %6 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %9 = tensor.empty() : tensor<1x3xf32>
    %10 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.matmul ins(%6, %7 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %8 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%9 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %13 = arith.addf %in, %in_1 : f32
      linalg.yield %13 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %12, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  }
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %4 = "flow.tensor.reshape"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %3 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %2 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %1 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f00160c0) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

  ** Erase   : 'arith.constant'(0x7f71f00160c0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223be1fe80) {
  "flow.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x56223b0ee600) {
  "flow.dispatch.tensor.store"(%15, %arg4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1d420) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1f100) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb7dcd0) {
  %16 = "arith.addf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb6f820) {
  %17 = "arith.addf"(%arg7, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223b1a5570) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be223d0) {
  %16 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223be1dc10) {

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertLinalgFillPattern : 'linalg.fill -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertLinalgFillPattern"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertLinalgFillPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f0005520) {
  %12 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223ba466e0) {
  %11 = "flow.dispatch.tensor.load"(%arg3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb8e3e0) {
  %10 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb83690) {
  %9 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %8 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223be1fe80) {
  "flow.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x56223b0ee600) {
  "flow.dispatch.tensor.store"(%15, %arg4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::FoldInsertSliceWithTensorStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::FoldInsertSliceWithTensorStoreOp"
"mlir::iree_compiler::IREE::Flow::FoldInsertSliceWithTensorStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1d420) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1f100) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb7dcd0) {
  %16 = "arith.addf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb6f820) {
  %17 = "arith.addf"(%arg7, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223b1a5570) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be223d0) {
  %16 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223be1dc10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f0005520) {
  %12 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223ba466e0) {
  %11 = "flow.dispatch.tensor.load"(%arg3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb8e3e0) {
  %10 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb83690) {
  %9 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %8 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223be1fe80) {
  "flow.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x56223b0ee600) {
  "flow.dispatch.tensor.store"(%15, %arg4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1d420) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1f100) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb7dcd0) {
  %16 = "arith.addf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb6f820) {
  %17 = "arith.addf"(%arg7, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223b1a5570) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be223d0) {
  %16 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223be1dc10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f0005520) {
  %12 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223ba466e0) {
  %11 = "flow.dispatch.tensor.load"(%arg3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb8e3e0) {
  %10 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb83690) {
  %9 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {

  * Pattern mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Flow::DispatchWorkgroupsOp> : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Flow::DispatchWorkgroupsOp>"
"mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Flow::DispatchWorkgroupsOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ElideRedundantWorkloadValues : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ElideRedundantWorkloadValues"
"mlir::iree_compiler::IREE::Flow::ElideRedundantWorkloadValues" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ElideRedundantOperandsOfWorkgroupCountFromSliceOp : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ElideRedundantOperandsOfWorkgroupCountFromSliceOp"
"mlir::iree_compiler::IREE::Flow::ElideRedundantOperandsOfWorkgroupCountFromSliceOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceDispatchResultIfZeroElements : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceDispatchResultIfZeroElements"
"mlir::iree_compiler::IREE::Flow::ReplaceDispatchResultIfZeroElements" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %8 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NResults<3>::Impl<Empty>)
// -----// IR Dump After FormDispatchWorkgroups (iree-flow-form-dispatch-workgroups) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %6 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %9 = tensor.empty() : tensor<1x3xf32>
    %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.matmul ins(%6, %7 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %8 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%9 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %13 = arith.addf %in, %in_0 : f32
      linalg.yield %13 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %12, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CaptureDispatchDynamicDims (iree-flow-capture-dispatch-dynamic-dims) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %6 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %9 = tensor.empty() : tensor<1x3xf32>
    %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.matmul ins(%6, %7 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %8 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%9 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %13 = arith.addf %in, %in_0 : f32
      linalg.yield %13 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %12, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {

  * Pattern mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Flow::DispatchWorkgroupsOp> : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Flow::DispatchWorkgroupsOp>"
"mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Flow::DispatchWorkgroupsOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ElideRedundantWorkloadValues : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ElideRedundantWorkloadValues"
"mlir::iree_compiler::IREE::Flow::ElideRedundantWorkloadValues" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ElideRedundantOperandsOfWorkgroupCountFromSliceOp : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ElideRedundantOperandsOfWorkgroupCountFromSliceOp"
"mlir::iree_compiler::IREE::Flow::ElideRedundantOperandsOfWorkgroupCountFromSliceOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceDispatchResultIfZeroElements : 'flow.dispatch.workgroups -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceDispatchResultIfZeroElements"
"mlir::iree_compiler::IREE::Flow::ReplaceDispatchResultIfZeroElements" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %8 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb83690) {
  %9 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb8e3e0) {
  %10 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223ba466e0) {
  %11 = "flow.dispatch.tensor.load"(%arg3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f0005520) {
  %12 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223be1dc10) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223b1a5570) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be223d0) {
  %16 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb6f820) {
  %17 = "arith.addf"(%arg7, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1f100) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb7dcd0) {
  %16 = "arith.addf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1d420) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x56223b0ee600) {
  "flow.dispatch.tensor.store"(%15, %arg4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223be1fe80) {
  "flow.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %8:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%8#0, %8#1, %8#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %6 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %9 = tensor.empty() : tensor<1x3xf32>
    %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.matmul ins(%6, %7 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %8 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%9 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %13 = arith.addf %in, %in_0 : f32
      linalg.yield %13 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %12, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %6 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %9 = tensor.empty() : tensor<1x3xf32>
    %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.matmul ins(%6, %7 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %8 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%9 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %13 = arith.addf %in, %in_0 : f32
      linalg.yield %13 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %12, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%8#0, %8#1, %8#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %8:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223be1fe80) {
  "flow.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x56223b0ee600) {
  "flow.dispatch.tensor.store"(%15, %arg4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1d420) {
  "linalg.yield"(%16) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x56223be1f100) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb7dcd0) {
  %16 = "arith.addf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1e5f0) {
  "linalg.yield"(%17) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223bb6f820) {
  %17 = "arith.addf"(%arg7, %16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223b1a5570) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be223d0) {
  %16 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x56223be1dc10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f00058a0) {
  "linalg.yield"(%arg5) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x7f71f0005520) {
  %12 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::RewriteTensorEmptyToEmpty : 'tensor.empty -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::RewriteTensorEmptyToEmpty"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::RewriteTensorEmptyToEmpty" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223ba466e0) {
  %11 = "flow.dispatch.tensor.load"(%arg3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb8e3e0) {
  %10 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223bb83690) {
  %9 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroups'(0x56223be1ffb0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f0021510) {
  %8 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After InitializeEmptyTensors (iree-flow-initialize-empty-tensors) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch.workgroups(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %6 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
    %9 = tensor.empty() : tensor<1x3xf32>
    %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %11 = linalg.matmul ins(%6, %7 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%10 : tensor<1x3xf32>) -> tensor<1x3xf32>
    %12 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %8 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%9 : tensor<1x3xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %13 = arith.addf %in, %in_0 : f32
      linalg.yield %13 : f32
    } -> tensor<1x3xf32>
    flow.dispatch.tensor.store %12, %arg4, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::iree_compiler::IREE::Flow::ExecutableOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::iree_compiler::IREE::Flow::ExecutableEndOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Flow::detail::ExecutableExportOpGenericAdaptorBase::Properties)
// -----// IR Dump After OutlineDispatchRegions (iree-flow-outline-dispatch-regions) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// linalg.fill cost: 3
// new best op: 'linalg.fill', cost: 3
// linalg.matmul cost: 12
// new best op: 'linalg.matmul', cost: 12
// linalg.generic cost: 3
// best op summary: 'matmul_1x3x4_f32'
// -----// IR Dump After AnnotateDispatches (iree-flow-annotate-dispatches) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IREE::Util::DebugOnly<Empty>)

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  // -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
mlir-asm-printer: Verifying operation: flow.executable
%1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  flow.executable private @main_dispatch_0 {
  flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, %index5,  = index")f l{o
w.    dis%pxa, t%cyh, "%(z% = 3, flow.dispatch.workgroup_count_from_slice% 0
,     %flow.return4 )%x, %y, %z : index, index, index
  } <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32
]  , builtin.moduleoperandSegmentSizes  = {
array<    ifunc.func32:  @0main_dispatch_0_matmul_1x3x4_f32, (3%, arg00: , 0>, tied_operands = ![flow-1.dispatch.tensor<readonly:tensor<1x4xf32>> : , index%]arg1}: > : (!tensor<flow1.xdispatch.tensor<readonly:tensor<4x3xf32>>4, x%f32arg2>: , tensor<4!xflow3.xdispatch.tensor<readonly:tensor<1x3xf32>>f32, >%, arg3: tensor<1x3x!f32flow>.) -> dispatch.tensor<writeonly:tensor<1x3xf32>>)tensor<1x3xf32> {


      %cst
 =   * Pattern arith.constantmlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
 "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 00.000000e+00
 :   } -> f32failure
 :       pattern failed to match%
0} ->  = failure : flow.dispatch.tensor.loadpattern failed to match 
%//===-------------------------------------------===//
arg0
,//===-------------------------------------------===//
 Processing operation : 'offsetsflow.tensor.reshape '(=0x56223b1253b0 ) {
[  0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<%16x = 4"xff32l>o
w      .%t1e = nsflow.dispatch.tensor.loado r%.arg1r,e soffsetsh a=p e["0(, %05]), sizes = [4, 3] <,{ operandSegmentSizesstrides =  =array< [i132, : 1]1, 0, 0>}> : (tensor<1x3x f32:> ) -> tensor<3xf32!>flow.dispatch.tensor<readonly:tensor<4x3xf32>> ->

 tensor<4x3xf32>
      
%  2* Pattern  = mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.dispatch.tensor.loadflow.tensor.reshape  -> (%)' {
arg2,Trying to match " mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>offsets"
 = "[mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>0" result , 00
]  ,} ->  failuresizes :  pattern failed to match=
 
[  1* Pattern , mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>3 : ']flow.tensor.reshape, -> ( )' {
stridesTrying to match " mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>="
 ["1mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>, " result 10]
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> " result :0 
  } -> failure : pattern failed to match
!
flow  .* Pattern dispatch.tensor<readonly:tensor<1x3xf32>>mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>  : '->flow.tensor.reshape  -> ()' {
tensor<Trying to match "1mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>x"
3x"f32mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>>" result 
0      
%  3} ->  = failure : tensor.emptypattern failed to match(
)} ->  failure: :  pattern failed to match
tensor<//===-------------------------------------------===//
1
x//===-------------------------------------------===//
3Processing operation : 'xhal.tensor.exportf32'(>0x56223b9ed920
) {
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
      %5 = linalg.matmul ins(%0%, 7% = 1" : haltensor<.1txe4nxsf32o>r, .tensor<e4xxp3oxrf32t>")( outs(%%64) :  {tensor<name1 = x3"xof32u>t)p -> uttensor< 10x"3x, f32operandSegmentSizes> = 
      array<%6i = 32: linalg.generic 1liuyinuo generic, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view{

indexing_maps = [affine_map<} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  (d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<"func(.dr0e, tdu1r)n -> ("d(0%, 7d)1 : )(>], !iterator_typeshal = .[buffer_view") -> p(a)rallel

", "pa} -> rfailurea : lpattern failed to matchl
e//===-------------------------------------------===//
l"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
      ^bb0(%in: f32, %in_0: f32, %out: f32):
        %7 = arith.addf %in, %in_0 : f32
        linalg.yield %7 : f32
      } -> tensor<1x3xf32>
      flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After DeduplicateExecutables (iree-flow-deduplicate-executables) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After CleanupTensorShapes (iree-flow-cleanup-tensor-shapes) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<
1//===-------------------------------------------===//
xProcessing operation : '3flow.executable.exportx'(f320x56223bb736a0>) {

} ->   failure% : 3pattern failed to match = 
//===-------------------------------------------===//
flow.dispatch
 //===-------------------------------------------===//
Processing operation : '@flow.dispatch.workgroup_count_from_slicemain_dispatch_0'(::0x56223be224f0@) {
main_dispatch_0_matmul_1x3x4_f32  (%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> %tensor<01:x33 = x"f32f>lo
w  .%d4i = spflow.tensor.reshapea t%c3h .:w ortensor<k1gxr3oxuf32p>_ c->o untensor<t3_xff32r>om_slice"(
)   : %(5) ->  = (hal.tensor.exportindex , index%, 4index )"outp

ut 0"} ->  failure: :  pattern failed to match
//===-------------------------------------------===//
tensor<
3//===-------------------------------------------===//
xProcessing operation : 'f32flow.return>'( 0x56223ba10a20->) {
   !hal.buffer_view
  return %5 : !hal.buffer_view
}

"flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 
0//===-------------------------------------------===//

Processing operation : '  util.global.load} -> '(failure0x56223b0bdb30 : ) {
pattern failed to match  
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %0 = "util.glo%b2a = l".fllooawd."d(i)spatch. <t{eglobaln = so@r_params.weight.}l>o : a(d) -> "(%tensor<arg14)x3xf32>

 <{} -> operandSegmentSizesfailure =  : pattern failed to matcharray<
//===-------------------------------------------===//
i
32//===-------------------------------------------===//
: Processing operation : 'util.global.load1'(, 0x56223b0bed000) {
,   0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> %tensor<14 = x"3uxtf32i>l.glo

bal.loa
d  "* Pattern (mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims) : 'flow.dispatch.tensor.load -> ()' {
 <Trying to match "{mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDimsglobal"
 = @_params.bias}>" : mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims(" result ) -> 0
  tensor<} -> 3failurex : f32pattern failed to match>

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor

 : 'flow.dispatch.tensor.load -> ()' {
} -> Trying to match "failuremlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : "
pattern failed to match"
mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor//===-------------------------------------------===//
" result 
0//===-------------------------------------------===//

Processing operation : '  hal.tensor.import} -> '(failure0x56223b9b1700 : ) {
pattern failed to match  

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes% = 3 = array<"fil32o: w.1d, i0s, p0a>t, ctarget_encodingh = .ttensor<e4nxsf32o>r}. : l(oad!"hal(.%buffer_viewarg2) -> )tensor<4xf32>

 <{operandSegmentSizes = } -> array<failure : ipattern failed to match32
: //===-------------------------------------------===//
1
, //===-------------------------------------------===//
0Processing operation : ', flow.tensor.reshape0'(, 0x56223b1254500) {
,   0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1%x33 = x"f32f>low.te

nsor.re
s  h* Pattern amlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDimsp : 'eflow.dispatch.tensor.load" -> (()' {
%2Trying to match ")mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
 <"{mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDimsoperandSegmentSizes" result  = 0
array<  } -> ifailure32 : : pattern failed to match
1
,   0* Pattern , mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor0 : '>flow.dispatch.tensor.load} -> (>)' {
 : Trying to match "(mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
tensor<"4mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensorx" result f320>
) ->   } -> tensor<failure1 : xpattern failed to match4
x
f32  >* Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> (

)' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0

    } -> * Pattern failuremlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> :  : 'pattern failed to matchflow.tensor.reshape
 -> (} -> )' {
failure : Trying to match "pattern failed to matchmlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>
"
//===-------------------------------------------===//

"//===-------------------------------------------===//
mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>Processing operation : '" result tensor.empty0'(
0x56223be1e0f0  ) {
} ->   failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
%"4mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> = " result "0t
e  n} -> sfailureo : rpattern failed to match.
e} -> mfailurep : tpattern failed to matchy
"//===-------------------------------------------===//
(
)//===-------------------------------------------===//
 : Processing operation : '(flow.tensor.reshape) -> '(0x56223b125310) {
tensor<  1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {
%4 = "flow
.  t* Pattern eFoldTensorCastProducerOpn : 'slinalg.fillo -> (r)' {
.rTrying to match "eFoldTensorCastProducerOps"
hape""FoldTensorCastProducerOp(" result %01
)  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp < : '{linalg.filloperandSegmentSizes -> ( = )' {
array<Trying to match "(anonymous namespace)::EraseDeadLinalgOpi"
32: "(anonymous namespace)::EraseDeadLinalgOp1" result , 00
,   0} -> >failure} : >pattern failed to match : 
(
  tensor<* Pattern 3(anonymous namespace)::InferStaticShapeOfOperandsx : 'f32linalg.fill> -> () -> )' {
tensor<Trying to match "1(anonymous namespace)::InferStaticShapeOfOperandsx"
3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
"Trying to match "(anonymous namespace)::InferStaticShapeOfOperandsmlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result "
0
"  mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>} -> " result failure0 : 
pattern failed to match  
} -> } -> failurefailure :  : pattern failed to matchpattern failed to match


//===-------------------------------------------===//
  
* Pattern //===-------------------------------------------===//
mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>Processing operation : ' : 'linalg.yieldflow.tensor.reshape'( -> (0x7f71f0005960)' {
) {
Trying to match "  mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {
%5 = "flow.dispat
c  h* Pattern "FoldTensorCastProducerOp( : '%linalg.matmul3 -> (, )' {
%0Trying to match ", FoldTensorCastProducerOp%"
4)"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match
 <
{  entry_points* Pattern  = (anonymous namespace)::EraseDeadLinalgOp[ : '@linalg.matmulmain_dispatch_0 -> (::)' {
@Trying to match "main_dispatch_0_matmul_1x3x4_f32(anonymous namespace)::EraseDeadLinalgOp]"
, operandSegmentSizes" = (anonymous namespace)::EraseDeadLinalgOparray<" result 0i
32  : } -> 0failure,  : 3pattern failed to match, 
0
,   0* Pattern >(anonymous namespace)::InferStaticShapeOfOperands,  : 'tied_operandslinalg.matmul =  -> ([)' {
-1Trying to match " : (anonymous namespace)::InferStaticShapeOfOperandsindex"
]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32">(anonymous namespace)::InferStaticShapeOfOperands" result 0
  

} -> failure : pattern failed to match
} -> failure : 
pattern failed to match  
* Pattern //===-------------------------------------------===//
mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs
 : '//===-------------------------------------------===//
flow.dispatchProcessing operation : ' -> (arith.mulf)' {
'(Trying to match "0x56223be26e70mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs) {
"
  "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %8 = "arith.mulf"(%arg4, %arg5)%6 =  <"{ffastmathl = ow.t#earithn.sfastmath<none>o}r>. : r(esf32h, af32p) -> ef32"(%5)

 <{operandSegmentSizes = 
array<  * Pattern i(anonymous namespace)::MulFOfNegF32 : ': arith.mulf1 -> (, arith.mulf0)' {
, 0>Trying to match "}(anonymous namespace)::MulFOfNegF>"
 : (  tensor<  1** Failure : x3There's no operation that defines operand 0 of castedOp0x
f32">(anonymous namespace)::MulFOfNegF) -> " result 0tensor<
3  x} -> f32failure> : pattern failed to match
} -> 

failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(
0x7f71f00160c0  ) {
* Pattern   mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  %} -> 9failure =  : "pattern failed to matcha
r
i  t* Pattern hmlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>. : 'aflow.tensor.reshaped -> (d)' {
fTrying to match ""mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>("
%arg6", mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>%" result 80)
  } -> failure : pattern failed to match
 <} -> {failurefastmath :  = pattern failed to match
#//===-------------------------------------------===//
arith
.//===-------------------------------------------===//
fastmath<none>Processing operation : '}hal.tensor.export>'( : 0x56223b9ed920() {
  f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
%  7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<"3lxif32n>a}l : g(.ytensor<i3exlf32d>") -> (%!9hal). : buffer_view(f32) -> 

()

} -> failure : pattern failed to match} -> 
failure//===-------------------------------------------===//
 : 
pattern failed to match//===-------------------------------------------===//

Processing operation : '//===-------------------------------------------===//
func.return
'(//===-------------------------------------------===//
0x56223b8f73c0Processing operation : ') {
linalg.generic  '(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern "(anonymous namespace)::InferStaticShapeOfOperandsf : 'ulinalg.genericn -> (c)' {
.Trying to match "r(anonymous namespace)::InferStaticShapeOfOperandse"
turn"(%7) : (!hal.buffer_view) -> ()

} -> failure : "pattern failed to match(anonymous namespace)::InferStaticShapeOfOperands
" result //===-------------------------------------------===//
0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  // -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
"linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.func.functen s@omainr.(s%targ0o: re"(!%hal7., buffer_view%)arg3 -> )!hal.buffer_view <{operandSegmentSizes attributes =  {iree.abi.stubarray<} i{32
: 1  , %1_params.weight,  = 0, util.global.load0 , 0@, _params.weight0>, static_offsets =  array<:i 64: tensor<04, x03>x, f32static_sizes> = 
array<  i%64_params.bias:  = 1, util.global.load3 >, @static_strides_params.bias = array<i64 : :1 , 1tensor<>3}x>f32 : >(
  tensor<%10x = 3xhal.tensor.importf32> , %arg0 "!iflown.pdispatch.tensor<writeonly:tensor<1x3xf32>>u) -> t( )0" : 

!hal.buffer_view ->
   * Pattern tensor<mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder4 : 'xflow.dispatch.tensor.storef32 -> (>)' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"

  "%mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder1" result  = 0
flow.tensor.reshape   } -> %failure0 :  pattern failed to match:
 
  tensor<* Pattern 4mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOpx : 'f32flow.dispatch.tensor.store> -> ( )' {
->Trying to match " mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
tensor<"1mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOpx" result 40x
f32  >} -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : '
flow.dispatch.tensor.store   -> (%)' {
2Trying to match " = mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
flow.tensor.reshape %_params.bias : "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDimstensor<" result 30x
f32  >} ->  failure-> :  pattern failed to matchtensor<
1} -> xfailure3 : xpattern failed to matchf32
>//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {

    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<"4fxu3nxcf32.>r, ettensor<u1rxn3"x(f32)> : ) -> () -> ()tensor<1x3

xf32>} -> 
failure   : %pattern failed to match4
 = //===-------------------------------------------===//
flow.tensor.reshape
 //===-------------------------------------------===//
%Processing operation : '3flow.executable_end '(:0x56223be22ca0 ) {
  tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32">f l->o w.e!xhale.cbuffer_viewutabl
e  _ereturnn d%"5( ): :  () -> (!)hal.buffer_view


}} -> failure : 

pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: flow.executable
flow.executable private @main_dispatch_0 {
  flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
      %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
      %3 = tensor.empty() : tensor<1x3xf32>
      %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
      %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
      %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
      ^bb0(%in: f32, %in_0: f32, %out: f32):
        %7 = arith.addf %in, %in_0 : f32
        linalg.yield %7 : f32
      } -> tensor<1x3xf32>
      flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
      return
    }
  }
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: flow.executable
flow.executable private @main_dispatch_0 {
  flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
      %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
      %3 = tensor.empty() : tensor<1x3xf32>
      %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
      %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
      %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
      ^bb0(%in: f32, %in_0: f32, %out: f32):
        %7 = arith.addf %in, %in_0 : f32
        linalg.yield %7 : f32
      } -> tensor<1x3xf32>
      flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
      return
    }
  }
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInput (iree-stream-verify-input) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

moving immutable global _params.weight load to the entry block
moving immutable global _params.bias load to the entry block
==== REARRANGING BLOCK ACCESSES ====
// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====
// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: func.func
FuseGlobals: analyzing func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}:
FuseGlobals correlation maps:
// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: builtin.module
  !! traversal incomplete due to public function-like op @main
mlir-asm-printer: Verifying operation: builtin.module
FuncAnalysis: INCOMPLETE! @main(!hal.buffer_view) -> !hal.buffer_view 
  args: 1
    %arg0: non-uniform used !hal.buffer_view 
  results: 1
    %result#0: non-uniform used !hal.buffer_view 
  callOps: 0
// -----// IR Dump After IPO (iree-util-ipo) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineConstants (iree-util-outline-constants) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

moving immutable global _params.weight load to the entry block
moving immutable global _params.bias load to the entry block
==== REARRANGING BLOCK ACCESSES ====
// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%7) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DeduplicateDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorResultZeroElements<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>"
"mlir::iree_compiler::IREE::Flow::ReplaceOpIfTensorOperandEmpty<mlir::iree_compiler::IREE::Flow::TensorReshapeOp, 0>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp> : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FlattenTensorCastLikeChain<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bed00) {
  %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%8) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%9) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %4 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223bbe1b90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable'(0x56223be28d90) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====
// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: func.func
FuseGlobals: analyzing func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
  %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
  %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}:
FuseGlobals correlation maps:
// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: builtin.module
  !! traversal incomplete due to public function-like op @main
mlir-asm-printer: Verifying operation: builtin.module
FuncAnalysis: INCOMPLETE! @main(!hal.buffer_view) -> !hal.buffer_view 
  args: 1
    %arg0: non-uniform used !hal.buffer_view 
  results: 1
    %result#0: non-uniform used !hal.buffer_view 
  callOps: 0
// -----// IR Dump After IPO (iree-util-ipo) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight {noinline} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global private @_params.bias {noinline} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %3 = tensor.empty() : tensor<1x3xf32>
        %4 = linalg.fill ins(%cst : f32) outs(%3 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %5 = linalg.matmul ins(%0, %1 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%4 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %2 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%3 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %6, %arg3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : tensor<4x3xf32>
    %_params.bias = util.global.load @_params.bias : tensor<3xf32>
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
    %1 = flow.tensor.reshape %0 : tensor<4xf32> -> tensor<1x4xf32>
    %2 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
    %3 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1, %_params.weight, %2) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %4 = flow.tensor.reshape %3 : tensor<1x3xf32> -> tensor<3xf32>
    %5 = hal.tensor.export %4 "output 0" : tensor<3xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x56223b0ed3b0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b0943f0) {
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'util.global -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::GlobalOpExpansion"
    ** Insert  : 'util.global'(0x56223bfa5d40)
    ** Replace : 'util.global'(0x56223b0943f0)
    ** Insert  : 'util.global'(0x56223bfa6280)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::InitializerOpInterface::Trait<Empty>)
    ** Insert  : 'util.initializer'(0x56223bb895e0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::detail::TensorConstantOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::AffinityOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::StreamableOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IREE::Stream::TensorPhaseOp<Empty>)
    ** Insert  : 'stream.tensor.constant'(0x56223bfbbeb0)
    ** Insert  : 'stream.resource.size'(0x56223bfbe570)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::detail::GlobalStoreOpGenericAdaptorBase::Properties)
    ** Insert  : 'util.global.store'(0x56223bb902b0)
    ** Insert  : 'util.global.store'(0x56223c03c890)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::iree_compiler::IREE::Util::InitializerOp>::Impl<Empty>)
    ** Insert  : 'util.initializer.return'(0x56223c042cf0)
"mlir::iree_compiler::(anonymous namespace)::GlobalOpExpansion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'util.global'(0x56223bfa5d40) {
      "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global'(0x56223bfa6280) {
      "util.global"() <{sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.initializer'(0x56223bb895e0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.constant'(0x56223bfbbeb0) {
      %0 = "stream.tensor.constant"() <{result_encoding = tensor<4x3xf32>, value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : () -> !stream.resource<constant>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.resource.size'(0x56223bfbe570) {
      %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.store'(0x56223bb902b0) {
      "util.global.store"(%0) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.store'(0x56223c03c890) {
      "util.global.store"(%1) <{global = @_params.weight__size}> : (index) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.initializer.return'(0x56223c042cf0) {
      "util.initializer.return"() : () -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: builtin.module
redefinition of symbol named '_params.weight'
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
"builtin.module"() <{sym_name = "LinearModule"}> ({
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()
  "util.global"() <{sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()
  "util.initializer"() <{function_type = () -> ()}> ({
    %0 = "stream.tensor.constant"() <{result_encoding = tensor<4x3xf32>, value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : () -> !stream.resource<constant>
    %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index
    "util.global.store"(%0) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()
    "util.global.store"(%1) <{global = @_params.weight__size}> : (index) -> ()
    "util.initializer.return"() : () -> ()
  }) : () -> ()
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()
  "flow.executable"() <{sym_name = "main_dispatch_0", sym_visibility = "private"}> ({
    "flow.executable.export"() <{function_ref = @main_dispatch_0_matmul_1x3x4_f32, sym_name = "main_dispatch_0_matmul_1x3x4_f32"}> ({
      %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
      "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()
    }) : () -> ()
    "builtin.module"() ({
      "func.func"() <{function_type = (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>, !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> (), sym_name = "main_dispatch_0_matmul_1x3x4_f32"}> ({
      ^bb0(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>):
        %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
        %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>
        %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>
        %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>
        %4 = "tensor.empty"() : () -> tensor<1x3xf32>
        %5 = "linalg.fill"(%0, %4) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32):
          "linalg.yield"(%arg4) : (f32) -> ()
        }) : (f32, tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = "linalg.matmul"(%1, %2, %5) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32, %arg6: f32):
          %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%9) : (f32) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
        %7 = "linalg.generic"(%6, %3, %4) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32, %arg6: f32):
          %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%8) : (f32) -> ()
        }) : (tensor<1x3xf32>, tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
        "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()
        "func.return"() : () -> ()
      }) : () -> ()
    }) : () -> ()
    "flow.executable_end"() : () -> ()
  }) : () -> ()
  "func.func"() <{function_type = (!hal.buffer_view) -> !hal.buffer_view, sym_name = "main"}> ({
  ^bb0(%arg0: !hal.buffer_view):
    %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>
    %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>
    %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>
    %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>
    %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>
    %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>
    %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view
    "func.return"(%7) : (!hal.buffer_view) -> ()
  }) {iree.abi.stub} : () -> ()
}) {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global'(0x56223b095220) {
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'util.global -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::GlobalOpExpansion"
    ** Insert  : 'util.global'(0x56223c0215c0)
    ** Replace : 'util.global'(0x56223b095220)
    ** Insert  : 'util.global'(0x56223c03d790)
    ** Insert  : 'util.initializer'(0x56223c03d810)
    ** Insert  : 'stream.tensor.constant'(0x56223c04d1c0)
    ** Insert  : 'stream.resource.size'(0x56223be28700)
    ** Insert  : 'util.global.store'(0x56223c036110)
    ** Insert  : 'util.global.store'(0x56223c036190)
    ** Insert  : 'util.initializer.return'(0x56223c034c30)
"mlir::iree_compiler::(anonymous namespace)::GlobalOpExpansion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'util.global'(0x56223c0215c0) {
      "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global'(0x56223c03d790) {
      "util.global"() <{sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.initializer'(0x56223c03d810) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.constant'(0x56223c04d1c0) {
      %0 = "stream.tensor.constant"() <{result_encoding = tensor<3xf32>, value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : () -> !stream.resource<constant>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.resource.size'(0x56223be28700) {
      %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.store'(0x56223c036110) {
      "util.global.store"(%0) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.store'(0x56223c036190) {
      "util.global.store"(%1) <{global = @_params.bias__size}> : (index) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.initializer.return'(0x56223c034c30) {
      "util.initializer.return"() : () -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: builtin.module
redefinition of symbol named '_params.weight'
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
"builtin.module"() <{sym_name = "LinearModule"}> ({
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()
  "util.global"() <{sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()
  "util.initializer"() <{function_type = () -> ()}> ({
    %0 = "stream.tensor.constant"() <{result_encoding = tensor<4x3xf32>, value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : () -> !stream.resource<constant>
    %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index
    "util.global.store"(%0) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()
    "util.global.store"(%1) <{global = @_params.weight__size}> : (index) -> ()
    "util.initializer.return"() : () -> ()
  }) : () -> ()
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()
  "util.global"() <{sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()
  "util.initializer"() <{function_type = () -> ()}> ({
    %0 = "stream.tensor.constant"() <{result_encoding = tensor<3xf32>, value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : () -> !stream.resource<constant>
    %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index
    "util.global.store"(%0) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()
    "util.global.store"(%1) <{global = @_params.bias__size}> : (index) -> ()
    "util.initializer.return"() : () -> ()
  }) : () -> ()
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()
  "flow.executable"() <{sym_name = "main_dispatch_0", sym_visibility = "private"}> ({
    "flow.executable.export"() <{function_ref = @main_dispatch_0_matmul_1x3x4_f32, sym_name = "main_dispatch_0_matmul_1x3x4_f32"}> ({
      %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
      "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()
    }) : () -> ()
    "builtin.module"() ({
      "func.func"() <{function_type = (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>, !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> (), sym_name = "main_dispatch_0_matmul_1x3x4_f32"}> ({
      ^bb0(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<4x3xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<1x3xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>):
        %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
        %1 = "flow.dispatch.tensor.load"(%arg0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>
        %2 = "flow.dispatch.tensor.load"(%arg1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>
        %3 = "flow.dispatch.tensor.load"(%arg2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>
        %4 = "tensor.empty"() : () -> tensor<1x3xf32>
        %5 = "linalg.fill"(%0, %4) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32):
          "linalg.yield"(%arg4) : (f32) -> ()
        }) : (f32, tensor<1x3xf32>) -> tensor<1x3xf32>
        %6 = "linalg.matmul"(%1, %2, %5) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32, %arg6: f32):
          %8 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %9 = "arith.addf"(%arg6, %8) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%9) : (f32) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
        %7 = "linalg.generic"(%6, %3, %4) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32, %arg6: f32):
          %8 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%8) : (f32) -> ()
        }) : (tensor<1x3xf32>, tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
        "flow.dispatch.tensor.store"(%7, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()
        "func.return"() : () -> ()
      }) : () -> ()
    }) : () -> ()
    "flow.executable_end"() : () -> ()
  }) : () -> ()
  "func.func"() <{function_type = (!hal.buffer_view) -> !hal.buffer_view, sym_name = "main"}> ({
  ^bb0(%arg0: !hal.buffer_view):
    %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>
    %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>
    %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>
    %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>
    %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>
    %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>
    %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view
    "func.return"(%7) : (!hal.buffer_view) -> ()
  }) {iree.abi.stub} : () -> ()
}) {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.executable'(0x56223be28d90) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'flow.executable -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::ConvertExecutableOp"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::iree_compiler::IREE::Stream::ExecutableOp>::Impl<Empty>)
    ** Insert  : 'stream.executable.end'(0x56223c037400)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::iree_compiler::IREE::Stream::ExecutableEndOp>::Impl<Empty>)
    ** Insert  : 'stream.executable'(0x56223c036210)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::detail::ExecutableExportOpGenericAdaptorBase::Properties)
    ** Insert  : 'stream.executable.export'(0x56223c036400)
    ** Insert  : 'builtin.module'(0x56223c04e800)
    ** Insert  : 'arith.constant'(0x56223c04e880)
    ** Insert  : 'stream.binding.subspan'(0x56223b1240f0)
    ** Insert  : 'stream.binding.subspan'(0x56223b123600)
    ** Insert  : 'stream.binding.subspan'(0x56223b123800)
    ** Insert  : 'stream.binding.subspan'(0x56223b1239e0)
    ** Erase   : 'flow.executable'(0x56223be28d90)
"mlir::iree_compiler::(anonymous namespace)::ConvertExecutableOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'stream.executable.end'(0x56223c037400) {
      "stream.executable.end"() : () -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.executable'(0x56223c036210) {
    } -> SUCCESS : operation marked legal by the target; NOTE: operation is recursively legal; skipping internals
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.executable.export'(0x56223c036400) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'builtin.module'(0x56223c04e800) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x56223c04e880) {
      %0 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.binding.subspan'(0x56223b1240f0) {
      %1 = "stream.binding.subspan"(%arg0, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.binding.subspan'(0x56223b123600) {
      %2 = "stream.binding.subspan"(%arg1, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.binding.subspan'(0x56223b123800) {
      %3 = "stream.binding.subspan"(%arg2, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.binding.subspan'(0x56223b1239e0) {
      %4 = "stream.binding.subspan"(%arg3, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: builtin.module
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::iree_compiler::IREE::Stream::ExecutableExportOp>::Impl<Empty>)
'builtin.module' op region #0 ('bodyRegion') failed to verify constraint: region with 1 blocks
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
"builtin.module"() <{sym_name = "LinearModule"}> ({
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()
  "util.global"() <{sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()
  "util.initializer"() <{function_type = () -> ()}> ({
    %0 = "stream.tensor.constant"() <{result_encoding = tensor<4x3xf32>, value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : () -> !stream.resource<constant>
    %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index
    "util.global.store"(%0) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()
    "util.global.store"(%1) <{global = @_params.weight__size}> : (index) -> ()
    "util.initializer.return"() : () -> ()
  }) : () -> ()
  "util.global"() <{initial_value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>, sym_name = "_params.weight", sym_visibility = "private", type = tensor<4x3xf32>}> {noinline} : () -> ()
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()
  "util.global"() <{sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()
  "util.initializer"() <{function_type = () -> ()}> ({
    %0 = "stream.tensor.constant"() <{result_encoding = tensor<3xf32>, value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : () -> !stream.resource<constant>
    %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index
    "util.global.store"(%0) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()
    "util.global.store"(%1) <{global = @_params.bias__size}> : (index) -> ()
    "util.initializer.return"() : () -> ()
  }) : () -> ()
  "util.global"() <{initial_value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>, sym_name = "_params.bias", sym_visibility = "private", type = tensor<3xf32>}> {noinline} : () -> ()
  "stream.executable"() <{sym_name = "main_dispatch_0", sym_visibility = "private"}> ({
    "stream.executable.export"() <{function_ref = @main_dispatch_0_matmul_1x3x4_f32, sym_name = "main_dispatch_0_matmul_1x3x4_f32"}> ({
      %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
      "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()
    }) : () -> ()
    "builtin.module"() ({
      "func.func"() <{function_type = (!stream.binding, !stream.binding, !stream.binding, !stream.binding) -> (), sym_name = "main_dispatch_0_matmul_1x3x4_f32"}> ({
      ^bb0(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding):
        %0 = "arith.constant"() <{value = 0 : index}> : () -> index
        %1 = "stream.binding.subspan"(%arg0, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %2 = "stream.binding.subspan"(%arg1, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %3 = "stream.binding.subspan"(%arg2, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %4 = "stream.binding.subspan"(%arg3, %0) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %5 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
        %6 = "flow.dispatch.tensor.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>
        %7 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>
        %8 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>
        %9 = "tensor.empty"() : () -> tensor<1x3xf32>
        %10 = "linalg.fill"(%5, %9) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32):
          "linalg.yield"(%arg4) : (f32) -> ()
        }) : (f32, tensor<1x3xf32>) -> tensor<1x3xf32>
        %11 = "linalg.matmul"(%6, %7, %10) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32, %arg6: f32):
          %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%14) : (f32) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
        %12 = "linalg.generic"(%11, %8, %9) <{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg4: f32, %arg5: f32, %arg6: f32):
          %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%13) : (f32) -> ()
        }) : (tensor<1x3xf32>, tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
        "flow.dispatch.tensor.store"(%12, %4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()
        "func.return"() : () -> ()
      }) : () -> ()
    }) : () -> ()
    "stream.executable.end"() : () -> ()
  }) : () -> ()
  "flow.executable"() <{sym_name = "main_dispatch_0", sym_visibility = "private"}> ({
    "flow.executable.export"() <{function_ref = @main_dispatch_0_matmul_1x3x4_f32, sym_name = "main_dispatch_0_matmul_1x3x4_f32"}> ({
      %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
      "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()
    }) : () -> ()
    "builtin.module"() ({
    }) : () -> ()
    "flow.executable_end"() : () -> ()
  }) : () -> ()
  "func.func"() <{function_type = (!hal.buffer_view) -> !hal.buffer_view, sym_name = "main"}> ({
  ^bb0(%arg0: !hal.buffer_view):
    %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>
    %1 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>
    %2 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>
    %3 = "flow.tensor.reshape"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>
    %4 = "flow.tensor.reshape"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>
    %5 = "flow.dispatch"(%3, %0, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
    %6 = "flow.tensor.reshape"(%5) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>
    %7 = "hal.tensor.export"(%6) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view
    "func.return"(%7) : (!hal.buffer_view) -> ()
  }) {iree.abi.stub} : () -> ()
}) {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.executable.export'(0x56223bb736a0) {
} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be224f0) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.return'(0x56223ba10a20) {
  "flow.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x56223bbe1b90) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x7f71f0021640) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x56223be26e00) {
  %5 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x7f71f00142e0) {
} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.matmul'(0x56223be1eda0) {
} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x7f71f001fe60) {
} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %4) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.executable_end'(0x56223be22ca0) {
  "flow.executable_end"() : () -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x56223b0d3460) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bdb30) {
  %0 = "util.global.load"() <{global = @_params.weight}> : () -> tensor<4x3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'util.global.load -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::GlobalLoadOpExpansion"
    ** Insert  : 'util.global.load'(0x56223c022060)
    ** Insert  : 'util.global.load'(0x56223c021d90)
    ** Insert  : 'stream.async.transfer'(0x56223b1a5570)
    ** Replace : 'util.global.load'(0x56223b0bdb30)
"mlir::iree_compiler::(anonymous namespace)::GlobalLoadOpExpansion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.load'(0x56223c022060) {
      %0 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.load'(0x56223c021d90) {
      %1 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.async.transfer'(0x56223b1a5570) {
      %2 = "stream.async.transfer"(%0, %1, %1) : (!stream.resource<constant>, index, index) -> !stream.resource<*>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : tensor<3xf32>
  %1 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %2 = flow.tensor.reshape %1 : tensor<4xf32> -> tensor<1x4xf32>
  %3 = flow.tensor.reshape %_params.bias : tensor<3xf32> -> tensor<1x3xf32>
  %4 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%2, %_params.weight_0, %3) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %5 = flow.tensor.reshape %4 : tensor<1x3xf32> -> tensor<3xf32>
  %6 = hal.tensor.export %5 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'util.global.load'(0x56223b0bed00) {
  %4 = "util.global.load"() <{global = @_params.bias}> : () -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'util.global.load -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::GlobalLoadOpExpansion"
    ** Insert  : 'util.global.load'(0x56223c021cd0)
    ** Insert  : 'util.global.load'(0x56223c0218f0)
    ** Insert  : 'stream.async.transfer'(0x56223b0d21e0)
    ** Replace : 'util.global.load'(0x56223b0bed00)
"mlir::iree_compiler::(anonymous namespace)::GlobalLoadOpExpansion" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.load'(0x56223c021cd0) {
      %4 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'util.global.load'(0x56223c0218f0) {
      %5 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.async.transfer'(0x56223b0d21e0) {
      %6 = "stream.async.transfer"(%4, %5, %5) : (!stream.resource<constant>, index, index) -> !stream.resource<*>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  %_params.bias_1 = util.global.load @_params.bias : tensor<3xf32>
  %2 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %3 = flow.tensor.reshape %2 : tensor<4xf32> -> tensor<1x4xf32>
  %4 = flow.tensor.reshape %_params.bias_1 : tensor<3xf32> -> tensor<1x3xf32>
  %5 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3, %_params.weight_0, %4) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %6 = flow.tensor.reshape %5 : tensor<1x3xf32> -> tensor<3xf32>
  %7 = hal.tensor.export %6 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'hal.tensor.import'(0x56223b9b1700) {
  %8 = "hal.tensor.import"(%arg0) {name = "input 0", operandSegmentSizes = array<i32: 1, 0, 0>, target_encoding = tensor<4xf32>} : (!hal.buffer_view) -> tensor<4xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'hal.tensor.import -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::ConvertTensorImportOp"
    ** Insert  : 'arith.constant'(0x56223c04eaa0)
    ** Insert  : 'arith.constant'(0x56223c03c980)
    ** Insert  : 'arith.constant'(0x56223c03c9f0)
    ** Insert  : 'hal.buffer_view.assert'(0x56223be1f320)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::detail::TensorSizeOfOpGenericAdaptorBase::Properties)
    ** Insert  : 'stream.tensor.sizeof'(0x56223c0362d0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::detail::TensorImportOpGenericAdaptorBase::Properties)
    ** Insert  : 'stream.tensor.import'(0x7f71f000adc0)
    ** Insert  : 'stream.async.transfer'(0x56223b0a2340)
    ** Replace : 'hal.tensor.import'(0x56223b9b1700)
"mlir::iree_compiler::(anonymous namespace)::ConvertTensorImportOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x56223c04eaa0) {
      %8 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x56223c03c980) {
      %9 = "arith.constant"() <{value = 1 : i32}> : () -> i32

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x56223c03c9f0) {
      %10 = "arith.constant"() <{value = 4 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
      "hal.buffer_view.assert"(%arg0, %8, %9, %10) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.sizeof'(0x56223c0362d0) {
      %11 = "stream.tensor.sizeof"() <{encoding = tensor<4xf32>}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.import'(0x7f71f000adc0) {
      %12 = "stream.tensor.import"(%arg0, %11) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.async.transfer'(0x56223b0a2340) {
      %13 = "stream.async.transfer"(%12, %11, %11) : (!stream.resource<external>, index, index) -> !stream.resource<*>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  %_params.bias_1 = util.global.load @_params.bias : tensor<3xf32>
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %6 = flow.tensor.reshape %5 : tensor<4xf32> -> tensor<1x4xf32>
  %7 = flow.tensor.reshape %_params.bias_1 : tensor<3xf32> -> tensor<1x3xf32>
  %8 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%6, %_params.weight_0, %7) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %9 = flow.tensor.reshape %8 : tensor<1x3xf32> -> tensor<3xf32>
  %10 = hal.tensor.export %9 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %10 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.tensor.reshape'(0x56223b125450) {
  %15 = "flow.tensor.reshape"(%14) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<4xf32>) -> tensor<1x4xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::ConvertTensorCastLikeOp<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::SizeAwareOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IREE::Stream::AsyncPhaseOp<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::AsyncAccessOpInterface::Trait<Empty>)
    ** Insert  : 'stream.tensor.sizeof'(0x56223c03ed10)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::detail::TensorCloneOpGenericAdaptorBase::Properties)
    ** Insert  : 'stream.tensor.clone'(0x56223c020f40)
    ** Replace : 'flow.tensor.reshape'(0x56223b125450)
"mlir::iree_compiler::(anonymous namespace)::ConvertTensorCastLikeOp<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.sizeof'(0x56223c03ed10) {
      %15 = "stream.tensor.sizeof"() <{encoding = tensor<1x4xf32>}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.clone'(0x56223c020f40) {
      %16 = "stream.tensor.clone"(%13, %11, %15) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 1>, result_encoding = tensor<1x4xf32>, source_encoding = tensor<4xf32>}> : (!stream.resource<*>, index, index) -> !stream.resource<*>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  %_params.bias_1 = util.global.load @_params.bias : tensor<3xf32>
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %6 = stream.tensor.sizeof tensor<1x4xf32> : index
  %7 = stream.tensor.clone %4 : tensor<4xf32> in !stream.resource<*>{%2} -> tensor<1x4xf32> in !stream.resource<*>{%6}
  %8 = flow.tensor.reshape %5 : tensor<4xf32> -> tensor<1x4xf32>
  %9 = flow.tensor.reshape %_params.bias_1 : tensor<3xf32> -> tensor<1x3xf32>
  %10 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%8, %_params.weight_0, %9) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %11 = flow.tensor.reshape %10 : tensor<1x3xf32> -> tensor<3xf32>
  %12 = hal.tensor.export %11 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %12 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.tensor.reshape'(0x56223b125310) {
  %18 = "flow.tensor.reshape"(%7) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<3xf32>) -> tensor<1x3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::ConvertTensorCastLikeOp<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
    ** Insert  : 'stream.tensor.sizeof'(0x56223c03d5d0)
    ** Insert  : 'stream.tensor.clone'(0x56223c042c00)
    ** Replace : 'flow.tensor.reshape'(0x56223b125310)
"mlir::iree_compiler::(anonymous namespace)::ConvertTensorCastLikeOp<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.sizeof'(0x56223c03d5d0) {
      %18 = "stream.tensor.sizeof"() <{encoding = tensor<1x3xf32>}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.clone'(0x56223c042c00) {
      %19 = "stream.tensor.clone"(%6, %5, %18) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 1>, result_encoding = tensor<1x3xf32>, source_encoding = tensor<3xf32>}> : (!stream.resource<*>, index, index) -> !stream.resource<*>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  %_params.bias_1 = util.global.load @_params.bias : tensor<3xf32>
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %6 = stream.tensor.sizeof tensor<1x4xf32> : index
  %7 = stream.tensor.clone %4 : tensor<4xf32> in !stream.resource<*>{%2} -> tensor<1x4xf32> in !stream.resource<*>{%6}
  %8 = flow.tensor.reshape %5 : tensor<4xf32> -> tensor<1x4xf32>
  %9 = stream.tensor.sizeof tensor<1x3xf32> : index
  %10 = stream.tensor.clone %1 : tensor<3xf32> in !stream.resource<*>{%_params.bias__size} -> tensor<1x3xf32> in !stream.resource<*>{%9}
  %11 = flow.tensor.reshape %_params.bias_1 : tensor<3xf32> -> tensor<1x3xf32>
  %12 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%8, %_params.weight_0, %11) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %13 = flow.tensor.reshape %12 : tensor<1x3xf32> -> tensor<3xf32>
  %14 = hal.tensor.export %13 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %14 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.dispatch'(0x7f71f0020f30) {
  %21 = "flow.dispatch"(%17, %3, %20) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 0, 0>, tied_operands = [-1 : index]}> : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'flow.dispatch -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::ConvertDispatchOp"
    ** Insert  : 'arith.constant'(0x56223c042b90)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<3>::Impl<Empty>)
    ** Insert  : 'stream.tensor.sizeof'(0x56223c0354b0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::detail::AsyncDispatchOpGenericAdaptorBase::Properties)
    ** Insert  : 'stream.async.dispatch'(0x56223bfbe2c0)
    ** Replace : 'flow.dispatch'(0x7f71f0020f30)
"mlir::iree_compiler::(anonymous namespace)::ConvertDispatchOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x56223c042b90) {
      %21 = "arith.constant"() <{value = 0 : index}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.sizeof'(0x56223c0354b0) {
      %22 = "stream.tensor.sizeof"() <{encoding = tensor<1x3xf32>}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
      %23 = "stream.async.dispatch"(%16, %2, %19, %15, %1, %18, %21, %21, %21, %15, %1, %18, %15, %1, %18, %22) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  %_params.bias_1 = util.global.load @_params.bias : tensor<3xf32>
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %6 = stream.tensor.sizeof tensor<1x4xf32> : index
  %7 = stream.tensor.clone %4 : tensor<4xf32> in !stream.resource<*>{%2} -> tensor<1x4xf32> in !stream.resource<*>{%6}
  %8 = flow.tensor.reshape %5 : tensor<4xf32> -> tensor<1x4xf32>
  %9 = stream.tensor.sizeof tensor<1x3xf32> : index
  %10 = stream.tensor.clone %1 : tensor<3xf32> in !stream.resource<*>{%_params.bias__size} -> tensor<1x3xf32> in !stream.resource<*>{%9}
  %11 = flow.tensor.reshape %_params.bias_1 : tensor<3xf32> -> tensor<1x3xf32>
  %c0 = arith.constant 0 : index
  %12 = stream.tensor.sizeof tensor<1x3xf32> : index
  %13 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%7[%c0 to %6 for %6], %0[%c0 to %_params.weight__size for %_params.weight__size], %10[%c0 to %9 for %9]) : (!stream.resource<*>{%6}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%9}) -> !stream.resource<*>{%12}
  %14 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%8, %_params.weight_0, %11) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %15 = flow.tensor.reshape %14 : tensor<1x3xf32> -> tensor<3xf32>
  %16 = hal.tensor.export %15 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %16 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'flow.tensor.reshape'(0x56223b1253b0) {
  %25 = "flow.tensor.reshape"(%24) <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<1x3xf32>) -> tensor<3xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'flow.tensor.reshape -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::ConvertTensorCastLikeOp<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>"
    ** Insert  : 'stream.tensor.sizeof'(0x56223bb88300)
    ** Insert  : 'stream.tensor.clone'(0x56223bb7da60)
    ** Replace : 'flow.tensor.reshape'(0x56223b1253b0)
"mlir::iree_compiler::(anonymous namespace)::ConvertTensorCastLikeOp<mlir::iree_compiler::IREE::Flow::TensorReshapeOp>" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.sizeof'(0x56223bb88300) {
      %25 = "stream.tensor.sizeof"() <{encoding = tensor<3xf32>}> : () -> index

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.clone'(0x56223bb7da60) {
      %26 = "stream.tensor.clone"(%23, %22, %25) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 1>, result_encoding = tensor<3xf32>, source_encoding = tensor<1x3xf32>}> : (!stream.resource<*>, index, index) -> !stream.resource<*>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  %_params.bias_1 = util.global.load @_params.bias : tensor<3xf32>
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %6 = stream.tensor.sizeof tensor<1x4xf32> : index
  %7 = stream.tensor.clone %4 : tensor<4xf32> in !stream.resource<*>{%2} -> tensor<1x4xf32> in !stream.resource<*>{%6}
  %8 = flow.tensor.reshape %5 : tensor<4xf32> -> tensor<1x4xf32>
  %9 = stream.tensor.sizeof tensor<1x3xf32> : index
  %10 = stream.tensor.clone %1 : tensor<3xf32> in !stream.resource<*>{%_params.bias__size} -> tensor<1x3xf32> in !stream.resource<*>{%9}
  %11 = flow.tensor.reshape %_params.bias_1 : tensor<3xf32> -> tensor<1x3xf32>
  %c0 = arith.constant 0 : index
  %12 = stream.tensor.sizeof tensor<1x3xf32> : index
  %13 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%7[%c0 to %6 for %6], %0[%c0 to %_params.weight__size for %_params.weight__size], %10[%c0 to %9 for %9]) : (!stream.resource<*>{%6}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%9}) -> !stream.resource<*>{%12}
  %14 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%8, %_params.weight_0, %11) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %15 = stream.tensor.sizeof tensor<3xf32> : index
  %16 = stream.tensor.clone %13 : tensor<1x3xf32> in !stream.resource<*>{%12} -> tensor<3xf32> in !stream.resource<*>{%15}
  %17 = flow.tensor.reshape %14 : tensor<1x3xf32> -> tensor<3xf32>
  %18 = hal.tensor.export %17 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %18 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'hal.tensor.export'(0x56223b9ed920) {
  %28 = "hal.tensor.export"(%27) {name = "output 0", operandSegmentSizes = array<i32: 1, 0, 0>, source_encoding = tensor<3xf32>} : (tensor<3xf32>) -> !hal.buffer_view

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'hal.tensor.export -> ()' {
Trying to match "mlir::iree_compiler::(anonymous namespace)::ConvertTensorExportOp"
    ** Insert  : 'stream.async.transfer'(0x56223b096690)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Stream::detail::TensorExportOpGenericAdaptorBase::Properties)
    ** Insert  : 'stream.tensor.export'(0x56223bb83690)
    ** Replace : 'hal.tensor.export'(0x56223b9ed920)
"mlir::iree_compiler::(anonymous namespace)::ConvertTensorExportOp" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'stream.async.transfer'(0x56223b096690) {
      %28 = "stream.async.transfer"(%26, %25, %25) : (!stream.resource<*>, index, index) -> !stream.resource<external>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'stream.tensor.export'(0x56223bb83690) {
      %29 = "stream.tensor.export"(%28, %25) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.weight_0 = util.global.load @_params.weight : tensor<4x3xf32>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  %_params.bias_1 = util.global.load @_params.bias : tensor<3xf32>
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<4xf32>
  %6 = stream.tensor.sizeof tensor<1x4xf32> : index
  %7 = stream.tensor.clone %4 : tensor<4xf32> in !stream.resource<*>{%2} -> tensor<1x4xf32> in !stream.resource<*>{%6}
  %8 = flow.tensor.reshape %5 : tensor<4xf32> -> tensor<1x4xf32>
  %9 = stream.tensor.sizeof tensor<1x3xf32> : index
  %10 = stream.tensor.clone %1 : tensor<3xf32> in !stream.resource<*>{%_params.bias__size} -> tensor<1x3xf32> in !stream.resource<*>{%9}
  %11 = flow.tensor.reshape %_params.bias_1 : tensor<3xf32> -> tensor<1x3xf32>
  %c0 = arith.constant 0 : index
  %12 = stream.tensor.sizeof tensor<1x3xf32> : index
  %13 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%7[%c0 to %6 for %6], %0[%c0 to %_params.weight__size for %_params.weight__size], %10[%c0 to %9 for %9]) : (!stream.resource<*>{%6}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%9}) -> !stream.resource<*>{%12}
  %14 = flow.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%8, %_params.weight_0, %11) : (tensor<1x4xf32>, tensor<4x3xf32>, tensor<1x3xf32>) -> tensor<1x3xf32>
  %15 = stream.tensor.sizeof tensor<3xf32> : index
  %16 = stream.tensor.clone %13 : tensor<1x3xf32> in !stream.resource<*>{%12} -> tensor<3xf32> in !stream.resource<*>{%15}
  %17 = flow.tensor.reshape %14 : tensor<1x3xf32> -> tensor<3xf32>
  %18 = stream.async.transfer %16 : !stream.resource<*>{%15} -> !stream.resource<external>{%15}
  %19 = stream.tensor.export %18 : tensor<3xf32> in !stream.resource<external>{%15} -> !hal.buffer_view
  %20 = hal.tensor.export %17 "output 0" : tensor<3xf32> -> !hal.buffer_view
  return %20 : !hal.buffer_view
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%30) : (!hal.buffer_view) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::iree_compiler::IREE::Util::GlobalStoreOpInterface::Trait<Empty>)
// -----// IR Dump After ConvertToStream (iree-stream-conversion) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.global.store %0, @_params.weight__size : index
    util.initializer.return
  }
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.bias : !stream.resource<constant>
    util.global.store %0, @_params.bias__size : index
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %cst = arith.constant 0.000000e+00 : f32
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.sizeof tensor<4xf32> : index
    %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
    %5 = stream.tensor.sizeof tensor<1x4xf32> : index
    %6 = stream.tensor.clone %4 : tensor<4xf32> in !stream.resource<*>{%2} -> tensor<1x4xf32> in !stream.resource<*>{%5}
    %7 = stream.tensor.sizeof tensor<1x3xf32> : index
    %8 = stream.tensor.clone %1 : tensor<3xf32> in !stream.resource<*>{%_params.bias__size} -> tensor<1x3xf32> in !stream.resource<*>{%7}
    %c0 = arith.constant 0 : index
    %9 = stream.tensor.sizeof tensor<1x3xf32> : index
    %10 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%6[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %8[%c0 to %7 for %7]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%7}) -> !stream.resource<*>{%9}
    %11 = stream.tensor.sizeof tensor<3xf32> : index
    %12 = stream.tensor.clone %10 : tensor<1x3xf32> in !stream.resource<*>{%9} -> tensor<3xf32> in !stream.resource<*>{%11}
    %13 = stream.async.transfer %12 : !stream.resource<*>{%11} -> !stream.resource<external>{%11}
    %14 = stream.tensor.export %13 : tensor<3xf32> in !stream.resource<external>{%11} -> !hal.buffer_view
    return %14 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensors (iree-stream-verify-lowering-to-tensors) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.global.store %0, @_params.weight__size : index
    util.initializer.return
  }
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.bias : !stream.resource<constant>
    util.global.store %0, @_params.bias__size : index
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %cst = arith.constant 0.000000e+00 : f32
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.sizeof tensor<4xf32> : index
    %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
    %5 = stream.tensor.sizeof tensor<1x4xf32> : index
    %6 = stream.tensor.clone %4 : tensor<4xf32> in !stream.resource<*>{%2} -> tensor<1x4xf32> in !stream.resource<*>{%5}
    %7 = stream.tensor.sizeof tensor<1x3xf32> : index
    %8 = stream.tensor.clone %1 : tensor<3xf32> in !stream.resource<*>{%_params.bias__size} -> tensor<1x3xf32> in !stream.resource<*>{%7}
    %c0 = arith.constant 0 : index
    %9 = stream.tensor.sizeof tensor<1x3xf32> : index
    %10 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%6[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %8[%c0 to %7 for %7]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%7}) -> !stream.resource<*>{%9}
    %11 = stream.tensor.sizeof tensor<3xf32> : index
    %12 = stream.tensor.clone %10 : tensor<1x3xf32> in !stream.resource<*>{%9} -> tensor<3xf32> in !stream.resource<*>{%11}
    %13 = stream.async.transfer %12 : !stream.resource<*>{%11} -> !stream.resource<external>{%11}
    %14 = stream.tensor.export %13 : tensor<3xf32> in !stream.resource<external>{%11} -> !hal.buffer_view
    return %14 : !hal.buffer_view
  }
}




//===-------------------------------------------===//
//===-------------------------------------------===//
Processing operation : 'Processing operation : 'stream.tensor.constantstream.tensor.constant'('(0x56223c04d1c00x56223bfbbeb0) {
) {
    
//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %0 = "st%r0e = a"ms.ttreenasmo.rt.ecnosnosrt.acnotn"s(t)ant"()%4 = "ut <i{lresult_encoding <. = {gresult_encodingl = tensor<o4btensor<xa33lxx.f32f32l>>o, , avaluevalued =  = "(dense<)dense<[[[ <{global = @_params.weight}> : () -> 1.54099607-0.856674611, , !stream.resource<constant>-0.293428898, 1.10060418

, } -> -2.17878938failure] : -1.07118738, pattern failed to match][
>//===-------------------------------------------===//
 : 
tensor<//===-------------------------------------------===//
0.5684312583Processing operation : ', xutil.global.loadf32'(>0x56223c021d90}) {
>-1.08452237   : , () -> !-1.39859545stream]., resource<constant>[

0.403346837, 
  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty : 'stream.tensor.constant -> (0.838026344)' {
, Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty-0.719257593" result ]0, 
[  } -> failure : pattern failed to match

  * Pattern -0.403343529mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat,  : '%stream.tensor.constant5 -> ( = )' {
"-0.596635341Trying to match "u, mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplatt"
il.gl  o0.182036489  b]** Failure : a]only constant splat attrs can be converted to splat opsl>
 : ."ltensor<mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplato4" result ax0d3
"x  (f32} -> )>failure} : > <pattern failed to match : {
(global} -> ) ->  = failure@ : _params.weight__sizepattern failed to match!}
stream>//===-------------------------------------------===//
. : 
resource<constant>(//===-------------------------------------------===//
) -> Processing operation : 'indexstream.resource.size

'(0x56223be28700

) {
  
} ->   failure* Pattern  : mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmptypattern failed to match : '
stream.tensor.constant//===-------------------------------------------===//
 -> (
)' {
//===-------------------------------------------===//
Processing operation : 'Trying to match "stream.async.transfermlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty'("
0x56223b1a5570) {
  "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat"
    ** Failure : only constant splat attrs can be converted to splat ops
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat" result 0
  } -> failure : pattern failed to match
} -> failure% : 1pattern failed to match = 
"//===-------------------------------------------===//
s
t//===-------------------------------------------===//
rProcessing operation : 'estream.resource.sizea'(m0x56223bfbe570.) {
%r  6e = s"osutrrceea.ms.iazsey"n(c%.0t)ra : n(sfer"(!%stream4., resource<constant>%) -> 5index, %5) : 

(!stream.resource<constant>, index, %index
1) ->    = * Pattern !"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOpstreams : '.tstream.resource.sizeresource<*>r -> (e)' {
a

Trying to match "mmlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp."
r"emlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp
s" result   o0* Pattern u
mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>r   : 'c} -> stream.async.transferefailure -> (. : )' {
spattern failed to matchTrying to match "i
mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>z} -> "
efailure"" : mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>(pattern failed to match" result %
00//===-------------------------------------------===//

)
   : //===-------------------------------------------===//
} -> (Processing operation : 'failureutil.global.store : '(!pattern failed to match0x56223c036110stream
) {
.
  resource<constant>  ) -> * Pattern indexmlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {


Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match


    * Pattern * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOpmlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : ' : 'stream.resource.sizestream.async.transfer -> ( -> ()' {
)' {
Trying to match "Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOpmlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"
""mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOpmlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result " result 00

    } -> } -> failurefailure" :  : upattern failed to matchpattern failed to matcht

i} -> } -> lfailurefailure. :  : gpattern failed to matchpattern failed to matchl

o//===-------------------------------------------===//
//===-------------------------------------------===//
b

a//===-------------------------------------------===//
//===-------------------------------------------===//
lProcessing operation : 'Processing operation : '.util.global.storeutil.global.loads'('(t0x56223bb902b00x56223c021cd0o) {
) {
r    e"(%0) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure" : %upattern failed to match7t
 = i} -> "lfailureu. : tgpattern failed to matchil
lo//===-------------------------------------------===//
.b
ga//===-------------------------------------------===//
llProcessing operation : 'o.util.global.storebs'(at0x56223c036190lo) {
.r  leo"a(d%"0()) < <{{globalglobal =  = @@_params.weight_params.bias}}>> :  : (() -> !stream!.streamresource<constant>.) -> resource<constant>()

"

u} -> tfailurei : lpattern failed to match
.
  g//===-------------------------------------------===//
* Pattern l
mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpo//===-------------------------------------------===//
 : 'bProcessing operation : 'util.global.storeautil.global.load -> (l'()' {
.0x56223c0218f0Trying to match "s) {
mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpt  "
or"emlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"" result (0%
1  )} -> failure : pattern failed to match
} ->  <failure{ : globalpattern failed to match = 
@//===-------------------------------------------===//
_params.bias__size
}//===-------------------------------------------===//
>Processing operation : ' : util.global.store('(index0x56223c03c890) -> ) {
(  )

%8 = "
u  t* Pattern imlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpl : '.util.global.storeg -> (l)' {
obTrying to match "amlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpl"
.l"omlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpa" result d0"
(  )} -> failure : pattern failed to match
} ->  <failure{ : globalpattern failed to match = 
@//===-------------------------------------------===//
_params.bias__size
}//===-------------------------------------------===//
>Processing operation : ' : util.initializer.return"('(u) -> 0x56223c034c30tindex) {
i  l

.glo} -> bfailurea : lpattern failed to match.
s//===-------------------------------------------===//
t
o//===-------------------------------------------===//
rProcessing operation : 'estream.async.transfer"'((0x56223b0d21e0%) {
1  ) <{global = @_params.weight__size}> : (index) -> ()"

util.ini
t  i* Pattern amlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpl : 'iutil.global.storez -> (e)' {
rTrying to match ".mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpr"
et"u%mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpr9" result n = 0""
(s  )t} ->  : rfailure(e : ) -> apattern failed to match(m
).} -> afailure

s : ypattern failed to match} -> n
failurec//===-------------------------------------------===//
 : .
pattern failed to matcht//===-------------------------------------------===//

rProcessing operation : '//===-------------------------------------------===//
autil.initializer.returnn'(s0x56223c042cf0f) {
e  r"(%7, %8, %8) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElisionu : 'tstream.async.transfer// -----// IR Dump After i -> (Canonicalizerl)' {
 (.Trying to match "canonicalizeimlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision)n"
 //----- //
i"mlir-asm-printertmlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision: Verifying operation: i" result util.initializera0
l
i  z} -> efailurer : .pattern failed to matchr
e
t  u* Pattern rmlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElisionn : '"stream.async.transfer( -> ())' {
 : Trying to match "(mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision) -> "
(")mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  

} -> failure : pattern failed to match} -> 
failure} ->  : failureutil.initializerpattern failed to match : 
pattern failed to match//===-------------------------------------------===//

//===-------------------------------------------===//
 
{//===-------------------------------------------===//

Processing operation : 'arith.constant  '(%0x56223c04eaa0cst) {
 =   stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738%]3> = " : artensor<i3txhf32.>constant"()
  % <0{ = value = stream.resource.size 553648160% : csti 32:} > : () -> !istream32.resource<constant>
  

util.global.store } -> %failurecst : ,pattern failed to match 
//===-------------------------------------------===//
@
_params.bias//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
   : !stream.resource<constant>
  util.global.store %0, @_params.bias__size : index
  util.initializer.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: util.initializer
%2 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  util.initializer {
  %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[%1 = 1.54099607", arith.cons-0.293428898t, ant"()-2.17878938],  <[{value = 4 : index0.568431258}, > : () -> index

-1.08452237, } -> failure : pattern failed to match
//===-------------------------------------------===//
-1.39859545
]//===-------------------------------------------===//
, Processing operation : '[hal.buffer_view.assert'(0x56223be1f320) {
  0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %0 = stream.resource.size %cst : !stream.resource<constant>
"  hutil.global.storea l%.cstb,u f@f_params.weighter_vi e:w .as!sstreame.rresource<constant>t
"  (util.global.store% arg0%, 0%,3 , @%_params.weight__size2, %1 ): { messageindex = 
"  iutil.initializer.returnn
p}ut 0

"} : (!hal.buffer_view, i32, i32, index) -> ()// -----// IR Dump After 

CSE (cse) //----- //
mlir-asm-printer: Verifying operation: util.initializer
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c0362d0) {
  util.initializer {
  %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<%[10[ = "stream.te1.54099607n, sor.si-0.293428898z, eof"()-2.17878938], [ <{0.568431258encoding,  = tensor<4-1.08452237x, f32>}> : -1.39859545(]) -> , index[

0.403346837, } -> failure : pattern failed to match
0.838026344//===-------------------------------------------===//
, 
//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0-0.719257593) {
]  , [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %0 = stream.resource.size %cst : !stream.resource<constant>
  util.global.store %cst, @_params.weight : %11! = stream".sresource<constant>t
r  eutil.global.storea m%.0t,e n@s_params.weight__sizeor.i m:p oindexr
t  "util.initializer.return(
%}arg0, %

10) <{==== REARRANGING BLOCK ACCESSES ====
result_encoding// -----// IR Dump After  = CSE (csetensor<)4 //----- //
xf32mlir-asm-printermoving mutable global >: Verifying operation: _params.weight}util.initializer store downward
>
 : (moving mutable global _params.weight__size store downward
!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %12 = util.initializer"stream.async.tran s{f
er"(  %11%, cst% = 10, stream.tensor.constant% 10:)  : (tensor<!3streamx.f32resource<external>>,  indexin,  index) -> !stream.!resource<*>stream.resource<constant>

 = dense<
  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
[Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
-0.856674611Trying to match ", mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

1.10060418  , * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0-1.07118738
]  >} -> failure :  : pattern failed to match
tensor<} -> 3failurex : f32pattern failed to match>
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c03ed10) {
  
  %0 = stream.resource.size %cst : !stream.resource<constant>
  util.global.store %cst, @_params.bias :% 13 = "stre!astreamm..resource<constant>t
e  nsutil.global.storeo r.%s0i,z eo@f_params.bias__size"()  <:{ encodingindex = 
  tensor<1util.initializer.returnx
4}xf32>}>

 : () -> index// -----// IR Dump After SimplifyGlobalAccesses

 (iree-util-simplify-global-accesses} -> )failure //----- //
 : mlir-asm-printerpattern failed to match: Verifying operation: 
util.initializer//===-------------------------------------------===//


//===-------------------------------------------===//
Processing operation : 'stream.tensor.clone'(0x56223c020f40) {
  %util.initializer14 = "stream .{t
en  s%ocstr = .stream.tensor.constantc l:o ne"(tensor<%412x, 3%x10f32, >% 13in) !stream.resource<constant> =  <dense<{operandSegmentSizes = array<[i[32: 1, 0, 1, 0, 1>1.54099607, , result_encoding = tensor<1x4x-0.293428898f32, >, source_encoding = tensor<4-2.17878938x]f32, >[}> : (!0.568431258stream, .resource<*>, index, index) -> -1.08452237, !stream.resource<*>-1.39859545

], [  0.403346837** Replace : ', stream.tensor.clone'(0x56223c020f40)
  0.838026344** Modified: ', stream.async.dispatch'(0x56223bfbe2c0)
  ** Erase   : 'stream.tensor.clone'(-0.7192575930x56223c020f40])
, [} -> success : operation was folded
//===-------------------------------------------===//

-0.403343529//===-------------------------------------------===//
, Processing operation : 'stream.tensor.sizeof'(0x56223c03d5d0) {
  -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %0 = stream.resource.size %cst : !stream.resource<constant>
%  14util.global.store =  "%scstt,r ea@m_params.weight.tensor. s:i zeof"(!)stream.resource<constant>
 <  {util.global.storeencoding  = %0tensor<,1 x@3_params.weight__sizexf32>} >: :  (index) -> 
index  util.initializer.return


}} -> failure

 : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.clone'(0x56223c042c00) {
  ==== REARRANGING BLOCK ACCESSES ====
moving mutable global _params.bias store downward
%moving mutable global 15_params.bias__size =  store downward
"stream.tensor.clone"(%9, %8, %14) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 1>, result_encoding = tensor<1x3xf32>, source_encoding = // -----// IR Dump After tensor<SimplifyGlobalAccesses3 (xiree-util-simplify-global-accessesf32)> //----- //
}mlir-asm-printer>: Verifying operation:  : util.initializer(
!stream.resource<*>, index, index) -> !stream.resource<*>

  ** Replace : 'stream.tensor.clone'(0x56223c042c00)
  ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
  ** Erase   : 'stream.tensor.clone'(0x56223c042c00)
} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  util.initializer {
  %cst = %stream.tensor.constant0  = :" arith.ctensor<o3nxstf32a>n tin" ()! <stream{.valueresource<constant> =  0= :  index}> : dense<() -> index[

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c0354b0) {
  -0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
%  15 = %"0s = trstream.resource.sizee a%mcst.t e:n sor.size!ostreamf."resource<constant>(
)  util.global.store < {%encodingcst = , tensor<1@x_params.bias3xf32>}> :  (:) ->  index

!stream.} -> resource<constant>failure
 :   pattern failed to match
util.global.store//===-------------------------------------------===//
 
//===-------------------------------------------===//
%Processing operation : '0stream.async.dispatch,'( 0x56223bfbe2c0) {
@  _params.bias__size : index
  util.initializer.return
}

%16 = "stream.async.dispatch"(%12, %6, %9, %13, %5, %14, %0, %0, %0, %13, %5, %14, %13, %5, %14, %15) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223bb88300) {
  %17 = "stream.tensor.sizeof"() <{encoding = tensor<3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.clone'(0x56223bb7da60) {
  %18 = "stream.tensor.clone"(%16, %15, %17) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 1>, result_encoding = tensor<3xf32>, source_encoding = tensor<1x3xf32>}> : (!stream.resource<*>, index, index) -> !stream.resource<*>

  ** Replace : 'stream.tensor.clone'(0x56223bb7da60)
  ** Modified: 'stream.async.transfer'(0x56223b096690)
  ** Erase   : 'stream.tensor.clone'(0x56223bb7da60)
} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %18 = "stream.async.transfer"(%16, %17, %17) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %19 = "stream.tensor.export"(%18, %17) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%19) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %0 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %1 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %3 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %4 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %5 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %6 = "stream.async.transfer"(%4, %5, %5) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %7 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %8 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %9 = "stream.async.transfer"(%7, %8, %8) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %3, %2, %1) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c0362d0) {
  %10 = "stream.tensor.sizeof"() <{encoding = tensor<4xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %11 = "stream.tensor.import"(%arg0, %10) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %12 = "stream.async.transfer"(%11, %10, %10) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c03ed10) {
  %13 = "stream.tensor.sizeof"() <{encoding = tensor<1x4xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c03d5d0) {
  %14 = "stream.tensor.sizeof"() <{encoding = tensor<1x3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c0354b0) {
  %15 = "stream.tensor.sizeof"() <{encoding = tensor<1x3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %16 = "stream.async.dispatch"(%12, %6, %9, %13, %5, %14, %0, %0, %0, %13, %5, %14, %13, %5, %14, %15) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223bb88300) {
  %17 = "stream.tensor.sizeof"() <{encoding = tensor<3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %18 = "stream.async.transfer"(%16, %17, %17) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %19 = "stream.tensor.export"(%18, %17) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%19) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = stream.tensor.sizeof tensor<1x4xf32> : index
  %6 = stream.tensor.sizeof tensor<1x3xf32> : index
  %7 = stream.tensor.sizeof tensor<1x3xf32> : index
  %8 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
  %9 = stream.tensor.sizeof tensor<3xf32> : index
  %10 = stream.async.transfer %8 : !stream.resource<*>{%9} -> !stream.resource<external>{%9}
  %11 = stream.tensor.export %10 : tensor<3xf32> in !stream.resource<external>{%9} -> !hal.buffer_view
  return %11 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = stream.tensor.sizeof tensor<1x4xf32> : index
  %6 = stream.tensor.sizeof tensor<1x3xf32> : index
  %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
  %8 = stream.tensor.sizeof tensor<3xf32> : index
  %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
  %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
  return %10 : !hal.buffer_view
}

moving immutable global _params.weight load to the entry block
moving immutable global _params.weight__size load to the entry block
moving immutable global _params.bias load to the entry block
moving immutable global _params.bias__size load to the entry block
==== REARRANGING BLOCK ACCESSES ====
// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = stream.tensor.sizeof tensor<1x4xf32> : index
  %6 = stream.tensor.sizeof tensor<1x3xf32> : index
  %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
  %8 = stream.tensor.sizeof tensor<3xf32> : index
  %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
  %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
  return %10 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%18) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %18 = "stream.tensor.export"(%17, %16) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %17 = "stream.async.transfer"(%15, %16, %16) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223bb88300) {
  %16 = "stream.tensor.sizeof"() <{encoding = tensor<3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %15 = "stream.async.dispatch"(%12, %8, %9, %13, %5, %14, %3, %3, %3, %13, %5, %14, %13, %5, %14, %14) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c03d5d0) {
  %14 = "stream.tensor.sizeof"() <{encoding = tensor<1x3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c03ed10) {
  %13 = "stream.tensor.sizeof"() <{encoding = tensor<1x4xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %12 = "stream.async.transfer"(%11, %10, %10) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %11 = "stream.tensor.import"(%arg0, %10) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c0362d0) {
  %10 = "stream.tensor.sizeof"() <{encoding = tensor<4xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %0, %1, %2) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %9 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %8 = "stream.async.transfer"(%4, %5, %5) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %0 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %1 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %2 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %7 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %6 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %5 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %4 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036190) {
  "util.global.store"(%1) <{global = @_params.bias__size}> : (index) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%0) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.resource.size'(0x56223be28700) {
  %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp : 'stream.resource.size -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223c03d810) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.constant'(0x56223c04d1c0) {
  %0 = "stream.tensor.constant"() <{result_encoding = tensor<3xf32>, value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : () -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat"
    ** Failure : only constant splat attrs can be converted to splat ops
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c03d790) {
  "util.global"() <{sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c042cf0) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c03c890) {
  "util.global.store"(%1) <{global = @_params.weight__size}> : (index) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%0) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.resource.size'(0x56223bfbe570) {
  %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp : 'stream.resource.size -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb895e0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.constant'(0x56223bfbbeb0) {
  %0 = "stream.tensor.constant"() <{result_encoding = tensor<4x3xf32>, value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : () -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat"
    ** Failure : only constant splat attrs can be converted to splat ops
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa6280) {
  "util.global"() <{sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.global.store %0, @_params.weight__size : index
    util.initializer.return
  }
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.bias : !stream.resource<constant>
    util.global.store %0, @_params.bias__size : index
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.sizeof tensor<4xf32> : index
    %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
    %5 = stream.tensor.sizeof tensor<1x4xf32> : index
    %6 = stream.tensor.sizeof tensor<1x3xf32> : index
    %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
    %8 = stream.tensor.sizeof tensor<3xf32> : index
    %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
    %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%18) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %18 = "stream.tensor.export"(%17, %16) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %17 = "stream.async.transfer"(%15, %16, %16) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223bb88300) {
  %16 = "stream.tensor.sizeof"() <{encoding = tensor<3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %15 = "stream.async.dispatch"(%12, %8, %9, %13, %5, %14, %3, %3, %3, %13, %5, %14, %13, %5, %14, %14) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c03d5d0) {
  %14 = "stream.tensor.sizeof"() <{encoding = tensor<1x3xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c03ed10) {
  %13 = "stream.tensor.sizeof"() <{encoding = tensor<1x4xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %12 = "stream.async.transfer"(%11, %10, %10) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %11 = "stream.tensor.import"(%arg0, %10) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223c0362d0) {
  %10 = "stream.tensor.sizeof"() <{encoding = tensor<4xf32>}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %0, %1, %2) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %9 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %8 = "stream.async.transfer"(%4, %5, %5) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %7 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %6 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %5 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %4 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %3 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %2 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %1 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %0 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036190) {
  "util.global.store"(%1) <{global = @_params.bias__size}> : (index) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%0) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.resource.size'(0x56223be28700) {
  %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp : 'stream.resource.size -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223c03d810) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.constant'(0x56223c04d1c0) {
  %0 = "stream.tensor.constant"() <{result_encoding = tensor<3xf32>, value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : () -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat"
    ** Failure : only constant splat attrs can be converted to splat ops
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c03d790) {
  "util.global"() <{sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c042cf0) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c03c890) {
  "util.global.store"(%1) <{global = @_params.weight__size}> : (index) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%0) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.resource.size'(0x56223bfbe570) {
  %1 = "stream.resource.size"(%0) : (!stream.resource<constant>) -> index


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp : 'stream.resource.size -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SelectResourceSizeOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb895e0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.constant'(0x56223bfbbeb0) {
  %0 = "stream.tensor.constant"() <{result_encoding = tensor<4x3xf32>, value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : () -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToEmpty" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat : 'stream.tensor.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat"
    ** Failure : only constant splat attrs can be converted to splat ops
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TensorConstantToSplat" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa6280) {
  "util.global"() <{sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====
// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.global.store %0, @_params.weight__size : index
    util.initializer.return
  }
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.bias : !stream.resource<constant>
    util.global.store %0, @_params.bias__size : index
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.sizeof tensor<4xf32> : index
    %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
    %5 = stream.tensor.sizeof tensor<1x4xf32> : index
    %6 = stream.tensor.sizeof tensor<1x3xf32> : index
    %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
    %8 = stream.tensor.sizeof tensor<3xf32> : index
    %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
    %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: util.initializer
FuseGlobals: analyzing util.initializer {
  %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %0 = stream.resource.size %cst : !stream.resource<constant>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.global.store %0, @_params.weight__size : index
  util.initializer.return
}:
 - store #0: util.global.store %cst, @_params.weight : !stream.resource<constant>; candidate=1
 - store #1: util.global.store %0, @_params.weight__size : index; candidate=1
= storing value %cst:
 => @_params.weight
= storing value %0:
 => @_params.weight__size
mlir-asm-printer: Verifying operation: util.initializer
FuseGlobals: analyzing util.initializer {
  %cst = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  %0 = stream.resource.size %cst : !stream.resource<constant>
  util.global.store %cst, @_params.bias : !stream.resource<constant>
  util.global.store %0, @_params.bias__size : index
  util.initializer.return
}:
 - store #2: util.global.store %cst, @_params.bias : !stream.resource<constant>; candidate=1
 - store #3: util.global.store %0, @_params.bias__size : index; candidate=1
= storing value %cst:
 => @_params.bias
= storing value %0:
 => @_params.bias__size
mlir-asm-printer: Verifying operation: func.func
FuseGlobals: analyzing func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.sizeof tensor<4xf32> : index
  %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
  %5 = stream.tensor.sizeof tensor<1x4xf32> : index
  %6 = stream.tensor.sizeof tensor<1x3xf32> : index
  %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
  %8 = stream.tensor.sizeof tensor<3xf32> : index
  %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
  %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
  return %10 : !hal.buffer_view
}:
FuseGlobals correlation maps:
= #1 "_params.weight__size" = 0100:
  => _params.weight__size
= #3 "_params.bias__size" = 0001:
  => _params.bias__size
= #0 "_params.weight" = 1000:
  => _params.weight
= #2 "_params.bias" = 0010:
  => _params.bias
// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.global.store %0, @_params.weight__size : index
    util.initializer.return
  }
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.bias : !stream.resource<constant>
    util.global.store %0, @_params.bias__size : index
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.sizeof tensor<4xf32> : index
    %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
    %5 = stream.tensor.sizeof tensor<1x4xf32> : index
    %6 = stream.tensor.sizeof tensor<1x3xf32> : index
    %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
    %8 = stream.tensor.sizeof tensor<3xf32> : index
    %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
    %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: builtin.module
  !! traversal incomplete due to public function-like op @main
mlir-asm-printer: Verifying operation: builtin.module
FuncAnalysis: INCOMPLETE! @main(!hal.buffer_view) -> !hal.buffer_view 
  args: 1
    %arg0: non-uniform used !hal.buffer_view 
  results: 1
    %result#0: non-uniform used !hal.buffer_view 
  callOps: 0
// -----// IR Dump After IPO (iree-util-ipo) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.global.store %0, @_params.weight__size : index
    util.initializer.return
  }
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.bias : !stream.resource<constant>
    util.global.store %0, @_params.bias__size : index
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.sizeof tensor<4xf32> : index
    %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
    %5 = stream.tensor.sizeof tensor<1x4xf32> : index
    %6 = stream.tensor.sizeof tensor<1x3xf32> : index
    %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
    %8 = stream.tensor.sizeof tensor<3xf32> : index
    %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
    %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size : index
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size : index
  util.initializer {
    %cst = stream.tensor.constant : tensor<4x3xf32> in !stream.resource<constant> = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.global.store %0, @_params.weight__size : index
    %cst_0 = stream.tensor.constant : tensor<3xf32> in !stream.resource<constant> = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    %1 = stream.resource.size %cst_0 : !stream.resource<constant>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %1, @_params.bias__size : index
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.sizeof tensor<4xf32> : index
    %3 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%2}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%2} -> !stream.resource<*>{%2}
    %5 = stream.tensor.sizeof tensor<1x4xf32> : index
    %6 = stream.tensor.sizeof tensor<1x3xf32> : index
    %7 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%4[%c0 to %5 for %5], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
    %8 = stream.tensor.sizeof tensor<3xf32> : index
    %9 = stream.async.transfer %7 : !stream.resource<*>{%8} -> !stream.resource<external>{%8}
    %10 = stream.tensor.export %9 : tensor<3xf32> in !stream.resource<external>{%8} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  
//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {

  //===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
"Processing operation : 'sutil.global.storet"'(rf0x56223c036190eu) {
an  mc..erxeetcuurtna"b(l%e18.)e : n(d"() : (!) -> hal(.)buffer_view) -> ()



} -> failure : pattern failed to match
//===-------------------------------------------===//
} -> 
failure//===-------------------------------------------===//
 : Processing operation : 'pattern failed to matchfunc.return
'("//===-------------------------------------------===//
0x56223be1e4b0u
) {
t//===-------------------------------------------===//
  iProcessing operation : 'lstream.tensor.export.'(g0x56223bb83690l) {
o  bal.store"(%3) <{global = @_params.bias__size}> : (index) -> ()

} -> "failuref : upattern failed to matchn
c//===-------------------------------------------===//
.
r//===-------------------------------------------===//
%eProcessing operation : '18tutil.global.store = u'("r0x56223c036110sn) {
t"  r(e)a : m(.) -> t(e)nsor.e

xport} -> "failure( : %pattern failed to match17
, //===-------------------------------------------===//
%
16//===-------------------------------------------===//
)Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
   <{source_encoding = tensor<3xf32>}> : ("uti!lstream..gresource<external>l, oindexb) -> al!.hals.tbuffer_viewore"(

%2)"fl
o <  w{* Pattern .globalmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOpd =  : '@istream.tensor.export_params.biass -> (}p)' {
>aTrying to match " : tmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp(c"
h.ten!  sstream  o.** Failure : rresource<constant>.tensor export not handled) -> s
(t")omlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOpr" result e0

"
(  %} -> 12failure} -> ,  : failure%pattern failed to match : 5
pattern failed to match)} -> 
failure//===-------------------------------------------===//
 : 
pattern failed to match//===-------------------------------------------===//

Processing operation : '//===-------------------------------------------===//
stream.resource.size <
'({//===-------------------------------------------===//
0x56223be28700operandSegmentSizesProcessing operation : ') {
 = stream.async.transfer  '(array<0x56223b096690) {
  i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3x%f323> = %, "17s = t!"rflowse.tadispatch.tensor<writeonly:tensor<1x3xf32>>rm) -> e.(ar)me.sa

osuyrncc
e.  .t* Pattern srmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorStoreOpia : 'znflow.dispatch.tensor.storees -> ("f)' {
(eTrying to match "%rmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorStoreOp2""
)(" : %mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorStoreOp(15" result , 0%
!16  stream, } -> .%failureresource<constant>16 : ) -> )pattern failed to matchindex : 
(} -> failure

 : !pattern failed to matchstream
.//===-------------------------------------------===//
resource<*>
, //===-------------------------------------------===//
indexProcessing operation : ', } -> linalg.yieldindexfailure'() ->  : 0x56223be1dfd0pattern failed to match) {
!
  stream//===-------------------------------------------===//
.
resource<external>//===-------------------------------------------===//
Processing operation : 'stream.tensor.constant'(0x56223c04d1c0

) {
  } -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(0x56223bb88300) {
  "linalg.yield"(%%213 = )" : s(trf32e) -> a%(m16). = t"e

sntsr} -> oefailurera : .mpattern failed to matchc.
ot//===-------------------------------------------===//
ne
sn//===-------------------------------------------===//
tsProcessing operation : 'aolinalg.genericnr'(t.0x7f71f001fe60"s) {
(i)zeo} -> ffailure" : ( <pattern failed to match){
result_encoding//===-------------------------------------------===//
 =  <
{//===-------------------------------------------===//
tensor<encodingProcessing operation : '3 = arith.addfx'(tensor<f320x56223be21d303>) {
x,   f32value> = }> : dense<() -> index[


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp : 'stream.tensor.sizeof -> ()' {
-0.856674611Trying to match ", mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp"
1.10060418, -1.07118738  ]  >** Insert  : ' : arith.constant'(tensor<0x7f71f8007b90%3)
13x =   f32"  >a** Replace : '}rstream.tensor.sizeof>i'( : t0x56223bb88300(h)
) -> .a  d  !d** Modified: 'streamfstream.tensor.export."'(resource<constant>(0x56223bb83690%)
arg4  

,   %** Modified: '
arg5stream.async.transfer  )'(* Pattern 0x56223b096690mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorConstantOp)
 : ' <  stream.tensor.constant{   -> (fastmath** Modified: ')' {
 = stream.async.transferTrying to match "'(#mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorConstantOp0x56223b096690arith"
)
.fastmath<none>  }  >** Erase   : ' : stream.tensor.sizeof('(0x56223bb88300f32)
,   f32"  ) -> mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp** Insert  : 'f32" result arith.constant1'(


0x7f71e0008e80  )
} -> success} ->  : failurepattern applied successfullyImplicitTypeIDRegistry::lookupOrInsert( : 
mlir::iree_compiler::IREE::Stream::detail::AsyncConstantOpGenericAdaptorBase::Propertiespattern failed to match// *** IR Dump After Pattern Application ***
)

mlir-asm-printer//===-------------------------------------------===//
: Verifying operation: 
func.func//===-------------------------------------------===//

  Processing operation : '  linalg.yield** Insert  : ''(stream.async.constant0x56223be1ea40'() {
0x7f71e00049b0  )
    ** Replace : 'stream.tensor.constant'(0x56223c04d1c0)
    ** Modified: 'util.global.store'(0x56223c036110)
    ** Modified: 'stream.resource.size'(0x56223be28700)
    ** Erase   : 'stream.tensor.constant'(0x56223c04d1c0)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorConstantOp" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: util.initializer
"linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  util.initializer {
  %cst = %stream.tensor.constant14  = :" arittensor<h4.xa3dxdf32f>" (in% arg6, %13)!stream.resource<constant>  <={ fastmath = dense<#arith.fastmath<none>}[>[ : (f32, f32) -> f32func.func

1.54099607 , @main} -> (failure% : arg0pattern failed to match-0.293428898: 
, //===-------------------------------------------===//
!
hal//===-------------------------------------------===//
.Processing operation : 'buffer_viewlinalg.matmul-2.17878938)'(] -> 0x56223be1eda0, ) {
![hal.buffer_view} -> failure : pattern failed to match0.568431258
, //===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : ' attributesarith.mulf {'(-1.08452237iree.abi.stub0x56223be26e70, }) {
   {
-1.39859545  ]%, c553648160_i32[ = arith.constant0.403346837 , 553648160 : i320.838026344
,   %c1_i32 = arith.constant-0.719257593], [ 1 : i-0.40334352932, 
  %c4 = arith.constant%-0.59663534113,  =  "a4r : i0.182036489indext]
h]  .>%m : c0u = tensor<larith.constant4fx"3 (x0%f32 : arg4>index, 
%  arg5%)_params.weight = util.global.load
  <  @{%_params.weightfastmath0 =  =  #stream.resource.size:arith  .%fastmath<none>cst!} stream>:. :  resource<constant>(
  f32!%, stream_params.weight__sizef32. = ) -> resource<constant>util.global.loadf32
   @

util.global.store_params.weight__size %} -> cstfailure , : : pattern failed to match @
index_params.weight//===-------------------------------------------===//


  //===-------------------------------------------===//
%Processing operation : ' _params.biaslinalg.fill: = '( util.global.load0x7f71f00142e0 ) {
!@stream_params.bias.resource<constant>} -> 
failure    : :util.global.storepattern failed to match  
%//===-------------------------------------------===//
!0
stream,//===-------------------------------------------===//
. Processing operation : 'resource<constant>@linalg.yield
_params.weight__size'(  0x7f71f0005960%) {
_params.bias__size    = :util.global.load  index@
_params.bias__size  %c12 =  arith.constant: index
  %0 = stream.async.transfer  %_params.weight12  : :index 
  %!cst_0stream = .stream.async.constantresource<constant> {:% _params.weight__size"}l i!->nstream a.l!resource<constant>gstream{..%yresource<*>c12i{}e% l_params.weight__size=d} "
(  dense<%%arg41) =  : [stream.async.transfer( %f32_params.bias) ->  (:) -0.856674611!

, stream} -> .failureresource<constant> : {pattern failed to match%
_params.bias__size//===-------------------------------------------===//
1.10060418}
,  //===-------------------------------------------===//
->Processing operation : ' tensor.empty'(0x56223be1e0f0!) {
stream-1.07118738  .]resource<*>>{ : %_params.bias__sizetensor<}3
x  f32hal.buffer_view.assert><%arg0 : !hal.buffer_view>
   message%(1" = instream.resource.size%p 9u% = tcst_0"  t0:e" n)s oshaper!(.stream[e.%mresource<constant>c4p
]t  )yutil.global.store " type(%()cst_0% : ,c553648160_i32( )) ->  @encodingtensor<_params.bias(1%xc1_i323)x 
f32:  > %2

 = !stream.tensor.sizeof} -> stream failure. : tensor<resource<constant>pattern failed to match4

x  //===-------------------------------------------===//
f32util.global.store
> //===-------------------------------------------===//
%Processing operation : ' 1flow.dispatch.tensor.load:,'(  0x56223be1de90index) {
@
  _params.bias__size  %3 = stream.tensor.import  %:arg0  index:
   !util.initializer.returnhal
.}buffer_view -> tensor<
4

x} -> f32success>% :  8pattern matchedin = 
 "//===-------------------------------------------===//
f!
lstream//===-------------------------------------------===//
o.Processing operation : 'wresource<external>stream.resource.size.{'(d%0x56223be28700i2) {
s}  patch
.  t%e4n = sostream.async.transferr .%l3o a:d "(%4!)stream.resource<external>{%2} -> !stream <.{resource<*>operandSegmentSizes{ = %2array<}i
32  : %51 = , stream.tensor.sizeof0 , 0, tensor<01, x04>x, f32static_offsets> = array<i64:  0:,  0index%>
4,    = static_sizes%" = 6sarray< = tistream.tensor.sizeofr64 e: tensor<a11m, x.33r>xe, f32sstatic_strides>o = uarray< ri:c64 e: index.1
s,   i1%z>7e} = ">stream.async.dispatch( :  %(@3main_dispatch_0):: : !@(flowmain_dispatch_0_matmul_1x3x4_f32.dispatch.tensor<readonly:tensor<1x3xf32>>() -> !%tensor<stream41.[xresource<constant>%3) -> c0xindex to f32%>5 for 

%

5], %0[%c0
 to   %* Pattern _params.weight__sizemlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp   for  : '** Replace : '%flow.dispatch.tensor.loadstream.resource.size_params.weight__size -> ('(])' {
0x56223be28700, Trying to match ")
%mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp1"
["  %mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp** Modified: 'c0" result util.global.store to 0'(%
0x56223c0361906  )
 for } ->   %failure** Erase   : '6 : stream.resource.size]pattern failed to match'()
0x56223be28700} -> )
failure : pattern failed to match
} -> //===-------------------------------------------===//
success 
 : ://===-------------------------------------------===//
operation was folded Processing operation : '
flow.dispatch.tensor.load(//===-------------------------------------------===//
'(
!0x56223be1fd70//===-------------------------------------------===//
stream) {
Processing operation : '.  util.global.storeresource<*>'({0x56223c036190%) {
5  }, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%6}) -> !stream.resource<*>{%6}
  %c12 = arith.constant%7 = " flo12w : .indexd
i  s%p8a = tcstream.async.transferh .%t7e n:s or.l!ostreama.d"resource<*>"u{(t%%ic123l}). g->l o <b!{astreamoperandSegmentSizesl. = .resource<external>array<s{it%32oc12: r}1e
, "  0(%, %902 = , )stream.tensor.export0 , %08> < , {:static_offsetsglobal  =  = tensor<array<@3i_params.bias__sizex64}f32: >>0 :  , (in0index >) -> , !(static_sizesstream) = .array<resource<external>i{64

%: c124},  3->> , static_strides = !} -> array<halfailurei. : 64buffer_viewpattern failed to match: 
1//===-------------------------------------------===//

, 
  1//===-------------------------------------------===//
return>Processing operation : ' }util.global.store%>'(9 : 0x56223c036110 () {
:   !flow.!dispatch.tensor<readonly:tensor<4x3xf32>>hal) -> .buffer_viewtensor<
4}x3xf32>


} -> success : 

pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer
'(  0x56223b096690* Pattern ) {
mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp   : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  "util.global%.17s = t"osrter"e%(a6%m = 3.")afslyon <wc{..globaldt = ir@sa_params.biaspn}as>tf : ce(hr."t(e!%nstream15s., oresource<constant>%r) -> 16.(, l)%o16a)d

 : "((%2)!stream} -> .failureresource<*> : ,  <pattern failed to matchindex{
, operandSegmentSizes//===-------------------------------------------===//
index = 
) -> array<//===-------------------------------------------===//
!iProcessing operation : 'stream32stream.async.constant.: '(resource<external>10x7f71e00049b0, ) {


0  , 0, 0, 0>, static_offsets = } -> array<failurei : 64pattern failed to match: 
0//===-------------------------------------------===//
, 
0//===-------------------------------------------===//
>Processing operation : ', stream.tensor.exportstatic_sizes'( = 0x56223bb83690array<) {
i  64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>%) -> 3tensor< = 1"xs4txrf32e>am.as

ync%.18
c =   o"* Pattern nsmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOpst : 'trflow.dispatch.tensor.loadae -> (na)' {
tmTrying to match "".mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp(t"
%e"2nmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeDispatchTensorLoadOp)s" result o0r <
.{  evalue} -> x = failurep : dense<opattern failed to matchr
t[} -> "failure( : %pattern failed to match17
, //===-------------------------------------------===//
-0.856674611%
, 16//===-------------------------------------------===//
)Processing operation : 'stream.binding.subspan'(0x56223b1239e01.10060418) {
 <,   {source_encoding = -1.07118738tensor<]3>x : f32>tensor<}3>x : f32(>}>! : stream(.indexresource<external>) -> , index) -> !stream!.halresource<constant>.buffer_view%5

 = 

"strea} -> mfailure.
 : b  pattern failed to matchi* Pattern 
nmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp//===-------------------------------------------===//
d : '
istream.tensor.export//===-------------------------------------------===//
n -> (Processing operation : 'g)' {
arith.constant.Trying to match "'(smlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp0x7f71e0008e80u"
) {
b  s  p  a** Failure : n"tensor export not handled(
%"arg3mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp, " result %01
)   : } -> (failure : pattern failed to match
!} -> streamfailure. : bindingpattern failed to match, 
index//===-------------------------------------------===//
) -> 
//===-------------------------------------------===//
Processing operation : 'arith.constant'(!0x7f71f8007b90flow) {
.  dispatch.tensor<writeonly:tensor<1x3xf32>>

%2 = "arit
h  .* Pattern cmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOpo : 'nstream.binding.subspans -> (t)' {
aTrying to match "nmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOpt"
"(")mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOp" result 0
   <} -> {failurevalue% :  = 16pattern failed to match12 = 
 : "} -> indexafailure}r : >ipattern failed to match : t
(h//===-------------------------------------------===//
) -> .
indexc//===-------------------------------------------===//
oProcessing operation : 'n

stream.binding.subspans'(t0x56223b123800a) {
n  t"()  ** Insert  : ' <arith.constant{'(value0x7f71e00081a0 = )
12 :   index** Replace : '}arith.constant>'( : 0x7f71e0008e80()
) -> index  ** Modified: 'util.global.store'(0x56223c036190

)
  ** Modified: 'stream.async.constant'(0x7f71e00049b0)
  ** Erase   : 'arith.constant'(0x7f71e0008e80)
%} -> 4  success = ** Insert  : ' : "arith.constantoperation was foldeds'(
t0x7f71f8007c00//===-------------------------------------------===//
r)

e//===-------------------------------------------===//
  aProcessing operation : '** Replace : 'mstream.async.constantarith.constant.'('(b0x7f71e00049b00x7f71f8007b90i) {
)
n  d  i** Modified: 'nstream.async.transferg'(.0x56223b096690s)
u  b** Modified: 'sstream.async.transferp'(a0x56223b096690n)
"  (** Modified: '%stream.tensor.exportarg2'(, 0x56223bb83690%)
1)   : ** Erase   : '(arith.constant'(0x7f71f8007b90)
!stream.} -> bindingsuccess,  : indexoperation was folded) -> 
//===-------------------------------------------===//

//===-------------------------------------------===//
!Processing operation : '%flowstream.tensor.export3.'( = dispatch.tensor<readonly:tensor<1x3xf32>>0x56223bb83690") {
s

  tream
.  a* Pattern smlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOpy : 'nstream.binding.subspanc -> (.)' {
cTrying to match "omlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOpn"
s"tmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOpa" result n0t
"  (} -> %failure%0 : 18)pattern failed to match = 
"} ->  <sfailure{t : valuerpattern failed to match = e
adense<//===-------------------------------------------===//
m
.//===-------------------------------------------===//
[tProcessing operation : 'estream.binding.subspann'(s0x56223b123600o) {
r-0.856674611  ., export"(1.10060418%, 17, %0)-1.07118738]> < : {source_encodingtensor< = 3xtensor<f323>x}f32>> : }(>index : ) -> (%!3!stream = stream.".resource<constant>sresource<external>t, rindexe

) -> am!.halb.ibuffer_view} -> nfailure

d : ipattern failed to matchn

g  //===-------------------------------------------===//
.* Pattern 
smlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp//===-------------------------------------------===//
u : 'Processing operation : 'bstream.tensor.exportutil.global.stores -> ('(p)' {
0x56223c036190aTrying to match ") {
nmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp  ""
(%  arg1  , ** Failure : %tensor export not handled1
)" : mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp(" result 0
!  stream} -> .failurebinding : , pattern failed to matchindex
) -> } -> failure : pattern failed to match
!//===-------------------------------------------===//
flow
.//===-------------------------------------------===//
dispatch.tensor<readonly:tensor<4x3xf32>>Processing operation : 'stream.async.transfer'(0x56223b096690) {


  
  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOp : 'stream.binding.subspan -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
"//===-------------------------------------------===//
u
t//===-------------------------------------------===//
iProcessing operation : 'lstream.binding.subspan.'(g0x56223b1240f0l) {
%o  17b = a"ls.tsrteoarme."a(s%y0n)c%.2t =  <r"{asglobalnt = sr@fe_params.bias__sizeea}rm>". : (b(%iindex16n) -> , d(%i)0n, g%.

0s)u : b(span} -> "!failure(stream : %.pattern failed to matcharg0resource<*>
, , //===-------------------------------------------===//
%index
1, //===-------------------------------------------===//
)indexProcessing operation : ' : ) -> arith.constant(!'(stream!0x7f71e00081a0.stream) {
resource<external>.  binding

, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>} -> failure : pattern failed to match


//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00
) {
    * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOp : 'stream.binding.subspan -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeBindingSubspanOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %0 = %"0a = r"iatrhi.%tc1ho = .n"csaotrnaisntttha".n(ct)o"n(s) <t{ <avalue{n = valuet12 = " : 12(index : )}index>} < : >{( : value) -> ( = index) -> 0index : index

}

} -> >} -> failure : failure : ( : pattern failed to match) -> pattern failed to match
index
//===-------------------------------------------===//
//===-------------------------------------------===//




} -> //===-------------------------------------------===//
//===-------------------------------------------===//
failureProcessing operation : 'Processing operation : ' : util.global.storestream.async.dispatchpattern failed to match'('(
0x56223c03c8900x56223bfbe2c0//===-------------------------------------------===//
) {
) {

    //===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %16 = "stream.async.dispatch"(%"13u, t%i9l%, .0%g = 10l", oa%br14ai, lt%.h6s., tc%oo15rn, es%"t4(a, %n%2t4)", (%)4 <, { <%global{14 = value, @ = %_params.weight__size60.000000e+00},  : >%f32 : 15}(, >index% : ) -> 14((, ) -> )%f326, %

15

, } -> %failure15} ->  : )failurepattern failed to match : 
pattern failed to match//===-------------------------------------------===//
 <

{//===-------------------------------------------===//
//===-------------------------------------------===//
entry_points
Processing operation : ' = //===-------------------------------------------===//
stream.return[Processing operation : ''(@util.global.store0x56223bfc47a0main_dispatch_0'() {
::0x56223bb902b0  @) {
main_dispatch_0_matmul_1x3x4_f32  ], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, "!sstreamt.rresource<*>e, aindexm, .indexr, eindext, uindexr, nindex", ("index%u, 0tindex#i, 0lindex, ., %gindex0l, #oindex1b, , aindex%l, 0.index#s, 2tindex)o) ->  : r(!eindexstream", .(indexresource<*>%, 1

index)) -> () <

{} -> globalfailure} ->  =  : failure@pattern failed to match : _params.weight
pattern failed to match}//===-------------------------------------------===//

>
//===-------------------------------------------===//
 : //===-------------------------------------------===//

(Processing operation : '//===-------------------------------------------===//
stream.tensor.sizeofProcessing operation : ''(!stream.executable.export0x56223c03d5d0stream'() {
.0x56223c036400  resource<constant>) {
) -> ()} -> failure : pattern failed to match
//===-------------------------------------------===//



//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  } -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.resource.size'(0x56223bfbe570) {
  %15 = "stream.tensor.sizeof"()% <0{:encoding3 =  = "ftensor<l1oxw3.xdf32i>s}p>a : t(c) -> hindex.wor%k

2g = 
r"  os* Pattern utmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOppr : '_estream.tensor.sizeofca -> (om)' {
u.Trying to match "nrmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOpte"
_sforuor  mc  _e** Insert  : 's.arith.constantls'(ii0x7f71f8007b90cz)
ee"  "(  ()** Replace : '% : stream.tensor.sizeof1('()) -> 0x56223c03d5d0 : ()
(index  ,   index** Modified: '!, stream.async.dispatchstreamindex'(.)0x56223bfbe2c0resource<constant>)


) ->   } -> index  failure** Modified: ' : stream.async.dispatch

pattern failed to match'(
0x56223bfbe2c0//===-------------------------------------------===//
)
    ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
    ** Modified: '} -> stream.async.dispatchfailure'( : 0x56223bfbe2c0pattern failed to match)

  //===-------------------------------------------===//
  
** Erase   : '//===-------------------------------------------===//
stream.tensor.sizeofProcessing operation : ''(stream.tensor.constant0x56223c03d5d0'()
0x56223bfbbeb0") {
mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp  " result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
%1 = "stream.tensor.constant"() <{result_encoding = tensor<4x3xf32>, value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, func.func-0.719257593 ]@, main[(%arg0: !hal-0.403343529., buffer_view) -> !hal.-0.596635341buffer_view,  attributes0.182036489 {]iree.abi.stub]}>  : {
tensor<4  x%3c12x = f32arith.constant>}>  : (12) ->  : index
  !%streamc553648160_i32. = resource<constant>arith.constant 

553648160 : i
32  
* Pattern   mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorConstantOp% : 'c1_i32stream.tensor.constant =  -> (arith.constant)' {
Trying to match " mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorConstantOp1"
 : i32
  %c4 = arith.constant 4 : index
      %** Insert  : 'c0arith.constant = '(arith.constant0x7f71e0008e80)
 0 : index
      %** Insert  : '_params.weightstream.async.constant = '(util.global.load0x56223be28700 )
@  _params.weight  ** Replace : 'stream.tensor.constant'( 0x56223bfbbeb0:)
     ** Modified: 'util.global.store!'(stream0x56223bb902b0.)
resource<constant>  
    ** Modified: '%stream.resource.size_params.weight__size'( = 0x56223bfbe570util.global.load)
   @  _params.weight__size** Erase   : 'stream.tensor.constant'(0x56223bfbbeb0)
 ":mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorConstantOp " result index1

    %} -> _params.biassuccess =  : util.global.loadpattern applied successfully 
@// *** IR Dump After Pattern Application ***
_params.biasmlir-asm-printer: Verifying operation: util.initializer
 : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%util.initializer_params.bias__size}
  hal.buffer_view.assert <{%
arg0   %:c12  = !arith.constanthal.buffer_view>  12message : (index"
i  n%pc48u = tarith.constant 0") shape( [48% : c4index]
)   %typecst( = %stream.async.constantc553648160_i32 ):  encoding(%c1_i32)!
stream  .%resource<constant>2{ = %stream.tensor.sizeofc48 } tensor<=4 xdense<f32>[[ : index
  %3 = 1.54099607stream.tensor.import,  %arg0 :-0.293428898 , !hal.buffer_view ->-2.17878938 ], tensor<[4xf32> in0.568431258 , !stream.resource<external>{-1.08452237%, 2}-1.39859545
]  , %[4 = stream.async.transfer %0.4033468373,  : 0.838026344!, stream.resource<external>{%2-0.719257593}] , ->[ !stream.-0.403343529resource<*>, {%2}
-0.596635341  , %5 = stream.tensor.sizeof 0.182036489tensor<]1]x>4 : xtensor<f324>x3xf32> : index
  %
c12_0   = %arith.constant0 =  stream.resource.size12  : %indexcst
   :% 6 = stream.async.dispatch! stream@.main_dispatch_0resource<constant>::
@  main_dispatch_0_matmul_1x3x4_f32util.global.store( %%4cst[,% c0@ to _params.weight%5 for %5 ]:,  %0[!%streamc0. to resource<constant>%
_params.weight__size   for util.global.store% _params.weight__size%]0, ,% 1@[_params.weight__size%c0 to % c12_0: for  %indexc12_0
]  )%cst_0 = stream.async.constant : !stream.resource<constant>{ %:c12 } (= !dense<stream.resource<*>[{%5}, !-0.856674611stream, .resource<*>{%_params.weight__size}1.10060418, , !stream.resource<*>{-1.07118738%]c12_0>} : ) -> tensor<3xf32>!stream.resource<*>{
%  c12_0util.global.store} 
%  cst_0%,7// -----// IR Dump After   = EncodeDeviceTensors@stream.async.transfer (_params.bias iree-stream-encode-device-tensors%) 6 //----- //
: mlir-asm-printer :: Verifying operation:  stream.executable!
!streamstream..resource<constant>resource<*>
{  %util.global.storec12 }% c12->,  @!_params.bias__sizestream.resource<external>{ %:c12 }index

    %util.initializer.return8
 = }stream.tensor.export %
7

 } -> :success  : pattern matchedtensor<
3//===-------------------------------------------===//
x
f32//===-------------------------------------------===//
>Processing operation : ' stream.resource.sizein'( 0x56223bfbe570) {
  !stream.resource<external>{%c12} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %3 = "stream.resource.size"(%2) : (!stream.resource<constant>) -> index

  ** Replace : 'stream.resource.size'(0x56223bfbe570)
  ** Modified: 'util.global.store'(0x56223c03c890)
  ** Erase   : 'stream.resource.size'(0x56223bfbe570)
} -> %success16 :  = operation was folded"
s//===-------------------------------------------===//
t
r//===-------------------------------------------===//
eProcessing operation : 'autil.global.storem'(.0x56223c03c890a) {
s  ync.dispatch"(%13, %9, %10, %14, %6, %15, %4, %4, %4, %14, %6, %15, %14, %6, %15, %15) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0", u3t, i3l, .3g, l3o, b3a, l1.>s, ttied_operandso = r[e-1" : (index%]1})> : (! <stream{.globalresource<*> = , @!_params.weight__sizestream}.>resource<*> : , (!indexstream) -> .(resource<*>), index, index, 

index, index, index, index, index, index, } -> indexfailure,  : indexpattern failed to match, 
index//===-------------------------------------------===//
, 
index//===-------------------------------------------===//
, Processing operation : 'indexutil.global.store) -> '(!0x56223bb902b0stream) {
.  resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007b90) {
  "util.global.store"(%%215) = "arit <h{.globalc = o@n_params.weights}t>a : n(t"()!stream <.{resource<constant>value) ->  = (12) : index}> : 

() -> index} -> 

failure : pattern failed to match
//===-------------------------------------------===//
  
** Replace : '//===-------------------------------------------===//
arith.constantProcessing operation : ''(stream.async.constant0x7f71f8007b90'()
0x56223be28700) {
    ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
  ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
  ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
  ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
  ** Erase   : 'arith.constant'(0x7f71f8007b90)
} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %2 = "stream.async.c%o15n = s"tsatnrte"a(m%.1a)sync.di <s{pvaluea = tch"dense<(%13, %9[, [%10, %14, %6, %0, %4, %4, %1.540996074, , %14, %6, %0, -0.293428898%, 14, %6, %0, %0)-2.17878938], [ <{entry_points0.568431258 = , [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32]-1.08452237, , operandSegmentSizes = array<i32: -1.398595450], , 3[, 3, 3, 3, 3, 10.403346837>, , tied_operands = [-1 : index0.838026344], }> : (!-0.719257593stream]., resource<*>[, !stream.resource<*>, -0.403343529!, stream.resource<*>, index, index, index, -0.596635341index, , index, index, index, index, 0.182036489index], ]index>,  : index, tensor<index4, xindex3) -> x!f32stream>.}resource<*>> : (

index) -> !stream.resource<constant>} -> 

failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.sizeof'(} -> 0x56223c03ed10failure) {
 :   pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008e80) {
  %14 = "stream.tensor.sizeof"() <{encoding% = 1tensor< = 1"xa4rxif32t>h}.>c : o(n) -> sindextan

t"
(  )* Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp : 'stream.tensor.sizeof -> ()' {
 <Trying to match "{mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOpvalue"
 = 48 : index}> : () -> index

    ** Insert  : 'arith.constant'(0x7f71f8007b90)
    ** Replace : 'stream.tensor.sizeof'(0x56223c03ed10)
    ** Insert  : '  arith.constant** Modified: ''(stream.async.dispatch0x7f71e0008210'()
0x56223bfbe2c0)
      ** Replace : '** Modified: 'arith.constantstream.async.dispatch'('(0x7f71e0008e800x56223bfbe2c0)
)
      ** Modified: '** Modified: 'stream.async.dispatchutil.global.store'('(0x56223bfbe2c00x56223c03c890)
)
      ** Modified: '** Erase   : 'stream.async.constantstream.tensor.sizeof'('(0x56223be287000x56223c03ed10)
)
  "** Erase   : 'mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOparith.constant" result '(10x7f71e0008e80
)
  } -> } -> successsuccess :  : operation was foldedpattern applied successfully

//===-------------------------------------------===//
// *** IR Dump After Pattern Application ***

mlir-asm-printer//===-------------------------------------------===//
: Verifying operation: Processing operation : 'func.funcstream.async.constant
'(0x56223be28700) {
  %2 = "stream.asynstream.executablec .privatec o@nmain_dispatch_0stant"(%0) { <
{  valuestream.executable.export =  public dense<@main_dispatch_0_matmul_1x3x4_f32 [workgroups([) -> (index, index, index)1.54099607 , {
    %-0.293428898x, , func.func%y , -2.17878938@%]mainz, ( = [%flow.dispatch.workgroup_count_from_slicearg0 : 
0.568431258    !, stream.returnhal .%buffer_viewx-1.08452237), ,  -> %y!, hal%-1.39859545.z]buffer_view , :[  attributesindex {, iree.abi.stub0.403346837index}, ,  index{

0.838026344    , }%c12 = arith.constant-0.719257593] , 
12[   : builtin.moduleindex 
-0.403343529{  , 
%c553648160_i32 =     arith.constant-0.596635341func.func,   553648160@ : main_dispatch_0_matmul_1x3x4_f320.182036489i(]32%]
arg0>  :  : %c1_i32!tensor< = stream4arith.constant.xbinding 3, 1x% : f32arg1i>: 32}!
>stream   : .%(bindingc4index,  = ) -> %arith.constantarg2!:  stream!4.stream : resource<constant>.indexbinding
, 

  %%arg3c0:  = } -> !arith.constantfailurestream :  .pattern failed to match0binding
 : )//===-------------------------------------------===//
index

//===-------------------------------------------===//
   Processing operation : '%{util.global.store_params.weight
'( =       0x56223c03c890util.global.load%) {
 cst  @ = _params.weightarith.constant :  !stream.resource<constant>
0.000000e+00   : %f32_params.weight__size
 =       util.global.load% c0@ = _params.weight__sizearith.constant :  index
0   : %index_params.bias
 =       util.global.load% 0@ = _params.biasstream.binding.subspan"  u%:targ0 i[l!%.streamc0]g. lresource<constant>:o
 b  a!%lstream_params.bias__size.. = sbindingutil.global.loadt  o->@r _params.bias__sizee"( !%:flow0 .)indexdispatch.tensor<readonly:tensor<1x4xf32>>

   <      %{%0global1 =  =  = stream.async.transfer@stream.binding.subspan _params.weight__size %}%_params.weight>arg1  : [:(% indexc0!stream) -> ].( resource<constant>):{ %

_params.weight__size!}stream .->binding} ->   failure!-> : stream pattern failed to match.
!resource<*>//===-------------------------------------------===//
flow{
.%_params.weight__size//===-------------------------------------------===//
dispatch.tensor<readonly:tensor<4x3xf32>>}Processing operation : '

arith.constant        '(%%0x7f71e000821021) {
 =  =   stream.binding.subspanstream.async.transfer  %%arg2_params.bias[ %:c0 ] :! stream.resource<constant>!{stream%._params.bias__sizebinding}  ->->  !stream!.flowresource<*>.{dispatch.tensor<readonly:tensor<1x3xf32>>%
_params.bias__size      }%
3   = hal.buffer_view.assertstream.binding.subspan<% %0%arg0 = arg3 "[:a% rc0!i]halt .h:buffer_view. >c !omessagestreamn(.s"bindingti an->np tu"!t(flow ).0dispatch.tensor<writeonly:tensor<1x3xf32>>" <
) {      shapevalue%( = 4[48 = % : flow.dispatch.tensor.loadc4index ]}%)>0  : ,type( offsets() ->  %index=c553648160_i32 

)[} ->  0failureencoding,  : (0pattern failed to match%]
c1_i32,//===-------------------------------------------===//
) 
sizes   %=2  = [stream.tensor.sizeof1 , 4tensor<]4,x f32strides> = [1 , :1 ]index
  %3 = stream.tensor.import %arg0
 //===-------------------------------------------===//
: Processing operation : ' :util.initializer.return !'(hal0x56223c034c30!.) {
flowbuffer_view   .->dispatch.tensor<readonly:tensor<1x4xf32>>  ->tensor< 4xtensor<f321>x 4inx f32>!
stream      .%resource<external>5{ = %2flow.dispatch.tensor.load} %1, 
offsets   %=4  = [stream.async.transfer0 , %03]" ,u: t sizesi !l=stream. .i[resource<external>n4{i, %t32i]}a, l ->istrides z !streame=.r resource<*>.[{r1%e, 2t1}u]
r  n%"c16( =  )arith.constant: :  ( ) -> !16(flow : ).indexdispatch.tensor<readonly:tensor<4x3xf32>>


   ->%} ->  5failuretensor< =  : 4stream.async.dispatchpattern failed to matchx 
3//===-------------------------------------------===//
@x
main_dispatch_0f32//===-------------------------------------------===//
::>Processing operation : '@
util.global.storemain_dispatch_0_matmul_1x3x4_f32      '((%0x56223c036190%6) {
4 =   [flow.dispatch.tensor.load% c0% to 2%,c16  for offsets% c16=] , [%00, [0%]c0, to  %sizes_params.weight__size  for ="% u_params.weight__size[t]1i, , l%3.1]g[,l% oc0stridesb to  a%=lc12 . for [s%1tc12, o]1r)]e"(%1) < {: global :( =  !@stream_params.bias__size!.}flowresource<*>>.{ : dispatch.tensor<readonly:tensor<1x3xf32>>%( c16index->}) ->  , (tensor<!)1streamx

.3resource<*>x{} -> f32%failure>_params.weight__size : 
}pattern failed to match      , 
%!stream//===-------------------------------------------===//
7.
 = resource<*>//===-------------------------------------------===//
tensor.empty{Processing operation : '(%util.global.store)c12'( }0x56223c036110:) -> ) {
   !tensor<stream1.xresource<*>3{x%f32c12>}

        %%86 =  = linalg.fillstream.async.transfer %5 : !stream. ins(resource<*>{%%cstc12" : }uf32 t)->i outs( l%!.7streamg : .ltensor<resource<external>o1{bx%a3c12lx}.f32
s>  t)%o -> 7rtensor< = e1stream.tensor.export"x (3%%x63f32 )>: <
 {      tensor<global%3 = 9x@ = f32_params.biaslinalg.matmul>} >in :  ins( (!%stream4!., streamresource<external>%.{5resource<constant>% : ) -> c12tensor<(}1) x->

4 x} -> !f32failurehal> : ., pattern failed to matchbuffer_viewtensor<
4//===-------------------------------------------===//

x
  3//===-------------------------------------------===//
returnxProcessing operation : ' f32stream.async.constant%>'(7)0x7f71e00049b0  outs() {
:%   8 : !haltensor<.1buffer_viewx
3}xf32>)
 -> 

tensor<} -> 1successx : 3pattern matchedx
f32//===-------------------------------------------===//
>

//===-------------------------------------------===//
      Processing operation : '%stream.async.dispatch10'(% = 0x56223bfbe2c03linalg.generic) {
 =    "liuyinuo genericstream.async.constant"(%1) <{value = {dense<indexing_maps = [[affine_map<-0.856674611%, 15 = "stre1.10060418a, (md.0-1.07118738a, ]sd>y1 : ntensor<)c3 -> (.xddf320i>, s}dp>1a : )t(>cindex, h) -> affine_map<"!(stream%.13resource<constant>, (%d

90, , %d} -> 101failure, ) : % -> (pattern failed to match14d
, 0//===-------------------------------------------===//
%, 
6d//===-------------------------------------------===//
, 1Processing operation : '%)util.global.store0>'(, , 0x56223c03c890%affine_map<) {
4  , %4, %4, %14, %6(, d%00, , d%114),  -> (%d60, , %d01, )%>0]), iterator_types = "[u <"t{pientry_points = al[r.@agmain_dispatch_0ll::lo@ebmain_dispatch_0_matmul_1x3x4_f32la]"l, , .operandSegmentSizes"s = ptarray<aoirr32ae: l"0l(, e%3l0, ")3], } <3{, global ins(3 = %, @93_params.weight__size, , }%1>6> :  : , (tied_operandstensor<index = 1) -> [x(-13) : xindex

f32]>}, >} -> tensor< : failure1( : xpattern failed to match!3
streamx//===-------------------------------------------===//
.f32
resource<*>>//===-------------------------------------------===//
, )Processing operation : '! outs(util.global.storestream%'(.70x56223bb902b0resource<*> : ) {
, tensor<  !1streamx.3resource<*>x, f32index>, )index, index, index, index, index, index, index,  index{, 
index,       index^bb0, (index%, inindex: ") -> f32u!, tstream%i.in_0lresource<*>: .f32

g, l%ooutb: af32l} -> ).failure:s : 
tpattern failed to match        o
%r//===-------------------------------------------===//
11e
 = "//===-------------------------------------------===//
arith.addf(Processing operation : ' %arith.constant%2'(in)0x7f71f8007b90,) {
  <  %{in_0global = @_params.weight}> : (!stream. resource<constant>:) ->  (f32)
        

linalg.yield %} -> 11%failure : 14 : f32 = pattern failed to match
"
      a//===-------------------------------------------===//
}r
 -> i//===-------------------------------------------===//
ttensor<Processing operation : 'h1stream.async.constant.x'(c30x56223be28700ox) {
nf32  s>t
a      ntflow.dispatch.tensor.store" ()%10,  <%{3value, =  16offsets :  index=} >[ : 0(, ) -> 0index]%,

2  = sizes" s  =t** Insert  : ' rarith.constant[e'(1a0x7f71f8009630, m)
3.]  a,** Replace : 's arith.constantystrides'(n 0x7f71f8007b90c=)
.   c[** Modified: 'o1stream.async.dispatchn, '(s10x56223bfbe2c0t])
a  n** Modified: 'tstream.async.dispatch"'( (0x56223bfbe2c0:%)
 0  tensor<)** Modified: '1stream.async.dispatchx <'(3{0x56223bfbe2c0xvalue)
f32 =   >** Erase   : 'dense< arith.constant->'([ 0x7f71f8007b90[)
!flow} -> .successdispatch.tensor<writeonly:tensor<1x3xf32>> : 
1.54099607operation was folded      , 
return//===-------------------------------------------===//


-0.293428898    //===-------------------------------------------===//
, }Processing operation : '
stream.async.dispatch  -2.17878938'(}]0x56223bfbe2c0
, ) {
}[  

0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, %15 = "str-0.596635341e, am.async0.182036489.]d]i>s : ptensor<a4txc3hx"f32(>%}14>,  : %(10index, ) -> %11, !%stream0., resource<constant>%7, %

1, %5, %5, } -> %failure5 : , pattern failed to match%
0//===-------------------------------------------===//
, 
%//===-------------------------------------------===//
7Processing operation : ', arith.constant%'(10x7f71e00081a0, ) {
%  0, %7, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index%]1} = >" : a(rit!hstream..cresource<*>o, n!sstreamt.aresource<*>n, t!"stream(.)resource<*>, index,  <index{, valueindex = , 12index : , indexindex}, >index : , (index) -> , indexindex, index

, } -> indexfailure,  : indexpattern failed to match, 
index//===-------------------------------------------===//
, 
index//===-------------------------------------------===//
) -> Processing operation : '!arith.constantstream'(.0x7f71e0008210resource<*>) {
  

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//
%0 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  // -----// IR Dump After EncodeHostTensors (iree-stream-encode-host-tensors) //----- //
mlir-asm-printer: Verifying operation: util.initializer
%14 = "stream.async.transfer"(%13, %12, %12) : (!stream.resource<external>, index, index) -> !stream.resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  util.initializer {
  %c48 = arith.constant 48 : index
  %c12 = arith.constant %1213 :  = index"
s  t%rcste = astream.async.constantm .:t ensor.!istreamm.presource<constant>o{r%tc48"}( %=arg0 , dense<%12)[[ <{result_encoding = tensor<4xf32>1.54099607}, > : (!hal-0.293428898., buffer_view, index) -> -2.17878938!]stream, .[resource<external>

0.568431258, 
  -1.08452237* Pattern , mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp : 'stream.tensor.import -> (-1.39859545)' {
]Trying to match ", mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp["
  0.403346837  , ** Failure : tensor import not handled
"0.838026344mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp, " result 0
  } -> failure : -0.719257593pattern failed to match]
, } -> [failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
-0.403343529Processing operation : ', stream.tensor.sizeof'(0x56223c0362d0) {
  -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.global.store% 12% = c48",s t@r_params.weight__sizeeam.t e:n sindexo
r  .%scst_0i = zstream.async.constante o:f "()!stream.resource<constant> <{{%encodingc12 = } tensor<=4 xdense<f32>}[> : () -> index-0.856674611

, 
  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp : 'stream.tensor.sizeof1.10060418 -> (, )' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp"
-1.07118738]> : tensor<3xf32>    ** Insert  : 'arith.constant
'(  0x7f71f8007b90util.global.store)
   %  cst_0** Replace : ',stream.tensor.sizeof '(@0x56223c0362d0_params.bias)
   :   ** Modified: 'stream.async.transfer'(!0x56223b0a2340stream)
.  resource<constant>  
** Modified: '  stream.async.transferutil.global.store'( 0x56223b0a2340%)
c12  ,   ** Modified: '@stream.tensor.import_params.bias__size'(0x7f71f000adc0)
   :   ** Erase   : 'indexstream.tensor.sizeof
'(  0x56223c0362d0util.initializer.return)

"}mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorSizeOfOp" result 1


  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %_params.bias = util.global.load @_params.bias
//===-------------------------------------------===//
 Processing operation : ':arith.constant '(0x7f71e0008210) {
!  stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %%_params.bias0  = :" ar!istreamt.hresource<constant>.{c%o_params.bias__sizen}s t->a n!tstream".(resource<*>){%_params.bias__size}
 <  {hal.buffer_view.assertvalue< = %48arg0 :  index:} >! : hal(.) -> buffer_viewindex> message(

"} -> ifailuren : ppattern failed to matchu
t//===-------------------------------------------===//
 
0//===-------------------------------------------===//
"Processing operation : ')arith.constant '(shape0x7f71e00081a0() {
[  %c4]) type(%c553648160_i32) encoding(%c1_i32)
  %c16_0 = arith.constant 16 : index
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in% 1 = !"streama.rresource<external>i{t%hc16_0.}consta
n  t%"3( = )stream.async.transfer %2 < {:value  = 12! : streamindex.}resource<external>>{ : %(c16_0) -> }index -> 

!} -> streamfailure. : resource<*>pattern failed to match{
%//===-------------------------------------------===//
c16_0
}//===-------------------------------------------===//

Processing operation : '  stream.async.constant%'(40x56223be28700 = ) {
stream.async.dispatch   @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) %:2  = ("st!rstreame.aresource<*>m{.%ac16s}y, nc!.streamc.oresource<*>n{s%t_params.weight__sizea}n, t!"stream(.%resource<*>0{)%c12}) ->  <{value = !streamdense<.resource<*>{[%[c12}
  %5 = stream.async.transfer %41.54099607 , : !stream-0.293428898., resource<*>{%c12} ->-2.17878938 ], ![stream.resource<external>{%c12}0.568431258
,   %6 = stream.tensor.export-1.08452237 , %5 : -1.39859545tensor<]3, x[f32> in 0.403346837!, stream.resource<external>{%0.838026344c12, } -> !hal-0.719257593.]buffer_view, [
  -0.403343529return,  %6 : -0.596635341, !hal.buffer_view
}0.182036489]]>
 : 

tensor<} -> 4successx : 3pattern matchedx
f32//===-------------------------------------------===//
>
}//===-------------------------------------------===//
>Processing operation : ' : stream.tensor.import('(index0x7f71f000adc0) -> ) {
  !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  %13 = "stream.tensor.import"(%arg0, %12) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, "indexu) -> ti!lstream..gresource<external>lob

al.store"(%
2  )* Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp : 'stream.tensor.import -> ( <)' {
{Trying to match "globalmlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp = "
@_params.weight  }  >** Failure :  : tensor import not handled(
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp!" result stream0.
resource<constant>  ) -> } -> (failure) : pattern failed to match


} -> failure : pattern failed to match
//===-------------------------------------------===//


  //===-------------------------------------------===//
* Pattern Processing operation : 'mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpstream.async.transfer : ''(util.global.store0x56223b0a2340 -> () {
)' {
  Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c03c890) {
  %14 = "stream.async.transfer"(%13, %12", u%t12i)l : .(glob!astreaml..resource<external>s, tindexo, rindexe) -> "!(stream%.0resource<*>)

 <{global = @_params.weight__size}>} ->  : failure( : indexpattern failed to match) -> 
(//===-------------------------------------------===//
)
//===-------------------------------------------===//
Processing operation : 'arith.constant

'(0x7f71f8007b90) {
  
  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %12 = "arith.constant"() <{value = 16 : index}> : () -> index%3 = 

"stream.  a** Replace : 'sarith.constanty'(n0x7f71f8007b90c)
.c  o** Modified: 'nstream.tensor.imports'(t0x7f71f000adc0a)
n  t** Modified: '"stream.async.transfer('(%0x56223b0a23401)
)  ** Modified: 'stream.async.transfer'( <0x56223b0a2340{)
value   = ** Erase   : 'arith.constantdense<'(0x7f71f8007b90)
[} -> success : operation was folded
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
-0.856674611  , 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern %mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats13 : ' = stream.async.constant" -> (s)' {
tTrying to match "rmlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplatse"
a"mmlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats." result a0s
y  n} -> cfailure. : tpattern failed to matchr
a} -> nfailures : fpattern failed to matche
r//===-------------------------------------------===//
"
(//===-------------------------------------------===//
%Processing operation : '12util.global.store, '(%0x56223c0361100) {
,   %0) : (!stream.resource<external>, index, index) -> !stream.resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()%

12 = "str
e  a* Pattern mmlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp. : 'tutil.global.storee -> (n)' {
sTrying to match "omlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpr"
."imlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOpm" result p0o
r  t} -> "failure( : %pattern failed to matcharg0
, } -> %failure0 : )pattern failed to match
//===-------------------------------------------===//

 <//===-------------------------------------------===//
{Processing operation : 'result_encodingutil.global.store = '(tensor<0x56223c0361904) {
x  f32>}> : (!hal.buffer_view, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp : 'stream.tensor.import -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp"
    ** Failure : tensor import not handled
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp" result 0
  } -> "failureu : tpattern failed to matchi
l} -> .failureg : lpattern failed to matcho
b//===-------------------------------------------===//
a
l//===-------------------------------------------===//
.Processing operation : 'shal.buffer_view.assertt'(o0x56223be1f320) {
  re"(%1) <{global = @_params.bias__size}> : (index) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> "failureh : apattern failed to matchl
.} -> bfailureu : fpattern failed to matchf
e//===-------------------------------------------===//
r
_//===-------------------------------------------===//
vProcessing operation : 'iutil.initializer.returne'(w0x56223c034c30.) {
a  ssert"(%arg0, %2, %3, %4) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

"util.} -> ifailuren : ipattern failed to matcht
i//===-------------------------------------------===//
a
l//===-------------------------------------------===//
iProcessing operation : 'zstream.async.transfere'(r0x56223b0d21e0.) {
r  eturn"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
%11 = "stream.async.transfer"(%8, %9, %9) : (!stream.resource<constant>, index, index) -> !stream.resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  // -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: util.initializer
%10 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> util.initializer!stream.resource<*> 

{
  %} -> c48failure =  : arith.constantpattern failed to match
//===-------------------------------------------===//

 //===-------------------------------------------===//
Processing operation : '48util.global.load : '(index0x56223c0218f0
) {
    %c12 = arith.constant 12 : index
  %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[%9 = 1.54099607", util.glo-0.293428898b, al.load"-2.17878938(]), [ <{0.568431258global,  = @_params.bias__size}>-1.08452237 : , () -> index-1.39859545

], } -> [failure : pattern failed to match
//===-------------------------------------------===//

0.403346837//===-------------------------------------------===//
, Processing operation : 'util.global.load'(0x56223c021cd0) {
  0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %util.global.store8  = %"cstu,t i@l_params.weight.gl o:b al.l!ostreama.dresource<constant>"
(  )util.global.store % <c48{,global  = @@_params.weight__size_params.bias}> :  (:) ->  index
!  stream%.cst_0resource<constant> = stream.async.constant

 :} ->  failure : pattern failed to match!
stream//===-------------------------------------------===//
.
resource<constant>//===-------------------------------------------===//
{Processing operation : '%util.global.loadc12'(}0x56223c021d90 ) {
=   dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  util.global.store %%7cst_0 = ," u@t_params.biasil.gl o:b al.l!ostreama.dresource<constant>"
(  )util.global.store % <c12{,global  = @@_params.bias__size_params.weight__size}> :  (:) ->  indexindex
  

util.initializer.return} -> 
failure} : pattern failed to match
//===-------------------------------------------===//



//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  // -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: %util.initializer6
 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  util.initializer {
  %c48 = arith.constant%5 =  "48a : rindexi
t  h%.c12c = oarith.constantnstan t12" : (index)
  %cst < = {stream.async.constantvalue  = :0  : index}>! : stream(.) -> resource<constant>index{%c48

}} ->  failure= :  pattern failed to match
dense<//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : '[arith.constant['(0x56223c03c9f0) {
  1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [%4 = "a0.403346837r, ith.cons0.838026344t, ant"()-0.719257593] <, {[value = 4 : -0.403343529index, }> : () -> index-0.596635341, 

} -> failure : pattern failed to match0.182036489
]//===-------------------------------------------===//
]
>//===-------------------------------------------===//
 : Processing operation : 'tensor<arith.constant4'(x0x56223c03c9803) {
x  f32>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.global.store %c48, @_params.weight__size : index
  %%3cst_0 =  = "stream.async.constanta r:i th.!cstreamo.nresource<constant>s{t%ac12n}t "=( )dense< <[{value = 1 : i-0.85667461132, }> : () -> 1.10060418i, 32

-1.07118738} -> ]failure> :  : pattern failed to matchtensor<
3//===-------------------------------------------===//
x
f32//===-------------------------------------------===//
>Processing operation : 'arith.constant'(0x56223c04eaa0) {
  
  util.global.store %cst_0, @_params.bias : !stream.resource<constant>
  util.global.store %c12, @_params.bias__size : index
  util.initializer.return
}%

2 = "arith.constant"()==== REARRANGING BLOCK ACCESSES ====
 <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//
moving mutable global _params.bias store downward
moving mutable global _params.bias__size store downward
moving mutable global _params.weight store downward
moving mutable global _params.weight__size store downward
// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: util.initializer

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  util.initializer"func .{r
e  t%uc48r = narith.constant"(%16 )48 :  : (index
!  hal%.c12buffer_view = ) -> arith.constant() 

12 : index
} ->   failure% : cstpattern failed to match = 
stream.async.constant//===-------------------------------------------===//
 
://===-------------------------------------------===//
 Processing operation : 'stream.tensor.export'(0x56223bb83690) {
!  stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258%, 16 = "strea-1.08452237m, .tensor-1.39859545.]e, x[port"(%150.403346837, , %1)0.838026344,  <{source_encoding = -0.719257593tensor<]3, x[f32>}> : (-0.403343529, !stream.resource<external>, -0.596635341index, ) -> !hal.buffer_view0.182036489]]>

 : tensor<4x3xf32
>  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp : 'stream.tensor.export -> ()' {

Trying to match "  mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp%"
cst_0 =   stream.async.constant   ** Failure : :tensor export not handled 
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorExportOp!" result stream0.
resource<constant>  {} -> %failurec12 : }pattern failed to match 
=} ->  failure : dense<pattern failed to match
//===-------------------------------------------===//
[
//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  -0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  util.global.store %cst_0, @_params.bias : !%stream15. = resource<constant>"
s  tutil.global.storer e%ac12m,. a@s_params.bias__sizeync.t r:a nindexs
f  eutil.global.storer "%(cst%,14 , @%_params.weight1, %1 ): :  (!!streamstream..resource<constant>resource<*>
,   indexutil.global.store,  index%) -> c48,! stream@._params.weight__sizeresource<external> 

: index
  util.initializer.return
} -> }failure : pattern failed to match


//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %14 = "stream.async.dispatch"(%13, %10, %11, %0, %7, %1, %5, %5, %5, %0, %7, %1, %0, %7, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %13 = "stream.async.transfer"(%12, %0, %0) : (!stream.resource<external>, index, index) -> !stream.resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %12 = "stream.tensor.import"(%arg0, %0) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp : 'stream.tensor.import -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp"
    ** Failure : tensor import not handled
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::EncodeTensorImportOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %2, %3, %4) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %11 = "stream.async.transfer"(%8, %9, %9) : (!stream.resource<constant>, index, index) -> !stream.resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %10 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> !stream.resource<*>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %9 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %7 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %6 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %5 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %4 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %2 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %0 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After EncodeHostTensors (iree-stream-encode-host-tensors) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %0 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %2 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %4 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %5 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %6 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %7 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %9 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %10 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %11 = "stream.async.transfer"(%8, %9, %9) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %2, %3, %4) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %12 = "stream.tensor.import"(%arg0, %0) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %13 = "stream.async.transfer"(%12, %0, %0) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %14 = "stream.async.dispatch"(%13, %10, %11, %0, %7, %1, %5, %5, %5, %0, %7, %1, %0, %7, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %15 = "stream.async.transfer"(%14, %1, %1) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %16 = "stream.tensor.export"(%15, %1) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%16) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

moving immutable global _params.weight load to the entry block
moving immutable global _params.weight__size load to the entry block
moving immutable global _params.bias load to the entry block
moving immutable global _params.bias__size load to the entry block
==== REARRANGING BLOCK ACCESSES ====
// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.weight__size = util.global.load @_params.weight__size : index
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %_params.bias__size = util.global.load @_params.bias__size : index
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%16) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %16 = "stream.tensor.export"(%15, %4) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %15 = "stream.async.transfer"(%14, %4, %4) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %14 = "stream.async.dispatch"(%13, %10, %11, %5, %7, %4, %0, %0, %0, %5, %7, %4, %5, %7, %4, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %13 = "stream.async.transfer"(%12, %5, %5) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %12 = "stream.tensor.import"(%arg0, %5) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %3, %2, %1) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %11 = "stream.async.transfer"(%8, %9, %9) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %10 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %0 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %1 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %3 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %4 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %5 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %9 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %7 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %6 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c03c890) {
  "util.global.store"(%0) <{global = @_params.weight__size}> : (index) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036190) {
  "util.global.store"(%1) <{global = @_params.bias__size}> : (index) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
    ** Modified: 'util.global'(0x56223c03d790)
    ** Modified: 'util.global'(0x56223bfa6280)
    ** Erase   : 'util.global.store'(0x56223c036190)
    ** Erase   : 'util.global.store'(0x56223c03c890)
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size = 48 : index
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size = 12 : index
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c03d790) {
  "util.global"() <{initial_value = 12 : index, sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa6280) {
  "util.global"() <{initial_value = 48 : index, sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%16) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %16 = "stream.tensor.export"(%15, %4) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %15 = "stream.async.transfer"(%14, %4, %4) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %14 = "stream.async.dispatch"(%13, %10, %11, %5, %7, %4, %0, %0, %0, %5, %7, %4, %5, %7, %4, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %13 = "stream.async.transfer"(%12, %5, %5) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %12 = "stream.tensor.import"(%arg0, %5) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %3, %2, %1) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %11 = "stream.async.transfer"(%8, %9, %9) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %10 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %9 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %7 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %6 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %5 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %4 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %3 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %1 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %0 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c03d790) {
  "util.global"() <{initial_value = 12 : index, sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa6280) {
  "util.global"() <{initial_value = 48 : index, sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size = 48 : index
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size = 12 : index
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%16) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %16 = "stream.tensor.export"(%15, %4) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %15 = "stream.async.transfer"(%14, %4, %4) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %14 = "stream.async.dispatch"(%13, %10, %11, %5, %7, %4, %0, %0, %0, %5, %7, %4, %5, %7, %4, %4) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %13 = "stream.async.transfer"(%12, %5, %5) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %12 = "stream.tensor.import"(%arg0, %5) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %3, %2, %1) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %11 = "stream.async.transfer"(%8, %9, %9) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %10 = "stream.async.transfer"(%6, %7, %7) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c0218f0) {
  %9 = "util.global.load"() <{global = @_params.bias__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021d90) {
  %7 = "util.global.load"() <{global = @_params.weight__size}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %6 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %5 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %4 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %3 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %2 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %1 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %0 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c03d790) {
  "util.global"() <{initial_value = 12 : index, sym_name = "_params.bias__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa6280) {
  "util.global"() <{initial_value = 48 : index, sym_name = "_params.weight__size", sym_visibility = "private", type = index}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.weight__size = 48 : index
  util.global private @_params.bias : !stream.resource<constant>
  util.global private @_params.bias__size = 12 : index
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.weight__size = util.global.load @_params.weight__size : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %_params.bias__size = util.global.load @_params.bias__size : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%_params.weight__size} -> !stream.resource<*>{%_params.weight__size}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%_params.bias__size} -> !stream.resource<*>{%_params.bias__size}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %_params.weight__size for %_params.weight__size], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%_params.weight__size}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
mlir-asm-printer: Verifying operation: builtin.module
module @LinearModule attributes {hal.device.targets = [#hal.device.target<"cuda", {executable_targets = [#hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>], legacy_sync}>]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %c48 = arith.constant 48 : index
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %c12_0 = arith.constant 12 : index
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12_0} -> !stream.resource<*>{%c12_0}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====
** Replace : 'arith.constant'(0x56223c131470)
** Modified: 'stream.async.transfer'(0x56223b0d21e0)
** Modified: 'stream.async.transfer'(0x56223b0d21e0)
** Erase   : 'arith.constant'(0x56223c131470)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%15) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %15 = "stream.tensor.export"(%14, %5) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %14 = "stream.async.transfer"(%13, %5, %5) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %13 = "stream.async.dispatch"(%12, %9, %10, %6, %0, %5, %1, %1, %1, %6, %0, %5, %6, %0, %5, %5) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %12 = "stream.async.transfer"(%11, %6, %6) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %11 = "stream.tensor.import"(%arg0, %6) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %4, %3, %2) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %9 = "stream.async.transfer"(%7, %0, %0) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %10 = "stream.async.transfer"(%8, %5, %5) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %6 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %5 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %4 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %2 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====
// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c48 = arith.constant 48 : index
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: util.initializer
FuseGlobals: analyzing util.initializer {
  %c48 = arith.constant 48 : index
  %c12 = arith.constant 12 : index
  %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  util.global.store %cst_0, @_params.bias : !stream.resource<constant>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.initializer.return
}:
 - store #1: util.global.store %cst_0, @_params.bias : !stream.resource<constant>; candidate=1
 - store #0: util.global.store %cst, @_params.weight : !stream.resource<constant>; candidate=1
= storing value %cst_0:
 => @_params.bias
= storing value %cst:
 => @_params.weight
mlir-asm-printer: Verifying operation: func.func
FuseGlobals: analyzing func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}:
FuseGlobals correlation maps:
= #0 "_params.weight" = 10:
  => _params.weight
= #1 "_params.bias" = 01:
  => _params.bias
// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c48 = arith.constant 48 : index
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: builtin.module
  !! traversal incomplete due to public function-like op @main
mlir-asm-printer: Verifying operation: builtin.module
FuncAnalysis: INCOMPLETE! @main(!hal.buffer_view) -> !hal.buffer_view 
  args: 1
    %arg0: non-uniform used !hal.buffer_view 
  results: 1
    %result#0: non-uniform used !hal.buffer_view 
  callOps: 0
// -----// IR Dump After IPO (iree-util-ipo) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c48 = arith.constant 48 : index
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWrite (iree-stream-materialize-copy-on-write) //----- //
mlir-asm-printer: Verifying operation: util.initializer
util.initializer {
  %c48 = arith.constant 48 : index
  %c12 = arith.constant 12 : index
  %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  util.global.store %cst_0, @_params.bias : !stream.resource<constant>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.initializer.return
}

// -----// IR Dump After MaterializeCopyOnWrite (iree-stream-materialize-copy-on-write) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

mlir-asm-printer: Verifying operation: builtin.module
[Solver] identified and initialized 0 abstract elements


[Solver] iteration#: 1, worklist size: 0
[Solver] iteration#: 1, worklist + dependent size: 0

[Solver] fixpoint iteration done after: 1/32 iterations
// -----// IR Dump After ElideAsyncCopies (iree-stream-elide-async-copies) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c48 = arith.constant 48 : index
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
    %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
    %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
    %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  
//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  %0 = "arith.constant"() <{value = 48 : index%}0> =  : "(a) -> rindexith.con

sta} -> nfailuret : "pattern failed to match(
)//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'( <0x7f71e00081a0{) {
value   = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %1 = "arith.constant"() <{value = 12 : index}>% : 1( = ) -> "indexarith.

co} -> nfailures : tpattern failed to matcha
n//===-------------------------------------------===//
t
"//===-------------------------------------------===//
(Processing operation : ')stream.async.constant'(0x56223be28700) {
   <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %2 = "stream.async.constant"(%0) <{value = %2 = dense<"arith[.[constant"()1.54099607 <, {value = 4 : index-0.293428898}, > : () -> index-2.17878938]

, [} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : '0.568431258arith.constant, '(0x56223c03c980) {
  -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, %3 = "arith0.182036489.]c]o>ns : tantensor<t4"x(3)xf32>}> < : {(valueindex = ) -> 1 : !istream32.}resource<constant>> : () -> i

32

} -> failure
 :   pattern failed to match* Pattern 
mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats//===-------------------------------------------===//
 : '
stream.async.constant//===-------------------------------------------===//
 -> (Processing operation : ')' {
arith.constant'(Trying to match "0x56223c04eaa0mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats) {
"
  "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %4 = "arith.constant"() <%{3value =  = "s553648160tr : eaim32.}a>s : y(n) -> c.ic32onsta

nt} -> "failure( : %pattern failed to match1
)//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'( <0x7f71f8007c00{) {
value   = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>%5 = "ar

ith.consta
n  t* Pattern "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats( : ')stream.async.constant -> ()' {
Trying to match " <mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats{"
value = "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats12" result 0 : 
index  }} -> >failure :  : (pattern failed to match) -> 
index} -> failure : pattern failed to match
//===-------------------------------------------===//



//===-------------------------------------------===//
} -> Processing operation : 'failureutil.global.store : '(pattern failed to match0x56223c036110
) {
//===-------------------------------------------===//
  
//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  "uti%l6. = g"laorbiatlh..sctoonrset"a(n%t3")() < <{{globalvalue =  = @16_params.bias : }index>} : >( : () -> index!stream.resource<constant>

) -> (} -> )failure : pattern failed to match
//===-------------------------------------------===//



//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  
  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> "util.g!lstreamo.bresource<constant>al.sto

re"(} -> %failure2 : )pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
 <Processing operation : '{util.global.loadglobal'( = 0x56223c021cd0@) {
_params.weight  }> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//
"
u//===-------------------------------------------===//
tProcessing operation : 'istream.async.transferl'(.0x56223b1a5570i) {
n  itializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
%9 = "stream.async.transfer"(%7, %0, %0) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
// -----// IR Dump After Trying to match "Canonicalizermlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> ("
canonicalize")mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> //----- //
" result mlir-asm-printer0: Verifying operation: 
util.initializer  
} -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  util.initializer {
  %c48 = arith.constant 48 : %index10
 =   "%sc12t = rarith.constanteam.as yn12c : .indext
r  a%ncsts = fstream.async.constante r:" (%8, %!5stream, .%resource<constant>5{)% : c48(} = !dense<stream.resource<constant>, index[, [index) -> !stream.resource<*>

1.54099607, -0.293428898
,   * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "-2.17878938mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>]"
, "[mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match0.568431258
, 
  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "-1.08452237mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision, "
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure-1.39859545 : ]pattern failed to match, 
[
  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "0.403346837mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision, "
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> 0.838026344failure,  : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//
-0.719257593
]//===-------------------------------------------===//
, Processing operation : '[hal.buffer_view.assert'(0x56223be1f320) {
  -0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %cst_0 = stream.async.constant : !stream.resource<constant>{%c12}" h=a l.budense<ffer_[view.assert"(-0.856674611%, arg0, %4, %31.10060418, , %2) {message = "-1.07118738i]n>pu : t 0tensor<"3x}f32 : >(!hal.
buffer_view  , util.global.storei 32%, cst_0i,32 , index@) -> _params.bias() :

 !stream.resource<constant>
  util.global.store} ->  failure% : cstpattern failed to match,
 //===-------------------------------------------===//
@
_params.weight//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
   : !stream.resource<constant>
  util.initializer.return
}

%11 = "stream.t// -----// IR Dump After eEmplaceAllocationsn (siree-stream-emplace-allocationso)r //----- //
.mlir-asm-printeri: Verifying operation: mutil.initializerp
ort"(%arg0, %6) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  util.initializer {
  %c48 = arith.constant 48 : index
  %c12 = arith.constant 12 : index
  %cst% = 12stream.async.constant =  ":s tream.!astreams.yresource<constant>n{c%.c48t}r a=n sfedense<r"(%11[, [%6, %6) : (!1.54099607stream, .resource<external>, index, index) -> -0.293428898!, stream.resource<*>-2.17878938

], [0.568431258
,   * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
-1.08452237Trying to match ", mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> -1.39859545failure] : , pattern failed to match[

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
0.403346837Trying to match ", mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  0.838026344} -> , failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : '-0.719257593stream.async.transfer] -> (, )' {
[Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  -0.403343529} -> , failure : pattern failed to match
} -> failure : pattern failed to match
-0.596635341//===-------------------------------------------===//
, 
//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  0.182036489]]> : tensor<4x3xf32>
  %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, %13 = "strea1.10060418m, .async.dis-1.07118738p]a>t : chtensor<"3(x%f3212>, %9, %10, %
6  , %util.global.store0 , %%cst_05,,  %1@, _params.bias%1, %1 , :% 6, %0, !%stream5., resource<constant>%
6  , util.global.store% 0%, cst%,5 , @%_params.weight5) : !stream.resource<constant> <
{  entry_points = util.initializer.return[
@}main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32

], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<*>, !stream.resource<*>, !stream.resource<*>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %14 = "stream.async.transfer"(%13, %5, %5) : (!stream.resource<*>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::IntermediateTransferElision" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %15 = "stream.tensor.export"(%14, %5) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%15) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocations (iree-stream-emplace-allocations) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<*>{%c48}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<*>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

mlir-asm-printer: Verifying operation: builtin.module
[[ Explorer::walkValues ]]
? entering scc slice with 1 callables
   + entering callable region @util.initializer
   + recursing into op util.initializer
   + processing op results arith.constant
  == emitting value %c48
   + processing op results arith.constant
  == emitting value %c12
   + processing op results stream.async.constant
  == emitting value %cst
[Solver] updating: [ValueResourceUsage] value %cst with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %cst
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %cst
  == emitting op util.global.store
  -> traversing into global loads from @_params.weight:
   + queuing loaded value from %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
   ? working on %_params.weight
  == emitting op stream.async.transfer
[Solver] updating: [ValueResourceUsage] value %0 with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %0
  == emitting op stream.async.transfer
[Solver] updating: [ValueResourceUsage] value %_params.weight with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %_params.weight
  == emitting op util.global.load
  -> traversing into global stores to @_params.weight:
   + queuing stored value from util.global.store %cst, @_params.weight : !stream.resource<constant>
   ? working on %cst
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %_params.weight
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %_params.weight with state internal|immutable|constant|transfer_read|transfer_write|global_read|global_write
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %0
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %cst with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
   + processing op results stream.async.constant
  == emitting value %cst_0
[Solver] updating: [ValueResourceUsage] value %cst_0 with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %cst_0
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %cst_0
  == emitting op util.global.store
  -> traversing into global loads from @_params.bias:
   + queuing loaded value from %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
   ? working on %_params.bias
  == emitting op stream.async.transfer
[Solver] updating: [ValueResourceUsage] value %1 with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %1
  == emitting op stream.async.transfer
[Solver] updating: [ValueResourceUsage] value %_params.bias with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %_params.bias
  == emitting op util.global.load
  -> traversing into global stores to @_params.bias:
   + queuing stored value from util.global.store %cst_0, @_params.bias : !stream.resource<constant>
   ? working on %cst_0
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %_params.bias
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %_params.bias with state internal|immutable|constant|transfer_read|transfer_write|global_read|global_write
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %1
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %1 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %cst_0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
? entering scc slice with 1 callables
  -- ignoring callable region @main_dispatch_0_matmul_1x3x4_f32
? entering scc slice with 1 callables
   + entering callable region @main
   + recursing into op main
   + processing block ^bb0 arguments
  == emitting block arg %arg0
   + processing op results arith.constant
  == emitting value %c48
   + processing op results arith.constant
  == emitting value %c0
   + processing op results arith.constant
  == emitting value %c4
   + processing op results arith.constant
  == emitting value %c1_i32
   + processing op results arith.constant
  == emitting value %c553648160_i32
   + processing op results arith.constant
  == emitting value %c12
   + processing op results arith.constant
  == emitting value %c16
   + processing op results util.global.load
  == emitting value %_params.weight
   + processing op results util.global.load
  == emitting value %_params.bias
   + processing op results stream.async.transfer
  == emitting value %0
   + processing op results stream.async.transfer
  == emitting value %1
   + processing op results stream.tensor.import
  == emitting value %2
[Solver] updating: [ValueResourceUsage] value %2 with state external|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %2
  == emitting op stream.tensor.import
   + queuing tied operand %arg0
   ? working on %arg0
  !! traversal incomplete due to public function-like op @main
<< Explorer::walkDefiningOps >> is incomplete
[[ Explorer::walkTransitiveUses ]]
   ? working on %2
  == emitting op stream.async.transfer
[Solver] updating: [ValueResourceUsage] value %3 with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %3
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %3
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %3 with state external|mutable|transfer_read|transfer_write|dispatch_read
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %2 with state external|mutable|transfer_read|transfer_write|dispatch_read
   + processing op results stream.async.transfer
  == emitting value %3
   + processing op results stream.async.dispatch
  == emitting value %4
[Solver] updating: [ValueResourceUsage] value %4 with state internal|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %4
  == emitting op stream.async.dispatch
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %4
  == emitting op stream.async.transfer
[Solver] updating: [ValueResourceUsage] value %5 with state external|immutable
[[ Explorer::walkDefiningOps ]]
   ? working on %5
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %5
  == emitting op stream.tensor.export
   + queuing tied result %6
   ? working on %6
  == emitting op func.return
   + queuing region result %6
  !! traversal incomplete due to public function-like op @main
   ? working on %6
<< Explorer::walkTransitiveUses >> is incomplete
[Solver] update changed [ValueResourceUsage] value %5 with state external|mutable|transfer_read|transfer_write|dispatch_write
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %4 with state external|mutable|transfer_read|transfer_write|dispatch_write
   + processing op results stream.async.transfer
  == emitting value %5
   + processing op results stream.tensor.export
  == emitting value %6
? entering scc slice with 1 callables
<< Explorer::walkValues >> is complete
[Solver] identified and initialized 10 abstract elements


[Solver] iteration#: 1, worklist size: 10
[Solver] iteration#: 1, worklist + dependent size: 10
[Solver] updating: [ValueResourceUsage] value %cst with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %cst
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %cst
  == emitting op util.global.store
  -> traversing into global loads from @_params.weight:
   + queuing loaded value from %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
   ? working on %_params.weight
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %cst with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %0
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %0
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %_params.weight with state internal|immutable|constant|transfer_read|transfer_write|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %_params.weight
  == emitting op util.global.load
  -> traversing into global stores to @_params.weight:
   + queuing stored value from util.global.store %cst, @_params.weight : !stream.resource<constant>
   ? working on %cst
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %_params.weight
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %_params.weight with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %cst_0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %cst_0
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %cst_0
  == emitting op util.global.store
  -> traversing into global loads from @_params.bias:
   + queuing loaded value from %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
   ? working on %_params.bias
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %cst_0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %1 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %1
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %1
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %1 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %_params.bias with state internal|immutable|constant|transfer_read|transfer_write|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %_params.bias
  == emitting op util.global.load
  -> traversing into global stores to @_params.bias:
   + queuing stored value from util.global.store %cst_0, @_params.bias : !stream.resource<constant>
   ? working on %cst_0
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %_params.bias
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update changed [ValueResourceUsage] value %_params.bias with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %2 with state external|mutable|transfer_read|transfer_write|dispatch_read
[[ Explorer::walkDefiningOps ]]
   ? working on %2
  == emitting op stream.tensor.import
   + queuing tied operand %arg0
   ? working on %arg0
  !! traversal incomplete due to public function-like op @main
<< Explorer::walkDefiningOps >> is incomplete
[[ Explorer::walkTransitiveUses ]]
   ? working on %2
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %2 with state external|mutable|transfer_read|transfer_write|dispatch_read

[Solver] updating: [ValueResourceUsage] value %3 with state external|mutable|transfer_read|transfer_write|dispatch_read
[[ Explorer::walkDefiningOps ]]
   ? working on %3
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %3
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %3 with state external|mutable|transfer_read|transfer_write|dispatch_read

[Solver] updating: [ValueResourceUsage] value %4 with state external|mutable|transfer_read|transfer_write|dispatch_write
[[ Explorer::walkDefiningOps ]]
   ? working on %4
  == emitting op stream.async.dispatch
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %4
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %4 with state external|mutable|transfer_read|transfer_write|dispatch_write

[Solver] updating: [ValueResourceUsage] value %5 with state external|mutable|transfer_read|transfer_write|dispatch_write
[[ Explorer::walkDefiningOps ]]
   ? working on %5
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %5
  == emitting op stream.tensor.export
   + queuing tied result %6
   ? working on %6
  == emitting op func.return
   + queuing region result %6
  !! traversal incomplete due to public function-like op @main
   ? working on %6
<< Explorer::walkTransitiveUses >> is incomplete
[Solver] update unchanged [ValueResourceUsage] value %5 with state external|mutable|transfer_read|transfer_write|dispatch_write



[Solver] iteration#: 2, worklist size: 2
[Solver] iteration#: 2, worklist + dependent size: 4
[Solver] updating: [ValueResourceUsage] value %_params.weight with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %_params.weight
  == emitting op util.global.load
  -> traversing into global stores to @_params.weight:
   + queuing stored value from util.global.store %cst, @_params.weight : !stream.resource<constant>
   ? working on %cst
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %_params.weight
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %_params.weight with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %_params.bias with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %_params.bias
  == emitting op util.global.load
  -> traversing into global stores to @_params.bias:
   + queuing stored value from util.global.store %cst_0, @_params.bias : !stream.resource<constant>
   ? working on %cst_0
  == emitting op stream.async.constant
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %_params.bias
  == emitting op stream.async.transfer
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %_params.bias with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %0
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %0
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %0 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write

[Solver] updating: [ValueResourceUsage] value %1 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write
[[ Explorer::walkDefiningOps ]]
   ? working on %1
  == emitting op stream.async.transfer
<< Explorer::walkDefiningOps >> is complete
[[ Explorer::walkTransitiveUses ]]
   ? working on %1
  == emitting op stream.async.dispatch
<< Explorer::walkTransitiveUses >> is complete
[Solver] update unchanged [ValueResourceUsage] value %1 with state internal|immutable|constant|transfer_read|transfer_write|dispatch_read|global_read|global_write


[Solver] fixpoint iteration done after: 2/32 iterations

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyInitializerOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyInitializerOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp> : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp> : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %2 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %4 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %5 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %6 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %9 = "stream.async.transfer"(%7, %0, %0) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
    ** Modified: 'stream.async.transfer'(0x56223b1a5570)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %0 = stream.async.transfer %_params.weight : !stream.resource<constant>{%c48} -> !stream.resource<constant>{%c48}
  %1 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %4 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%3[%c0 to %c16 for %c16], %0[%c0 to %c48 for %c48], %1[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %6 = stream.tensor.export %5 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b1a5570) {
  %9 = "stream.async.transfer"(%7, %0, %0) : (!stream.resource<constant>, index, index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
    ** Replace : 'stream.async.transfer'(0x56223b1a5570)
    ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
    ** Erase   : 'stream.async.transfer'(0x56223b1a5570)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %0 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<*>{%c12}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%2[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %0[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<*>{%c12}) -> !stream.resource<*>{%c12}
  %4 = stream.async.transfer %3 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %5 = stream.tensor.export %4 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %9 = "stream.async.transfer"(%8, %5, %5) : (!stream.resource<constant>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
    ** Modified: 'stream.async.transfer'(0x56223b0d21e0)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %0 = stream.async.transfer %_params.bias : !stream.resource<constant>{%c12} -> !stream.resource<constant>{%c12}
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%2[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %0[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<*>{%c12}
  %4 = stream.async.transfer %3 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %5 = stream.tensor.export %4 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0d21e0) {
  %9 = "stream.async.transfer"(%8, %5, %5) : (!stream.resource<constant>, index, index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
    ** Replace : 'stream.async.transfer'(0x56223b0d21e0)
    ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
    ** Erase   : 'stream.async.transfer'(0x56223b0d21e0)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c16} -> !stream.resource<*>{%c16}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<*>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<*>{%c12}
  %3 = stream.async.transfer %2 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %4 = stream.tensor.export %3 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %4, %3, %2) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %9 = "stream.tensor.import"(%arg0, %6) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorImportOp> : 'stream.tensor.import -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorImportOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorImportOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %10 = "stream.async.transfer"(%9, %6, %6) : (!stream.resource<external>, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
    ** Modified: 'stream.async.transfer'(0x56223b0a2340)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c16} -> !stream.resource<external>{%c16}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%1[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<*>{%c12}
  %3 = stream.async.transfer %2 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %4 = stream.tensor.export %3 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b0a2340) {
  %10 = "stream.async.transfer"(%9, %6, %6) : (!stream.resource<external>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
    ** Replace : 'stream.async.transfer'(0x56223b0a2340)
    ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
    ** Erase   : 'stream.async.transfer'(0x56223b0a2340)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<*>{%c12}
  %2 = stream.async.transfer %1 : !stream.resource<*>{%c12} -> !stream.resource<external>{%c12}
  %3 = stream.tensor.export %2 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %10 = "stream.async.dispatch"(%9, %7, %8, %6, %0, %5, %1, %1, %1, %6, %0, %5, %6, %0, %5, %5) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<*>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
    ** Modified: 'stream.async.dispatch'(0x56223bfbe2c0)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%c12} -> !stream.resource<external>{%c12}
  %3 = stream.tensor.export %2 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %10 = "stream.async.dispatch"(%9, %7, %8, %6, %0, %5, %1, %1, %1, %6, %0, %5, %6, %0, %5, %5) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.transfer'(0x56223b096690) {
  %11 = "stream.async.transfer"(%10, %5, %5) : (!stream.resource<external>, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp> : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncTransferOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision : 'stream.async.transfer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision"
    ** Replace : 'stream.async.transfer'(0x56223b096690)
    ** Modified: 'stream.tensor.export'(0x56223bb83690)
    ** Erase   : 'stream.async.transfer'(0x56223b096690)
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::RedundantTransferElision" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %11 = "stream.tensor.export"(%10, %5) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorExportOp> : 'stream.tensor.export -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorExportOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorExportOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyInitializerOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyInitializerOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp> : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp> : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncConstantOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyFuncOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %2 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %4 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %5 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %6 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %4, %3, %2) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %9 = "stream.tensor.import"(%arg0, %6) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorImportOp> : 'stream.tensor.import -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorImportOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorImportOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %10 = "stream.async.dispatch"(%9, %7, %8, %6, %0, %5, %1, %1, %1, %6, %0, %5, %6, %0, %5, %5) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %11 = "stream.tensor.export"(%10, %5) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorExportOp> : 'stream.tensor.export -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorExportOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ApplyStreamableOp<mlir::iree_compiler::IREE::Stream::TensorExportOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After RefineUsage (iree-stream-refine-usage) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c48 = arith.constant 48 : index
    %c0 = arith.constant 0 : index
    %c4 = arith.constant 4 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c12 = arith.constant 12 : index
    %c16 = arith.constant 16 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  
//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : %index0} = >" : a(r) -> iindexth.con

st} -> afailuren : tpattern failed to match"
(//===-------------------------------------------===//
)
//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
   <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index%

1 = } -> "failurea : rpattern failed to matchi
t//===-------------------------------------------===//
h
.//===-------------------------------------------===//
cProcessing operation : 'oarith.constantn'(s0x56223c03c9f0t) {
a  nt"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "arith.constant"() <{value = 4 : index}> : () -> index

%} -> 2failure =  : "pattern failed to matchs
t//===-------------------------------------------===//
r
e//===-------------------------------------------===//
aProcessing operation : 'marith.constant.'(a0x56223c03c980s) {
y  nc.constant"(%0) <{value = dense<[[1.54099607, %3 = "ar-0.293428898i, th.constan-2.17878938t]", ([) <0.568431258{, value = 1 : i-1.0845223732, }> : () -> i32-1.39859545], [

} -> failure : pattern failed to match
//===-------------------------------------------===//
0.403346837
, //===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (%index4) ->  = "arit!hstream..cresource<constant>onstan

t"() <
{  value* Pattern  = mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats553648160 : ' : stream.async.constant -> (i)' {
32}Trying to match ">mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : "
() -> "imlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats32" result 0
  

} -> failure} ->  : failurepattern failed to match : 
pattern failed to match} -> 
failure//===-------------------------------------------===//
 : 
pattern failed to match//===-------------------------------------------===//

Processing operation : '//===-------------------------------------------===//
arith.constant
'(//===-------------------------------------------===//
0x7f71f8007c00Processing operation : ') {
stream.async.constant  '(0x7f71e00049b0) {
  %5 = "a%r3i = t"hs.tcroenasmt.aansty"n(c).cons <t{avaluen = t12" : (index%}1>) : () -> index <{

value = } -> failure : dense<pattern failed to match
//===-------------------------------------------===//

[//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  -0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>

%
6   = * Pattern "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplatsa : 'rstream.async.constanti -> (t)' {
hTrying to match ".mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplatsc"
on"smlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplatst" result a0n
t  "} -> (failure) : pattern failed to match
} -> failure < : {pattern failed to matchvalue
 = //===-------------------------------------------===//
16
 : //===-------------------------------------------===//
indexProcessing operation : '}util.global.store>'( : 0x56223c036110() {
) ->   index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  "util.global.%s7t = o"ruet"i(l%.3g)lobal.lo <a{dglobal" = (@)_params.bias}> :  <({global = @!_params.weightstream}.>resource<constant> : ) -> (() -> )!

stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp} ->  : 'failureutil.global.store :  -> (pattern failed to match)' {

Trying to match "//===-------------------------------------------===//
mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp
"
//===-------------------------------------------===//
Processing operation : '"util.global.loadmlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp'(" result 0x56223c021cd00) {

    } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  %8 = "util.global.load"()" <u{tglobali = l@._params.biasg}l>o : b(a) -> l.sto!rstreame."resource<constant>(%2)

} -> failure < : {pattern failed to matchglobal
 = //===-------------------------------------------===//
@
_params.weight//===-------------------------------------------===//
}Processing operation : '>hal.buffer_view.assert : '((0x56223be1f320) {
  !stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "hal.buffer_view.assert"(%arg0, %4, %3, %2) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> "(u)til.

initializer.retu} -> rfailuren : "pattern failed to match(
)//===-------------------------------------------===//
 : 
(//===-------------------------------------------===//
) -> Processing operation : '(stream.tensor.import)'(0x7f71f000adc0) {
  

} -> failure : pattern failed to match
//===-------------------------------------------===//
%9 = "stream.tensor.import"(%arg0, %6) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

// -----// IR Dump After Canonicalizer (canonicalize)} ->  //----- //
failuremlir-asm-printer : : Verifying operation: pattern failed to matchutil.initializer

//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %10 = "stream.async.dispatch"(%9, %7, %8util.initializer, %6, %0, %5,  %{1
, %  1%, c48% = 1arith.constant, %6, %0 , %485 : , index%
6  , %%c120 = , arith.constant%5, %5) 12 : index
  %cst < = {stream.async.constantentry_points  = :[ @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32!]stream, .operandSegmentSizesresource<constant> = {array<%c48i}32 : =0 , 3, dense<3, 3, 3, 3[, [1>, tied_operands = [-1 : index]}>1.54099607 : , (!stream.resource<external>-0.293428898, , !stream.resource<constant>, -2.17878938!]stream, .[resource<constant>, index, index0.568431258, , index, index, index-1.08452237, , index, index, index, -1.39859545index], , index[, index, index, 0.403346837index, ) -> !0.838026344stream, .resource<external>-0.719257593

], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32
>  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> (
)' {
  Trying to match "%mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>cst_0"
 = "stream.async.constantmlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> " result :0 
  } -> !failurestream : .pattern failed to matchresource<constant>
{
%  c12* Pattern }mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs  : '=stream.async.dispatch  -> ()' {
dense<Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
["mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  -0.856674611} -> , failure : pattern failed to match
} -> failure1.10060418 : , pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
-1.07118738Processing operation : ']stream.tensor.export>'( : 0x56223bb83690tensor<) {
3  xf32>
  util.global.store %cst_0, @_params.bias : !stream.resource<constant>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.initializer.return
}

%11 = "stream.tensor.export"(%10, %5)// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: util.initializer <
{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  util.initializer {
  %c48 = arith.constant 48 : index
  %c12 = arith.constant 12 : index
  %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<["[func.return1.54099607", (%11) : -0.293428898(, !hal.-2.17878938buffer_view]) -> , ([)0.568431258

, } -> -1.08452237failure,  : pattern failed to match
//===-------------------------------------------===//
-1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  util.global.store %cst_0, @_params.bias : !stream.resource<constant>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.initializer.return
}

==== REARRANGING BLOCK ACCESSES ====
moving mutable global _params.bias store downward
moving mutable global _params.weight store downward
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: util.initializer
util.initializer {
  %c48 = arith.constant 48 : index
  %c12 = arith.constant 12 : index
  %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  util.global.store %cst_0, @_params.bias : !stream.resource<constant>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.initializer.return
}

// -----// IR Dump After CSE (cse) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

moving immutable global _params.weight load to the entry block
moving immutable global _params.bias load to the entry block
==== REARRANGING BLOCK ACCESSES ====
// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  %c48 = arith.constant 48 : index
  %c0 = arith.constant 0 : index
  %c4 = arith.constant 4 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c12 = arith.constant 12 : index
  %c16 = arith.constant 16 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}


//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %11 = "stream.tensor.export"(%10, %1) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %10 = "stream.async.dispatch"(%9, %7, %8, %0, %6, %1, %5, %5, %5, %0, %6, %1, %0, %6, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %9 = "stream.tensor.import"(%arg0, %0) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %2, %3, %4) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %0 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %2 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %4 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %5 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  %6 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'func.func -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::FoldBlockArgumentsPattern" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::ElideBranchOperandsPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c16 = arith.constant 16 : index
    %c12 = arith.constant 12 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c48 = arith.constant 48 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}



//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%11) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %11 = "stream.tensor.export"(%10, %1) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x56223bfbe2c0) {
  %10 = "stream.async.dispatch"(%9, %7, %8, %0, %6, %1, %5, %5, %5, %0, %6, %1, %0, %6, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %9 = "stream.tensor.import"(%arg0, %0) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %2, %3, %4) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  %6 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %5 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %4 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %2 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x56223b0d3460) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %0 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.end'(0x56223c037400) {
  "stream.executable.end"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223be1e4b0) {
  "func.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.store'(0x7f71f001e6e0) {
  "flow.dispatch.tensor.store"(%12, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (tensor<1x3xf32>, !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>) -> ()


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorStoreOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::FoldCastOpIntoDispatchStoreOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims : 'flow.dispatch.tensor.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorStoreShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1dfd0) {
  "linalg.yield"(%13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x7f71f001fe60) {

  * Pattern FoldTensorCastProducerOp : 'linalg.generic -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseIdentityGenericOp : 'linalg.generic -> ()' {
Trying to match "(anonymous namespace)::EraseIdentityGenericOp"
"(anonymous namespace)::EraseIdentityGenericOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x56223be21d30) {
  %13 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x56223be1ea40) {
  "linalg.yield"(%14) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x7f71f00160c0) {
  %14 = "arith.addf"(%arg6, %13) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern CanonicalizeContractAdd<mlir::arith::AddFOp> : 'arith.addf -> ()' {
Trying to match "CanonicalizeContractAdd<mlir::arith::AddFOp>"
"CanonicalizeContractAdd<mlir::arith::AddFOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.matmul'(0x56223be1eda0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.matmul -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.matmul -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.mulf'(0x56223be26e70) {
  %13 = "arith.mulf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32


  * Pattern (anonymous namespace)::MulFOfNegF : 'arith.mulf -> (arith.mulf)' {
Trying to match "(anonymous namespace)::MulFOfNegF"
    ** Failure : There's no operation that defines operand 0 of castedOp0
"(anonymous namespace)::MulFOfNegF" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x7f71f00142e0) {

  * Pattern FoldTensorCastProducerOp : 'linalg.fill -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::EraseDeadLinalgOp : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::EraseDeadLinalgOp"
"(anonymous namespace)::EraseDeadLinalgOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InferStaticShapeOfOperands : 'linalg.fill -> ()' {
Trying to match "(anonymous namespace)::InferStaticShapeOfOperands"
"(anonymous namespace)::InferStaticShapeOfOperands" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x7f71f0005960) {
  "linalg.yield"(%arg4) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x56223be1e0f0) {
  %9 = "tensor.empty"() : () -> tensor<1x3xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1de90) {
  %8 = "flow.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x3xf32>>) -> tensor<1x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1fd70) {
  %7 = "flow.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 4, 3>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<4x3xf32>>) -> tensor<4x3xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.tensor.load'(0x56223be1f8f0) {
  %6 = "flow.dispatch.tensor.load"(%2) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0>, static_sizes = array<i64: 1, 4>, static_strides = array<i64: 1, 1>}> : (!flow.dispatch.tensor<readonly:tensor<1x4xf32>>) -> tensor<1x4xf32>


  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ReuseDispatchTensorLoadShapeDims" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::ConvertDispatchInputLoadOfTensorToSubTensor" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder : 'flow.dispatch.tensor.load -> ()' {
Trying to match "mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder"
"mlir::iree_compiler::IREE::Flow::(anonymous namespace)::DispatchTensorLoadOpWithOffsetSizesAndStridesConstantArgumentFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1239e0) {
  %5 = "stream.binding.subspan"(%arg3, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123800) {
  %4 = "stream.binding.subspan"(%arg2, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b123600) {
  %3 = "stream.binding.subspan"(%arg1, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.binding.subspan'(0x56223b1240f0) {
  %2 = "stream.binding.subspan"(%arg0, %1) : (!stream.binding, index) -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04e880) {
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'builtin.module'(0x56223c04e800) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x7f71f0021640) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223be26e00) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.return'(0x56223bfc47a0) {
  "stream.return"(%0#0, %0#1, %0#2) : (index, index, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable'(0x56223c036210) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.executable.export'(0x56223c036400) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'flow.dispatch.workgroup_count_from_slice'(0x56223be22460) {
  %0:3 = "flow.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer.return'(0x56223c034c30) {
  "util.initializer.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223bb902b0) {
  "util.global.store"(%2) <{global = @_params.weight}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.store'(0x56223c036110) {
  "util.global.store"(%3) <{global = @_params.bias}> : (!stream.resource<constant>) -> ()


  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp : 'util.global.store -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::EraseUnusedGlobalStoreOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x7f71e00049b0) {
  %3 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.constant'(0x56223be28700) {
  %2 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats : 'stream.async.constant -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ConvertSplatConstantsIntoSplats" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e00081a0) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.initializer'(0x56223bb8d8b0) {

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::DropEmptyInitializerOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer : 'util.initializer -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer"
"mlir::iree_compiler::IREE::Util::(anonymous namespace)::InlineConstantGlobalInitializer" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71e0008210) {
  %0 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223c0215c0) {
  "util.global"() <{sym_name = "_params.bias", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global'(0x56223bfa5d40) {
  "util.global"() <{sym_name = "_params.weight", sym_visibility = "private", type = !stream.resource<constant>}> : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
==== inlineConstantGlobalStores ====
==== renameChainedGlobals ====
==== updateGlobalImmutability ====
==== inlineConstantGlobalLoads ====
==== eraseUnusedGlobals ====
==== deduplicateConstantGlobals ====
// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c16 = arith.constant 16 : index
    %c12 = arith.constant 12 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c48 = arith.constant 48 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: util.initializer
FuseGlobals: analyzing util.initializer {
  %c48 = arith.constant 48 : index
  %c12 = arith.constant 12 : index
  %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
  %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
  util.global.store %cst_0, @_params.bias : !stream.resource<constant>
  util.global.store %cst, @_params.weight : !stream.resource<constant>
  util.initializer.return
}:
 - store #1: util.global.store %cst_0, @_params.bias : !stream.resource<constant>; candidate=1
 - store #0: util.global.store %cst, @_params.weight : !stream.resource<constant>; candidate=1
= storing value %cst_0:
 => @_params.bias
= storing value %cst:
 => @_params.weight
mlir-asm-printer: Verifying operation: func.func
FuseGlobals: analyzing func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %c48 = arith.constant 48 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}:
FuseGlobals correlation maps:
= #0 "_params.weight" = 10:
  => _params.weight
= #1 "_params.bias" = 01:
  => _params.bias
// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c16 = arith.constant 16 : index
    %c12 = arith.constant 12 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c48 = arith.constant 48 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


mlir-asm-printer: Verifying operation: builtin.module
  !! traversal incomplete due to public function-like op @main
mlir-asm-printer: Verifying operation: builtin.module
FuncAnalysis: INCOMPLETE! @main(!hal.buffer_view) -> !hal.buffer_view 
  args: 1
    %arg0: non-uniform used !hal.buffer_view 
  results: 1
    %result#0: non-uniform used !hal.buffer_view 
  callOps: 0
// -----// IR Dump After IPO (iree-util-ipo) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c16 = arith.constant 16 : index
    %c12 = arith.constant 12 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c48 = arith.constant 48 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyAsyncAccessRanges (iree-stream-verify-async-access-ranges) //----- //
mlir-asm-printer: Verifying operation: builtin.module
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_75"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module @LinearModule attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_params.weight : !stream.resource<constant>
  util.global private @_params.bias : !stream.resource<constant>
  util.initializer {
    %c48 = arith.constant 48 : index
    %c12 = arith.constant 12 : index
    %cst = stream.async.constant : !stream.resource<constant>{%c48} = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>
    %cst_0 = stream.async.constant : !stream.resource<constant>{%c12} = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>
    util.global.store %cst_0, @_params.bias : !stream.resource<constant>
    util.global.store %cst, @_params.weight : !stream.resource<constant>
    util.initializer.return
  }
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_matmul_1x3x4_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_matmul_1x3x4_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<4x3xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x3xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4xf32>> -> tensor<1x4xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<4x3xf32>> -> tensor<4x3xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x3xf32>> -> tensor<1x3xf32>
        %7 = tensor.empty() : tensor<1x3xf32>
        %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %9 = linalg.matmul ins(%4, %5 : tensor<1x4xf32>, tensor<4x3xf32>) outs(%8 : tensor<1x3xf32>) -> tensor<1x3xf32>
        %10 = linalg.generic liuyinuo generic{indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %6 : tensor<1x3xf32>, tensor<1x3xf32>) outs(%7 : tensor<1x3xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          linalg.yield %11 : f32
        } -> tensor<1x3xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [0, 0], sizes = [1, 3], strides = [1, 1] : tensor<1x3xf32> -> !flow.dispatch.tensor<writeonly:tensor<1x3xf32>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c16 = arith.constant 16 : index
    %c12 = arith.constant 12 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %c48 = arith.constant 48 : index
    %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
    %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


mlir-asm-printermlir-asm-printer: Verifying operation: : Verifying operation: builtin.modulebuiltin.module

Side-effecting op forcing flush and freeze:
util.initializer.return
====
Partitioning op:
util.initializer.return
Not streamable (skip)
(ignoring global store)
(ignoring global store)
====
Partitioning op:
%cst_0 = stream.async.constant : Side-effecting op forcing flush and freeze:
!stream.resource<constant>func.return{ %%c122}  :=  dense<!hal.buffer_view
[====
Partitioning op:
func.return %2 : -0.856674611!, hal.buffer_view
Not streamable (skip)
1.10060418, ====
Partitioning op:
%-1.071187382] = >stream.tensor.export :  %tensor<13 x:f32 >tensor<3xf32>
 in Created partition 0!
stream.resource<external>{====
Partitioning op:
%c12%}cst  = -> stream.async.constant :! hal.buffer_view!stream.resource<constant>{%c48} 
= Testing user:
dense<func.return %2[ [: !hal.buffer_view
1.54099607Not streamable (skip)
, ====
Partitioning op:
%1 = -0.293428898, stream.async.dispatch @main_dispatch_0::@-2.17878938main_dispatch_0_matmul_1x3x4_f32], [(%0[%c0 to 0.568431258%, c16 for %c16], %_params.weight-1.08452237[, %c0 to %c48 for %c48-1.39859545]], , %[_params.bias[%c0 to %c12 for 0.403346837%, c12])0.838026344, -0.719257593], [ : (-0.403343529, !stream.resource<external>-0.596635341, {%c16}, 0.182036489]!]stream>.resource<constant> : {tensor<%4c48x}3, xf32>!stream.resource<constant>{%
c12}) -> Moving to first candidate partition 0 (continue)
(ignoring constant)
!(ignoring constant)
stream.resource<external>{%c12}PARTITION[
0]:
Testing user:
 INS:
  %%2c48 = , %stream.tensor.exportc12 
 OUTS:
  %%1cst , :% cst_0
 OPS:
  tensor<%3cstx = f32>stream.async.constant  in:  !stream!.streamresource<external>.{resource<constant>%{c12%}c48 }->  = !haldense<.buffer_view[[
Created partition 1.540996070, 
-0.293428898====
Partitioning op:
, %0 = stream.tensor.import %-2.17878938arg0] , :[ !hal.buffer_view0.568431258 , -> tensor<4x-1.08452237f32, > in !-1.39859545stream]., resource<external>[{%c16}0.403346837, 
Testing user:
0.838026344%, 1 = stream.async.dispatch @-0.719257593main_dispatch_0]::, @[main_dispatch_0_matmul_1x3x4_f32(%0[%-0.403343529c0,  to %c16 for %c16], -0.596635341%, _params.weight[%c0 to %c48 for 0.182036489%]c48]]>,  : %_params.biastensor<[4%xc03 to x%f32c12> for %c12])
  %cst_0 = stream.async.constant :  :! stream.resource<constant>({%c12} !=stream .resource<external>dense<{%c16}[, !stream.resource<constant>{-0.856674611%, c48}, !stream1.10060418., resource<constant>{%c12}) -> -1.07118738]> : !streamtensor<.3resource<external>x{f32%>c12}
  member of partition 0

Not streamable (skip)
Side-effecting op forcing flush and freeze:
hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
====
Partitioning op:
hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("Cloned op into partition i0n: put 0") ImplicitTypeIDRegistry::lookupOrInsert(shapemlir::OpTrait::AtLeastNResults<1>::Impl<Empty>()
[%ImplicitTypeIDRegistry::lookupOrInsert(c4mlir::OpTrait::SingleBlockImplicitTerminator<mlir::iree_compiler::IREE::Stream::YieldOp>::Impl<Empty>])
) typeImplicitTypeIDRegistry::lookupOrInsert((mlir::RegionBranchOpInterface::Trait<Empty>%)
c553648160_i32)ImplicitTypeIDRegistry::lookupOrInsert( mlir::iree_compiler::IREE::Stream::TimelineOpInterface::Trait<Empty>encoding)
(%mlir-asm-printerc1_i32: Verifying operation: )util.initializer

Not streamable (skip)
====
Partitioning op:
%_params.bias = util.global.load @_params.bias : !stream.resource<constant>
Testing user:
%1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  member of partition 0
Not streamable (skip)
====
Partitioning op:
%_params.weight = util.global.load @_params.weight : !stream.resource<constant>
Testing user:
%1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%0[%c0 to %c16 for %c16], %_params.weight[%c0 to %c48 for %c48], %_params.bias[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  member of partition 0
Not streamable (skip)
(ignoring constant)
(ignoring constant)
(ignoring constant)
(ignoring constant)
(ignoring constant)
(ignoring constant)
(ignoring constant)
PARTITION[0block with no terminator, has ]:
%5 = "stream.async.constant"(%0) <{value = dense<[[1.54099607, -0.293428898, -2.17878938], [0.568431258, -1.08452237, -1.39859545], [0.403346837, 0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant> INS:
  
%0, mlir-asm-printer%: '_params.weightutil.initializer, ' failed to verify and will be printed in generic form
%_params.bias, %c16, %c48, %c12, %c0
 OUTS:
  %1
 OPS:
  %1 = stream.async.dispatch @main_dispatch_0::@%main_dispatch_0_matmul_1x3x4_f325 = ("%s0t[r%ec0a to m%.c16a for s%yc16n]c, .%c_params.weighto[n%sc0t to a%nc48t for "%(c48%]0, )%_params.bias[%c0 to  <%{c12value for  = %c12]dense<)[[ 1.54099607:,  (-0.293428898!, stream.resource<external>{%c16-2.17878938}], , [!stream.resource<constant>{0.568431258%, c48}, !stream-1.08452237., resource<constant>{%c12}) -> -1.39859545], [!stream.resource<external>0.403346837{, %c12}
0.838026344, -0.719257593], [-0.403343529, -0.596635341, 0.182036489]]> : tensor<4x3xf32>}> : (index) -> !stream.resource<constant>
Cloned op into partition 0: mlir-asm-printer: Verifying operation: util.initializer
Cloned op into partition 0: mlir-asm-printer: Verifying operation: func.func
block with no terminator, has %6 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> !stream.resource<constant>
mlir-asm-printer: 'util.initializer' failed to verify and will be printed in generic form
%6 = "stream.async.constant"(%1) <{value = dense<[-0.856674611, 1.10060418, -1.07118738]> : tensor<3xf32>}> : (index) -> block with no terminator, has !%13 = "stream.async.dispatch"(%arg1, %arg2, %arg3, %0, %6, %1, %5, %5, %5, %0, %6, %1, %0, %6, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<external>stream
.resource<constant>mlir-asm-printer: '
func.func' failed to verify and will be printed in generic form
%13 = "stream.async.dispatch"(%arg1, %arg2, %arg3, %0, %6, %1, %5, %
Partitions constructed:
5mlir-asm-printer, : Verifying operation: %builtin.module5
, %0, %6, %1, %0, %6, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, ImplicitTypeIDRegistry::lookupOrInsert(indexmlir::OpTrait::HasParent<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp, mlir::iree_compiler::IREE::Stream::AsyncConcurrentOp, mlir::iree_compiler::IREE::Stream::CmdExecuteOp, mlir::iree_compiler::IREE::Stream::CmdSerialOp, mlir::iree_compiler::IREE::Stream::CmdConcurrentOp>::Impl<Empty>, )
index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<external>

Partitions constructed:
mlir-asm-printer: Verifying operation: builtin.module
Please report issues to https://github.com/openxla/iree/issues and include the crash backtrace.
^bb0(%arg0: !hal.buffer_view):
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %c48 = arith.constant 48 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c16}, %_params.weight as %arg2: !stream.resource<constant>{%c48}, %_params.bias as %arg3: !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%arg1[%c0 to %c16 for %c16], %arg2[%c0 to %c48 for %c48], %arg3[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    stream.yield %3 : !stream.resource<external>{%c12}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  func.return %2 : !hal.buffer_view

//===-------------------------------------------===//
Processing operation : 'func.return'(0x56223b8f73c0) {
  "func.return"(%12) : (!hal.buffer_view) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.export'(0x56223bb83690) {
  %12 = "stream.tensor.export"(%11, %1) <{source_encoding = tensor<3xf32>}> : (!stream.resource<external>, index) -> !hal.buffer_view

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.timepoint.await'(0x7f71d4008f50) {
  %11 = "stream.timepoint.await"(%10#0, %1, %10#1) <{operandSegmentSizes = array<i32: 1, 1, 1>}> : (!stream.resource<external>, index, !stream.timepoint) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::TimepointAwaitOp> : 'stream.timepoint.await -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::TimepointAwaitOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::TimepointAwaitOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideImmediateHostAwaits : 'stream.timepoint.await -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideImmediateHostAwaits"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideImmediateHostAwaits" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SinkAwaitToFirstConsumer : 'stream.timepoint.await -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SinkAwaitToFirstConsumer"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SinkAwaitToFirstConsumer" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SinkSubviewsAcrossAwaits : 'stream.timepoint.await -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SinkSubviewsAcrossAwaits"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::SinkSubviewsAcrossAwaits" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::GroupAwaitsByTimepoint : 'stream.timepoint.await -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::GroupAwaitsByTimepoint"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::GroupAwaitsByTimepoint" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::FoldDuplicateAwaitResources : 'stream.timepoint.await -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::FoldDuplicateAwaitResources"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::FoldDuplicateAwaitResources" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.yield'(0x7f71d4008ea0) {
  "stream.yield"(%13, %1) : (!stream.resource<external>, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.execute'(0x7f71d40082d0) {

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp> : 'stream.async.execute -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideImmediateTimepointWait<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp> : 'stream.async.execute -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideImmediateTimepointWait<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideImmediateTimepointWait<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ChainDependentAwaits<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp> : 'stream.async.execute -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ChainDependentAwaits<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ChainDependentAwaits<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::CloneCapturedAsyncExecuteSubviewOps : 'stream.async.execute -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::CloneCapturedAsyncExecuteSubviewOps"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::CloneCapturedAsyncExecuteSubviewOps" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideNoOpAsyncExecuteOp : 'stream.async.execute -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideNoOpAsyncExecuteOp"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideNoOpAsyncExecuteOp" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp> : 'stream.async.execute -> ()' {
Trying to match "mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>"
"mlir::iree_compiler::IREE::Util::ClosureOptimizationPattern<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TieRegionResults<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp> : 'stream.async.execute -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TieRegionResults<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::TieRegionResults<mlir::iree_compiler::IREE::Stream::AsyncExecuteOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.async.dispatch'(0x7f71d4008460) {
  %13 = "stream.async.dispatch"(%arg1, %arg2, %arg3, %0, %6, %1, %5, %5, %5, %0, %6, %1, %0, %6, %1, %1) <{entry_points = [@main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32], operandSegmentSizes = array<i32: 0, 3, 3, 3, 3, 3, 1>, tied_operands = [-1 : index]}> : (!stream.resource<external>, !stream.resource<constant>, !stream.resource<constant>, index, index, index, index, index, index, index, index, index, index, index, index, index) -> !stream.resource<external>


  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp> : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::ElideUnusedOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>" result 0
  } -> failure : pattern failed to match

  * Pattern mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs : 'stream.async.dispatch -> ()' {
Trying to match "mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs"
"mlir::iree_compiler::IREE::Stream::(anonymous namespace)::DeduplicateAsyncDispatchEntryRefs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'stream.tensor.import'(0x7f71f000adc0) {
  %9 = "stream.tensor.import"(%arg0, %0) <{result_encoding = tensor<4xf32>}> : (!hal.buffer_view, index) -> !stream.resource<external>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'hal.buffer_view.assert'(0x56223be1f320) {
  "hal.buffer_view.assert"(%arg0, %2, %3, %4) {message = "input 0"} : (!hal.buffer_view, i32, i32, index) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c021cd0) {
  %8 = "util.global.load"() <{global = @_params.bias}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'util.global.load'(0x56223c022060) {
  %7 = "util.global.load"() <{global = @_params.weight}> : () -> !stream.resource<constant>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c131400) {
  %6 = "arith.constant"() <{value = 48 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c042b90) {
  %5 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c9f0) {
  %4 = "arith.constant"() <{value = 4 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c03c980) {
  %3 = "arith.constant"() <{value = 1 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x56223c04eaa0) {
  %2 = "arith.constant"() <{value = 553648160 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8007c00) {
  %1 = "arith.constant"() <{value = 12 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x7f71f8009630) {
  %0 = "arith.constant"() <{value = 16 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After ScheduleExecution (iree-stream-schedule-execution) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %c48 = arith.constant 48 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c16}, %_params.weight as %arg2: !stream.resource<constant>{%c48}, %_params.bias as %arg3: !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%arg1[%c0 to %c16 for %c16], %arg2[%c0 to %c48 for %c48], %arg3[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    stream.yield %3 : !stream.resource<external>{%c12}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

mlir-asm-printer: Verifying operation: func.func
mlir-asm-printer: Verifying operation: func.func
====
Partitioning op:
stream.yield %3 : !stream.resource<external>{%c12}
Not streamable/is subview (skip)
====
Partitioning op:
%3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%arg1[%c0 to %c16 for %c16], %arg2[%c0 to %c48 for %c48], %arg3[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
Testing user:
stream.yield %3 : !stream.resource<external>{%c12}
Created wave 0
PARTITION[0]:
 INS:
  %arg1, %arg2, %arg3, %c16, %c48, %c12, %c0
 OUTS:
  %3
 OPS:
  %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%arg1[%c0 to %c16 for %c16], %arg2[%c0 to %c48 for %c48], %arg3[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}

Waves constructed:
mlir-asm-printer: Verifying operation: builtin.module
^bb0(%arg1: !stream.resource<external>, %arg2: !stream.resource<constant>, %arg3: !stream.resource<constant>):
  %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%arg1[%c0 to %c16 for %c16], %arg2[%c0 to %c48 for %c48], %arg3[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
  stream.yield %3 : !stream.resource<external>{%c12}
// -----// IR Dump After ScheduleConcurrency (iree-stream-schedule-concurrency) //----- //
mlir-asm-printer: Verifying operation: func.func
func.func @main(%arg0: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c16 = arith.constant 16 : index
  %c12 = arith.constant 12 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %c48 = arith.constant 48 : index
  %_params.weight = util.global.load @_params.weight : !stream.resource<constant>
  %_params.bias = util.global.load @_params.bias : !stream.resource<constant>
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c4]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<4xf32> in !stream.resource<external>{%c16}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c16}, %_params.weight as %arg2: !stream.resource<constant>{%c48}, %_params.bias as %arg3: !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_matmul_1x3x4_f32(%arg1[%c0 to %c16 for %c16], %arg2[%c0 to %c48 for %c48], %arg3[%c0 to %c12 for %c12]) : (!stream.resource<external>{%c16}, !stream.resource<constant>{%c48}, !stream.resource<constant>{%c12}) -> !stream.resource<external>{%c12}
    stream.yield %3 : !stream.resource<external>{%c12}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c12}
  %2 = stream.tensor.export %1 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

 #0 0x00007f720503006d llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/lib/Support/Unix/Signals.inc:723:11
 #1 0x00007f720503055b PrintStackTraceSignalHandler(void*) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/lib/Support/Unix/Signals.inc:798:1
 #2 0x00007f720502e5c6 llvm::sys::RunSignalHandlers() /home/data/liuyn/code/iree/third_party/llvm-project/llvm/lib/Support/Signals.cpp:105:5
 #3 0x00007f7205030d15 SignalHandler(int) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/lib/Support/Unix/Signals.inc:413:1
 #4 0x00007f7201b69980 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x12980)
 #5 0x00007f7204e65a8c mlir::AbstractType::getTypeID() const /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/TypeSupport.h:101:37
 #6 0x00007f7204e657e0 mlir::Type::getTypeID() /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/Types.h:112:55
 #7 0x00007f72069ca015 bool mlir::detail::StorageUserBase<mlir::iree_compiler::IREE::Stream::ResourceType, mlir::Type, mlir::iree_compiler::IREE::Stream::detail::ResourceTypeStorage, mlir::detail::TypeUniquer, mlir::iree_compiler::IREE::Util::ReferenceTypeInterface::Trait, mlir::iree_compiler::IREE::Util::SizeAwareTypeInterface::Trait, mlir::iree_compiler::IREE::Util::GlobalTypeInterface::Trait, mlir::iree_compiler::IREE::Util::InferTypeSizeInterface::Trait, mlir::iree_compiler::IREE::Util::SubrangeTypeInterface::Trait>::classof<mlir::Type>(mlir::Type) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/StorageUniquerSupport.h:114:16
 #8 0x00007f72069df1cd llvm::CastInfo<mlir::iree_compiler::IREE::Stream::ResourceType, mlir::Type const, void>::isPossible(mlir::Type) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/Types.h:411:7
 #9 0x00007f72069ded10 bool llvm::isa<mlir::iree_compiler::IREE::Stream::ResourceType, mlir::Type>(mlir::Type const&) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/include/llvm/Support/Casting.h:549:3
#10 0x00007f7207ac5c95 bool mlir::Type::isa<mlir::iree_compiler::IREE::Stream::ResourceType>() const /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/Types.h:320:3
#11 0x00007f720b50f118 mlir::iree_compiler::IREE::Stream::__mlir_ods_local_type_constraint_StreamOps8(mlir::Operation*, mlir::Type, llvm::StringRef, unsigned int) /home/data/liuyn/code/iree/iree-build-2/compiler/src/iree/compiler/../../iree/compiler/Dialect/Stream/IR/StreamOps.cpp.inc:252:53
#12 0x00007f720b50f00f mlir::iree_compiler::IREE::Stream::AsyncDispatchOp::verifyInvariantsImpl() /home/data/liuyn/code/iree/iree-build-2/compiler/src/iree/compiler/../../iree/compiler/Dialect/Stream/IR/StreamOps.cpp.inc:5272:26
#13 0x00007f720b39adc2 mlir::OpTrait::OpInvariants<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>::verifyTrait(mlir::Operation*) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/OpDefinition.h:427:35
#14 0x00007f720b39ab35 std::enable_if<detect_has_verify_trait<mlir::OpTrait::OpInvariants<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>>::value, mlir::LogicalResult>::type mlir::op_definition_impl::verifyTrait<mlir::OpTrait::OpInvariants<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>>(mlir::Operation*) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/OpDefinition.h:1615:10
#15 0x00007f720b39a7f9 mlir::LogicalResult mlir::op_definition_impl::verifyTraits<mlir::OpTrait::ZeroRegions<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::OpTrait::VariadicResults<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::OpTrait::ZeroSuccessors<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::OpTrait::VariadicOperands<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::OpTrait::AttrSizedOperandSegments<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::OpTrait::OpInvariants<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::BytecodeOpInterface::Trait<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::SymbolUserOpInterface::Trait<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::iree_compiler::IREE::Stream::AffinityOpInterface::Trait<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::OpTrait::IREE::Stream::AsyncPhaseOp<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::iree_compiler::IREE::Stream::StreamableOpInterface::Trait<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::iree_compiler::IREE::Stream::AsyncAccessOpInterface::Trait<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::iree_compiler::IREE::Util::SizeAwareOpInterface::Trait<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>, mlir::iree_compiler::IREE::Util::TiedOpInterface::Trait<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>>(mlir::Operation*) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/OpDefinition.h:1626:29
#16 0x00007f720b39a6a5 mlir::Op<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp, mlir::OpTrait::ZeroRegions, mlir::OpTrait::VariadicResults, mlir::OpTrait::ZeroSuccessors, mlir::OpTrait::VariadicOperands, mlir::OpTrait::AttrSizedOperandSegments, mlir::OpTrait::OpInvariants, mlir::BytecodeOpInterface::Trait, mlir::SymbolUserOpInterface::Trait, mlir::iree_compiler::IREE::Stream::AffinityOpInterface::Trait, mlir::OpTrait::IREE::Stream::AsyncPhaseOp, mlir::iree_compiler::IREE::Stream::StreamableOpInterface::Trait, mlir::iree_compiler::IREE::Stream::AsyncAccessOpInterface::Trait, mlir::iree_compiler::IREE::Util::SizeAwareOpInterface::Trait, mlir::iree_compiler::IREE::Util::TiedOpInterface::Trait>::verifyInvariants(mlir::Operation*) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/OpDefinition.h:2007:16
#17 0x00007f72051bea85 mlir::LogicalResult llvm::detail::UniqueFunctionBase<mlir::LogicalResult, mlir::Operation*>::CallImpl<mlir::LogicalResult (* const)(mlir::Operation*)>(void*, mlir::Operation*) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/include/llvm/ADT/FunctionExtras.h:220:12
#18 0x00007f72051bda67 llvm::unique_function<mlir::LogicalResult (mlir::Operation*) const>::operator()(mlir::Operation*) const /home/data/liuyn/code/iree/third_party/llvm-project/llvm/include/llvm/ADT/FunctionExtras.h:408:12
#19 0x00007f720b398d16 mlir::RegisteredOperationName::Model<mlir::iree_compiler::IREE::Stream::AsyncDispatchOp>::verifyInvariants(mlir::Operation*) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/OperationSupport.h:558:14
#20 0x00007f72052aa4f6 mlir::OperationName::verifyInvariants(mlir::Operation*) const /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/OperationSupport.h:317:23
#21 0x00007f72052a291d (anonymous namespace)::OperationVerifier::verifyOnEntrance(mlir::Operation&) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/lib/IR/Verifier.cpp:179:48
#22 0x00007f72052a2670 auto (anonymous namespace)::OperationVerifier::verifyOperation(mlir::Operation&)::$_2::operator()<mlir::Operation>(mlir::Operation*) const /home/data/liuyn/code/iree/third_party/llvm-project/mlir/lib/IR/Verifier.cpp:293:45
#23 0x00007f72052a1557 auto (anonymous namespace)::OperationVerifier::verifyOperation(mlir::Operation&)::$_1::operator()<(anonymous namespace)::OperationVerifier::verifyOperation(mlir::Operation&)::$_2>((anonymous namespace)::OperationVerifier::verifyOperation(mlir::Operation&)::$_2&&, llvm::PointerUnion<mlir::Operation*, mlir::Block*>) const /home/data/liuyn/code/iree/third_party/llvm-project/mlir/lib/IR/Verifier.cpp:277:16
#24 0x00007f72052a0e1f (anonymous namespace)::OperationVerifier::verifyOperation(mlir::Operation&) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/lib/IR/Verifier.cpp:292:16
#25 0x00007f72052a0be1 (anonymous namespace)::OperationVerifier::verifyOpAndDominance(mlir::Operation&) /home/data/liuyn/code/iree/third_party/llvm-project/mlir/lib/IR/Verifier.cpp:85:14
#26 0x00007f72052a1d70 (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0::operator()(mlir::Operation*) const /home/data/liuyn/code/iree/third_party/llvm-project/mlir/lib/IR/Verifier.cpp:229:38
#27 0x00007f72052a2214 mlir::LogicalResult mlir::failableParallelForEach<mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0>(mlir::MLIRContext*, mlir::Operation**, mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0&&)::'lambda'()::operator()() const /home/data/liuyn/code/iree/third_party/llvm-project/mlir/include/mlir/IR/Threading.h:62:18
#28 0x00007f72052a2175 mlir::Operation** std::__invoke_impl<void, mlir::LogicalResult mlir::failableParallelForEach<mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0>(mlir::MLIRContext*, mlir::Operation**, mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0&&)::'lambda'()&>(std::__invoke_other, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0&&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/invoke.h:61:7
#29 0x00007f72052a2135 std::enable_if<is_invocable_r_v<mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0>, mlir::Operation**>::type std::__invoke_r<void, mlir::LogicalResult mlir::failableParallelForEach<mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0>(mlir::MLIRContext*, mlir::Operation**, mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0&&)::'lambda'()&>((anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0&&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/invoke.h:117:5
#30 0x00007f72052a202d std::_Function_handler<void (), mlir::LogicalResult mlir::failableParallelForEach<mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0>(mlir::MLIRContext*, mlir::Operation**, mlir::Operation**, (anonymous namespace)::OperationVerifier::verifyOnExit(mlir::Operation&)::$_0&&)::'lambda'()>::_M_invoke(std::_Any_data const&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/std_function.h:291:2
#31 0x00007f7204fb4dd5 std::function<void ()>::operator()() const /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/std_function.h:560:2
#32 0x00007f72052a890d llvm::ThreadPool::createTaskAndFuture(std::function<void ()>)::'lambda'()::operator()() const /home/data/liuyn/code/iree/third_party/llvm-project/llvm/include/llvm/Support/ThreadPool.h:135:15
#33 0x00007f72052a88e5 void std::__invoke_impl<void, llvm::ThreadPool::createTaskAndFuture(std::function<void ()>)::'lambda'()&>(std::__invoke_other, llvm::ThreadPool::createTaskAndFuture(std::function<void ()>)::'lambda'()&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/invoke.h:61:7
#34 0x00007f72052a88a5 std::enable_if<is_invocable_r_v<void, llvm::ThreadPool::createTaskAndFuture(std::function<void ()>)::'lambda'()&>, void>::type std::__invoke_r<void, llvm::ThreadPool::createTaskAndFuture(std::function<void ()>)::'lambda'()&>(llvm::ThreadPool::createTaskAndFuture(std::function<void ()>)::'lambda'()&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/invoke.h:117:5
#35 0x00007f72052a87ad std::_Function_handler<void (), llvm::ThreadPool::createTaskAndFuture(std::function<void ()>)::'lambda'()>::_M_invoke(std::_Any_data const&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/std_function.h:291:2
#36 0x00007f7204fb4dd5 std::function<void ()>::operator()() const /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/std_function.h:560:2
#37 0x00007f7204fb2f52 llvm::ThreadPool::processTasks(llvm::ThreadPoolTaskGroup*) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/lib/Support/ThreadPool.cpp:104:5
#38 0x00007f7204fb4045 llvm::ThreadPool::grow(int)::$_0::operator()() const /home/data/liuyn/code/iree/third_party/llvm-project/llvm/lib/Support/ThreadPool.cpp:50:5
#39 0x00007f7204fb3fc9 auto void llvm::thread::GenericThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*)::'lambda'(auto&&, auto&&...)::operator()<llvm::ThreadPool::grow(int)::$_0&>(auto&&, auto&&...) const /home/data/liuyn/code/iree/third_party/llvm-project/llvm/include/llvm/Support/thread.h:44:9
#40 0x00007f7204fb3f9d auto std::__invoke_impl<void, void llvm::thread::GenericThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*)::'lambda'(auto&&, auto&&...), llvm::ThreadPool::grow(int)::$_0&>(std::__invoke_other, void llvm::thread::GenericThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*)::'lambda'(auto&&, auto&&...)&&, llvm::ThreadPool::grow(int)::$_0&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/invoke.h:61:7
#41 0x00007f7204fb3f4d std::__invoke_result<auto, auto...>::type std::__invoke<void llvm::thread::GenericThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*)::'lambda'(auto&&, auto&&...), llvm::ThreadPool::grow(int)::$_0&>(auto&&, auto&&...) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/bits/invoke.h:96:7
#42 0x00007f7204fb3f1d decltype(auto) std::__apply_impl<void llvm::thread::GenericThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*)::'lambda'(auto&&, auto&&...), std::tuple<llvm::ThreadPool::grow(int)::$_0>&, 0ul>(auto&&, std::tuple<llvm::ThreadPool::grow(int)::$_0>&, std::integer_sequence<unsigned long, 0ul>) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/tuple:1806:7
#43 0x00007f7204fb3e6d decltype(auto) std::apply<void llvm::thread::GenericThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*)::'lambda'(auto&&, auto&&...), std::tuple<llvm::ThreadPool::grow(int)::$_0>&>(auto&&, std::tuple<llvm::ThreadPool::grow(int)::$_0>&) /usr/lib/gcc/x86_64-linux-gnu/11/../../../../include/c++/11/tuple:1817:7
#44 0x00007f7204fb3e3e void llvm::thread::GenericThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/include/llvm/Support/thread.h:46:3
#45 0x00007f7204fb3b65 void* llvm::thread::ThreadProxy<std::tuple<llvm::ThreadPool::grow(int)::$_0>>(void*) /home/data/liuyn/code/iree/third_party/llvm-project/llvm/include/llvm/Support/thread.h:56:5
#46 0x00007f7201b5e6db start_thread /build/glibc-CVJwZb/glibc-2.27/nptl/pthread_create.c:463:0
#47 0x00007f720126261f __clone /build/glibc-CVJwZb/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97:0
